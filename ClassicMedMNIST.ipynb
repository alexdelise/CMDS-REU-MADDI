{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48a5cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, ToTensor, GaussianBlur\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# MedMNIST import\n",
    "import medmnist\n",
    "from medmnist import INFO\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- MedMNIST Data setup ---\n",
    "data_flag = 'retinamnist'\n",
    "info = INFO[data_flag]\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "train_dataset = DataClass(split='train', download=True)\n",
    "numSamples = len(train_dataset)\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a09bbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data tensors \n",
    "def _get_tensor(ds, n):\n",
    "    \"\"\"Stack first n samples, convert to float32 in [0,1].\"\"\"\n",
    "    imgs = []\n",
    "    for i in range(n):\n",
    "        x, _ = ds[i]\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            t = x.float() / 255.0\n",
    "        elif isinstance(x, np.ndarray):\n",
    "            t = torch.from_numpy(x).float() / 255.0\n",
    "            if t.ndim == 3:\n",
    "                t = t.permute(2, 0, 1)\n",
    "        else:\n",
    "            t = TF.to_tensor(x)\n",
    "        imgs.append(t[0])\n",
    "    return torch.stack(imgs).to(device)\n",
    "\n",
    "# Load and prepare dataset\n",
    "X_raw = _get_tensor(train_dataset, numSamples)  # (N, H, W)\n",
    "dim = X_raw[0].numel()  # e.g. 28*28 = 784\n",
    "r = 25  # bottleneck / testing rank\n",
    "\n",
    "# X Stuff\n",
    "X = X_raw.view(numSamples, -1).T  # (dim, numSamples)\n",
    "gammaX = 1 / (numSamples) * X @ X.T + 1e-5 * torch.eye(dim, device=device)\n",
    "L_X = torch.linalg.cholesky(gammaX)  # (dim, dim)\n",
    "\n",
    "# Perform SVD of L_X as in proof\n",
    "U, S, Vh = torch.linalg.svd(L_X) \n",
    "U_r, S_r, Vh_r = U[:, :r], torch.diag(S[:r]), Vh[:r, :]\n",
    "L_X_r = U_r @ S_r @ Vh_r\n",
    "M_r = L_X_r @ torch.linalg.pinv(L_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a327e81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Autoencoder setup -------------------------------------------------------\n",
    "batch_size = 128\n",
    "num_epochs = 200\n",
    "\n",
    "# Define Linear Autoencoder model\n",
    "class LinearAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, bottleneck_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(input_dim, bottleneck_dim, bias=False)\n",
    "        self.decoder = nn.Linear(bottleneck_dim, input_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "\n",
    "# Instantiate model, optimizer, and loss\n",
    "model     = LinearAutoencoder(input_dim=dim, bottleneck_dim=r).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Prepare training data tensors\n",
    "X_tensor = X.T  # Shape (numSamples, dim)\n",
    "Y_tensor = X.T  # Here using X as both input/output since this is the identity reconstruction problem\n",
    "\n",
    "# Training loop using average per-sample L2 error -----------------------------\n",
    "train_errors = []\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(Y_tensor)\n",
    "    target  = X_tensor\n",
    "\n",
    "    loss = criterion(outputs, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluate and store error after each epoch\n",
    "    with torch.no_grad():\n",
    "        A_learned = model.decoder.weight @ model.encoder.weight\n",
    "\n",
    "        diffs = A_learned @ X - X\n",
    "        avg_l2_err = torch.norm(diffs, dim=0).mean().item()  # average per-sample l2 discrepancy\n",
    "\n",
    "        train_errors.append(avg_l2_err)\n",
    "\n",
    "print(f\"Final average per-sample L2 training error (learned): {train_errors[-1]:.4f}\")\n",
    "\n",
    "# --- Compact 2×2 gallery comparing Optimal vs Learned reconstructions --------\n",
    "from matplotlib import gridspec\n",
    "\n",
    "# Example sample index for plotting\n",
    "idx = 100\n",
    "\n",
    "# Prepare images for plotting\n",
    "x_img = X_raw[idx].view(28, 28).cpu()\n",
    "\n",
    "opt_img   = (M_r @ X)[:, idx].view(28, 28).cpu()\n",
    "learn_img = (A_learned @ X)[:, idx].view(28, 28).cpu()\n",
    "\n",
    "# Errors\n",
    "err_opt   = torch.abs(opt_img   - x_img)\n",
    "err_learn = torch.abs(learn_img - x_img)\n",
    "err_vmin, err_vmax = 0.0, max(err_opt.max(), err_learn.max()).item()\n",
    "\n",
    "# Create 2×3 plot grid (Original | Reconstruction | Errors)\n",
    "fig = plt.figure(figsize=(8, 4.2))\n",
    "gs  = gridspec.GridSpec(\n",
    "    2, 4,\n",
    "    width_ratios=[1, 1, 1, 0.06],\n",
    "    wspace=0.20, hspace=0.30\n",
    ")\n",
    "\n",
    "# row 0, col 0: Original image\n",
    "ax = fig.add_subplot(gs[0, 0])\n",
    "ax.imshow(x_img, cmap='gray')\n",
    "ax.set_title(r'Original $\\mathbf{x}$', pad=4)\n",
    "ax.axis('off')\n",
    "\n",
    "# row 0, col 1: Optimal reconstruction\n",
    "ax = fig.add_subplot(gs[0, 1])\n",
    "ax.imshow(opt_img, cmap='gray')\n",
    "ax.set_title(r'Optimal $\\widehat{\\mathbf{A}}_{25} \\mathbf{x}$', pad=4)\n",
    "ax.axis('off')\n",
    "\n",
    "# row 0, col 2: Optimal absolute error\n",
    "ax = fig.add_subplot(gs[0, 2])\n",
    "im = ax.imshow(err_opt, cmap='inferno', vmin=err_vmin, vmax=err_vmax)\n",
    "ax.set_title(r'$|\\widehat{\\mathbf{A}}_{25} \\mathbf{x} - \\mathbf{x}|$', pad=4)\n",
    "ax.axis('off')\n",
    "\n",
    "# row 1, col 0: Original image again\n",
    "ax = fig.add_subplot(gs[1, 0])\n",
    "ax.imshow(x_img, cmap='gray')\n",
    "ax.set_title(r'Original $\\mathbf{x}$', pad=4)\n",
    "ax.axis('off')\n",
    "\n",
    "# row 1, col 1: Learned reconstruction\n",
    "ax = fig.add_subplot(gs[1, 1])\n",
    "ax.imshow(learn_img, cmap='gray')\n",
    "ax.set_title(r'Learned $\\mathbf{A}_{25} \\mathbf{x}$', pad=4)\n",
    "ax.axis('off')\n",
    "\n",
    "# row 1, col 2: Learned absolute error\n",
    "ax = fig.add_subplot(gs[1, 2])\n",
    "ax.imshow(err_learn, cmap='inferno', vmin=err_vmin, vmax=err_vmax)\n",
    "ax.set_title(r'$|\\mathbf{A}_{25} \\mathbf{x} - \\mathbf{x}|$', pad=4)\n",
    "ax.axis('off')\n",
    "\n",
    "# color-bar\n",
    "cax = fig.add_subplot(gs[:, 3])\n",
    "plt.colorbar(im, cax=cax)\n",
    "cax.yaxis.tick_right()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d585c505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle, torch\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ── pastel-Everforest colours \n",
    "ef_pastel   = ['#a7c080', '#d3869b', \"#83c0c0\", '#e69875', '#a988b0', \"#b8912f\"]\n",
    "line_styles = {'theory': '-',  'learned': '--'}\n",
    "markers     = {'theory': 'o',  'learned': 's'}\n",
    "lw, ms      = 1.5, 4\n",
    "\n",
    "# ── where to stash matrices and final plots \n",
    "save_root = \"ClassicPics\"\n",
    "matrices_root = os.path.join(save_root, \"MedMNIST\", \"Matrices\")\n",
    "tests_root    = os.path.join(save_root, \"MedMNIST\", \"Tests\")\n",
    "os.makedirs(matrices_root, exist_ok=True)\n",
    "os.makedirs(tests_root, exist_ok=True)\n",
    "\n",
    "# helper: run rank sweep on ONE MedMNIST subset\n",
    "def run_rank_sweep(data_flag, ranks, train_epochs=500, lr=1e-3):\n",
    "    \"\"\"Return ([theory avg l2], [AE avg l2]) for the given dataset\n",
    "       and store matrices to pickles for every rank.\"\"\"\n",
    "    info        = INFO[data_flag]\n",
    "    DataClass   = getattr(medmnist, info['python_class'])\n",
    "    train_ds    = DataClass(split='train', download=True)\n",
    "\n",
    "    numSamples  = len(train_ds)\n",
    "    X_raw       = _get_tensor(train_ds, numSamples)\n",
    "    dim         = X_raw[0].numel()\n",
    "\n",
    "    # build data matrix X (not centered)\n",
    "    X = X_raw.view(numSamples, -1).T  # (dim, numSamples)\n",
    "\n",
    "    # compute empirical second moment matrix gammaX\n",
    "    gammaX = 1 / (numSamples) * X @ X.T + 1e-5 * torch.eye(dim, device=device)\n",
    "\n",
    "    # compute Cholesky factor L_X\n",
    "    L_X = torch.linalg.cholesky(gammaX)  # (dim, dim)\n",
    "\n",
    "    # SVD of L_X as in proof\n",
    "    U, S, Vh = torch.linalg.svd(L_X)\n",
    "\n",
    "    theory_err, learned_err = [], []\n",
    "    ds_dir = os.path.join(matrices_root, data_flag)\n",
    "    os.makedirs(ds_dir, exist_ok=True)\n",
    "\n",
    "    for r in ranks:\n",
    "        if r % 100 == 0:\n",
    "            print(f\"On Rank {r}\")\n",
    "\n",
    "        # Truncate SVD to rank-r and construct L_X_r\n",
    "        U_r     = U[:, :r]\n",
    "        S_r     = torch.diag(S[:r])\n",
    "        Vh_r    = Vh[:r, :]\n",
    "        L_X_r   = U_r @ S_r @ Vh_r\n",
    "\n",
    "        # Bayes-optimal projection matrix M_r = L_X_r * pinv(L_X)\n",
    "        M_r = L_X_r @ torch.linalg.pinv(L_X)\n",
    "\n",
    "        # compute optimal reconstruction error\n",
    "        X_opt = M_r @ X\n",
    "        diffs_opt = X_opt - X\n",
    "        avg_l2_opt = torch.norm(diffs_opt, dim=0).mean().item()\n",
    "        theory_err.append(avg_l2_opt)\n",
    "\n",
    "        # linear autoencoder learning\n",
    "        ae = LinearAutoencoder(dim, r).to(device)\n",
    "        opt = optim.Adam(ae.parameters(), lr=lr)\n",
    "\n",
    "        for _ in range(train_epochs):\n",
    "            opt.zero_grad()\n",
    "            out = ae(X.T)\n",
    "            loss = criterion(out, X.T)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            W_enc = ae.encoder.weight.detach().cpu()\n",
    "            W_dec = ae.decoder.weight.detach().cpu()\n",
    "            b_dec = ae.decoder.bias.detach().cpu() if ae.decoder.bias is not None else torch.zeros(dim)\n",
    "            A_learned = W_dec @ W_enc\n",
    "\n",
    "            X_learn = A_learned.to(device) @ X\n",
    "            diffs_learn = X_learn - X\n",
    "            avg_l2_learn = torch.norm(diffs_learn, dim=0).mean().item()\n",
    "        learned_err.append(avg_l2_learn)\n",
    "\n",
    "        # save matrices\n",
    "        dump_path = os.path.join(ds_dir, f\"rank_{r:03d}.pkl\")\n",
    "        with open(dump_path, \"wb\") as f:\n",
    "            pickle.dump(\n",
    "                {\n",
    "                    \"optimalMatrix\": M_r.cpu(),\n",
    "                    \"encoderWeight\": W_enc,\n",
    "                    \"decoderWeight\": W_dec,\n",
    "                },\n",
    "                f,\n",
    "                protocol=pickle.HIGHEST_PROTOCOL,\n",
    "            )\n",
    "\n",
    "    return theory_err, learned_err\n",
    "\n",
    "# run all requested datasets (original sizes of: 200k, 100k, 50k, 10k, 1k)\n",
    "datasets  = ['tissuemnist', 'chestmnist', 'organamnist', 'retinamnist']\n",
    "ranks     = list(range(25, 776, 25))\n",
    "results   = defaultdict(dict)\n",
    "num_epochs = 200 # define here for training / plot naming\n",
    "\n",
    "for flag in datasets:\n",
    "    print(f\"\\n▶ Running rank sweep for {flag} …\")\n",
    "    th, le = run_rank_sweep(flag, ranks, train_epochs=num_epochs)\n",
    "    results[flag]['theory']  = th\n",
    "    results[flag]['learned'] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a41f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined plot\n",
    "plt.figure(figsize=(7.8, 4.7))\n",
    "short_label = {'theory': 'O', 'learned': 'L'}\n",
    "\n",
    "for i, flag in enumerate(datasets):\n",
    "    col = ef_pastel[i % len(ef_pastel)]\n",
    "    for kind in ('theory', 'learned'):\n",
    "        plt.plot(ranks,\n",
    "                 results[flag][kind],\n",
    "                 line_styles[kind],\n",
    "                 marker=markers[kind],\n",
    "                 color=col,\n",
    "                 lw=lw,\n",
    "                 ms=ms,\n",
    "                 label=f\"{flag} ({short_label[kind]})\")\n",
    "\n",
    "\n",
    "plt.xlabel('Rank $r$')\n",
    "plt.ylabel(r'Average $\\ell_2$ Reconstruction Error')\n",
    "plt.yscale('log')\n",
    "#plt.title('Average $\\ell_2$ Error vs. Rank (Affine Linear)')\n",
    "plt.legend(ncol=3, fontsize='small')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save final plot with shortened name including epoch info\n",
    "plot_path = os.path.join(tests_root, f\"classic_ranksweep_{num_epochs}ep.png\")\n",
    "plt.savefig(plot_path)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b58cf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Inspect stored matrices & compare singular values (4 decimals) ─────────\n",
    "#    • Lists datasets / ranks, loads each pickle\n",
    "#    • Computes singular values of\n",
    "#        – Bayes–optimal map  Mr\n",
    "#        – Learned map        A = W_dec @ W_enc\n",
    "#    • Prints top-k singular values (rounded to 4 decimals) and relative ℓ2 error\n",
    "\n",
    "import os, pickle, torch\n",
    "\n",
    "base_dir  = \"ClassicPics/MedMNIST/Matrices\"  # updated save path for Affine Linear sweep\n",
    "top_k     = 10                              # how many singular values to show\n",
    "torch.set_printoptions(edgeitems=3, linewidth=120, sci_mode=False)\n",
    "\n",
    "for dataset in sorted(os.listdir(base_dir)):\n",
    "    ds_dir = os.path.join(base_dir, dataset)\n",
    "    if not os.path.isdir(ds_dir):\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n══════════  {dataset.upper()}  ══════════\")\n",
    "    for fname in sorted(fn for fn in os.listdir(ds_dir) if fn.endswith(\".pkl\")):\n",
    "        rank   = int(fname.split(\"_\")[1].split(\".\")[0])        # rank_050.pkl → 50\n",
    "        fpath  = os.path.join(ds_dir, fname)\n",
    "\n",
    "        # ── load matrices ───────────────────────────────────────────────────\n",
    "        with open(fpath, \"rb\") as f:\n",
    "            mats = pickle.load(f)\n",
    "\n",
    "        Mr   = mats[\"optimalMatrix\"]\n",
    "        Wenc = mats[\"encoderWeight\"]\n",
    "        Wdec = mats[\"decoderWeight\"]\n",
    "        bvec = mats[\"biasVector\"]       # also load biasVector if needed later\n",
    "        A    = Wdec @ Wenc              # learned full map\n",
    "\n",
    "        # ── singular values ────────────────────────────────────────────────\n",
    "        s_opt    = torch.linalg.svdvals(Mr)\n",
    "        s_learn  = torch.linalg.svdvals(A)\n",
    "\n",
    "        rel_err = torch.norm(s_opt - s_learn) / torch.norm(s_opt)\n",
    "\n",
    "        # Round singular values to 4 decimal places for print\n",
    "        s_opt_rounded   = [round(v.item(), 4) for v in s_opt[:top_k]]\n",
    "        s_learn_rounded = [round(v.item(), 4) for v in s_learn[:top_k]]\n",
    "\n",
    "        print(f\"\\n— Rank {rank:3d} —  ({fpath})\")\n",
    "        print(f\"Relative ℓ₂ error on singular values: {rel_err:.3e}\")\n",
    "        print(f\"Top {top_k} σ(M_r):\", s_opt_rounded)\n",
    "        print(f\"Top {top_k} σ(A)  :\", s_learn_rounded)\n",
    "\n",
    "        # Uncomment next lines if you want FULL singular value lists\n",
    "        # print(\"\\nAll σ(M_r):\\n\", s_opt.cpu())\n",
    "        # print(\"\\nAll σ(A):\\n\",   s_learn.cpu())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
