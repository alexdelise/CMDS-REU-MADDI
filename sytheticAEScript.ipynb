{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd1f43e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "import random\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcbf9eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e29fb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# theorem computation\n",
    "def compute_optimal_A_b_mu(X_np, r):\n",
    "    mu = np.mean(X_np, axis=0)\n",
    "    cov = (X_np - mu).T @ (X_np - mu)\n",
    "    U, S, _ = np.linalg.svd(cov)\n",
    "    Ur = U[:, :r]  \n",
    "    \n",
    "    A = Ur @ Ur.T  \n",
    "    b = np.zeros(r)  \n",
    "    \n",
    "    return A, b, mu, Ur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47a682f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plain linear autoencoder \n",
    "class ClassicAffineAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, r):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.r = r\n",
    "        \n",
    "        self.projection = nn.Linear(input_dim, input_dim, bias=True)\n",
    "        \n",
    "        self.decoder = nn.Linear(r, input_dim, bias=True)\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        projected = self.projection(x)  \n",
    "        return projected[:, :self.r]  \n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ac4312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal autoencoder \n",
    "class OptimalAffineAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, r, Ur, mu):\n",
    "        super().__init__()\n",
    "        self.Ur = torch.tensor(Ur, dtype=torch.float32)  \n",
    "        self.mu = torch.tensor(mu, dtype=torch.float32)\n",
    "\n",
    "    def encoder(self, x):\n",
    "        x_centered = x - self.mu\n",
    "        return x_centered @ self.Ur  \n",
    "\n",
    "    def decoder(self, z):\n",
    "        return z @ self.Ur.T + self.mu  \n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c4b31a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nonlinear autoencoder\n",
    "class NonlinearAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, bottleneck_dim, hidden_dim=5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.Linear(hidden_dim, bottleneck_dim)\n",
    "        )\n",
    "        \n",
    "        # decoder:\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(bottleneck_dim, hidden_dim),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e435baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoder training loop\n",
    "def train_autoencoder(model, train_loader, val_loader, num_epochs, lr=1e-3):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon = model(batch)\n",
    "            loss = criterion(recon, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item() * batch.size(0)\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader.dataset)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                recon = model(batch)\n",
    "                loss = criterion(recon, batch)\n",
    "                total_val_loss += loss.item() * batch.size(0)\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "\n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4a2b82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation function only code for the optimal affine autoencoder\n",
    "def valOnlyOptimalAffineAutoencoder(model, val_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = batch.to(device)\n",
    "            recon = model(batch)\n",
    "            loss = criterion(recon, batch)\n",
    "            total_loss += loss.item() * batch.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader.dataset)\n",
    "    return avg_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e5ab3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (2000, 10)\n",
      "\n",
      "Optimal Autoencoder:\n",
      " MSE: 0.00001494 \n",
      "\n",
      "PCA Baseline:\n",
      "MSE: 0.00001562 \n",
      "\n",
      " FACTOR RECOVERY ANALYSIS\n",
      "\n",
      "Per-Factor Correlations:\n",
      "Factor   Optimal   PCA       \n",
      "1        0.8052     0.8057    \n",
      "2        0.1584     0.0886    \n",
      "3        0.1030     0.0613    \n"
     ]
    }
   ],
   "source": [
    "# optimal and PCA and nonlinear models + OPA + MSE and factor analysis \n",
    "X_df = pd.read_csv(\"assetReturns_garch.csv\")\n",
    "X_np = X_df.to_numpy().astype(np.float32)\n",
    "X_tensor = torch.tensor(X_np)\n",
    "\n",
    "print(f\"Data shape: {X_np.shape}\")\n",
    "\n",
    "# set dims and latent space size \n",
    "input_dim = X_np.shape[1]\n",
    "r = 3  \n",
    "\n",
    "# compute the optimal params \n",
    "A, b, mu, Ur = compute_optimal_A_b_mu(X_np, r)\n",
    "\n",
    "\n",
    "results = {\n",
    "    'optimal_mse': [],    \n",
    "    'optimal_factors': [],  \n",
    "}\n",
    "\n",
    "#split \n",
    "n_samples = X_tensor.shape[0]\n",
    "train_size = int(0.8 * n_samples)\n",
    "val_size = n_samples - train_size\n",
    "\n",
    "#slice\n",
    "train_data = X_tensor[:train_size]\n",
    "train_indices = train_data.indices\n",
    "val_data = X_tensor[train_size:]\n",
    "val_indices = np.arange(train_size, len(X_np))\n",
    "val_dates = X_df.index[val_indices]\n",
    "\n",
    "# create the data loaders\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "\n",
    "model_optimal = OptimalAffineAutoencoder(input_dim, 3, Ur, mu)\n",
    "losses_optimal_val = valOnlyOptimalAffineAutoencoder(model_optimal, val_loader) \n",
    "\n",
    "results['optimal_mse'].append(losses_optimal_val)\n",
    "\n",
    "X_train_np = train_data.numpy()\n",
    "X_val_np = val_data.numpy()\n",
    "\n",
    "# load ground truth latent factors and slice val rows\n",
    "F_true_full = pd.read_csv(\"latentFactors_garch.csv\").to_numpy().astype(np.float32)\n",
    "F_true_tensor = torch.tensor(F_true_full[val_indices])\n",
    "\n",
    " # PCA\n",
    "pca = PCA(n_components=r)\n",
    "pca.fit(X_train_np)\n",
    "\n",
    "\n",
    "X_val_recon = pca.inverse_transform(pca.transform(X_val_np))\n",
    "\n",
    "pca_mse = np.mean((X_val_np - X_val_recon) ** 2)\n",
    "    \n",
    "Z_pca = pca.transform(X_val_np)\n",
    "\n",
    "F = F_true_tensor.numpy()\n",
    "R_pca, _ = orthogonal_procrustes(Z_pca, F)\n",
    "Z_pca_aligned = Z_pca @ R_pca\n",
    "\n",
    "\n",
    "corr_pca = np.abs([\n",
    "        np.corrcoef(Z_pca_aligned[:, i], F[:, i])[0, 1]\n",
    "        for i in range(F.shape[1])\n",
    "])\n",
    "    \n",
    "X_val_reconstructed = pca.inverse_transform(Z_pca)\n",
    "mse_pca = np.mean((X_val_np - X_val_reconstructed) ** 2)\n",
    "\n",
    "results.setdefault('pca_mse', []).append(mse_pca)\n",
    "results.setdefault('pca_factors', []).append(corr_pca)\n",
    "\n",
    "\n",
    "val_tensor = X_tensor[val_indices]\n",
    "\n",
    "def aligned_corr(model, X_val, F_true_val):\n",
    "    with torch.no_grad():\n",
    "        Z = model.encoder(X_val).cpu().numpy()\n",
    "        F = F_true_val.cpu().numpy()\n",
    "    R, _ = orthogonal_procrustes(Z, F)\n",
    "    Z_aligned = Z @ R\n",
    "    return np.abs([\n",
    "        np.corrcoef(Z_aligned[:, i], F[:, i])[0, 1]\n",
    "        for i in range(F.shape[1])\n",
    "    ])\n",
    "\n",
    "# compute and store factor correlations\n",
    "results['optimal_factors'].append(\n",
    "    aligned_corr(model_optimal, val_tensor, F_true_tensor)\n",
    ")\n",
    "\n",
    "\n",
    "def print_results_summary(results):\n",
    "\n",
    "    optimal_mse = np.array(results['optimal_mse'])\n",
    "    pca_mse = np.array(results['pca_mse'])  \n",
    "\n",
    "    print(f\"\\nOptimal Autoencoder:\")\n",
    "    print(f\" MSE: {optimal_mse.mean():.8f} \")\n",
    "\n",
    "    print(f\"\\nPCA Baseline:\")\n",
    "    print(f\"MSE: {pca_mse.mean():.8f} \")\n",
    "\n",
    "    \n",
    "# factor analysis results\n",
    "    print(f\"\\n FACTOR RECOVERY ANALYSIS\")\n",
    "\n",
    "    optimal_factors = np.array(results['optimal_factors'])\n",
    "    pca_factors = np.array(results['pca_factors'])  \n",
    "    \n",
    "\n",
    "    print(f\"\\nPer-Factor Correlations:\")\n",
    "    print(f\"{'Factor':<8} {'Optimal':<10}{'PCA':<10}\")\n",
    "    \n",
    "    for i in range(optimal_factors.shape[1]):\n",
    "        factor_optimal = optimal_factors[:, i].mean()\n",
    "        factor_pca = pca_factors[:, i].mean()\n",
    "        print(f\"{i+1:<8} {factor_optimal:<10.4f} {factor_pca:<10.4f}\")\n",
    "\n",
    "print_results_summary(results)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb40b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed run 10/100\n",
      "Completed run 20/100\n",
      "Completed run 30/100\n",
      "Completed run 40/100\n",
      "Completed run 50/100\n",
      "Completed run 60/100\n",
      "Completed run 70/100\n",
      "Completed run 80/100\n",
      "Completed run 90/100\n",
      "Completed run 100/100\n",
      "  Linear Train MSE:    0.00001873 ± 0.00000091\n",
      "  Linear Val MSE:      0.00001948 ± 0.00000110\n",
      "  Nonlinear Train MSE:    0.00002518 ± 0.00000130\n",
      "  Nonlinear Val MSE:      0.00002623 ± 0.00000144\n",
      "\n",
      "Per-Factor Correlations (mean across runs):\n",
      "Factor   Linear       Nonlinear   \n",
      "1        0.4118±0.2126 0.4152+0.2298\n",
      "2        0.1214±0.0569 0.1118+0.0602\n",
      "3        0.0875±0.0551 0.1156+0.0650\n"
     ]
    }
   ],
   "source": [
    "# classic and nonlinear models in 100 run loop + OPA + MSE and factor analysis \n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "results = {\n",
    "    'linear_train_mse': [],\n",
    "    'linear_val_mse': [],\n",
    "    'linear_factors': [],\n",
    "    'linear_analysis': [],\n",
    "    'nonlinear_train_mse': [],\n",
    "    'nonlinear_val_mse': [],\n",
    "    'nonlinear_factors': [],\n",
    "    'nonlinear_analysis': []\n",
    "}\n",
    "\n",
    "val_tensor = X_tensor[val_indices]\n",
    "for run in range(100):\n",
    "   # progress meter\n",
    "    if (run + 1) % 10 == 0:\n",
    "        print(f\"Completed run {run + 1}/100\")\n",
    "    \n",
    "    # init/train model\n",
    "    modellinear = ClassicAffineAutoencoder(input_dim, r).to(device)\n",
    "    modelNonlinear = NonlinearAutoencoder(input_dim, r).to(device)\n",
    "    modellinear, train_losslinear, val_losslinear = train_autoencoder(\n",
    "        modellinear, train_loader, val_loader, num_epochs=150, lr=0.001)\n",
    "    results['linear_train_mse'].append(train_losslinear[-1])  \n",
    "    results['linear_val_mse'].append(val_losslinear[-1])    \n",
    "    modelNonlinear, train_lossNonlinear, val_lossNonlinear = train_autoencoder(\n",
    "        modelNonlinear, train_loader, val_loader, num_epochs=100, lr=0.001) \n",
    "    results['nonlinear_train_mse'].append(train_lossNonlinear[-1])  \n",
    "    results['nonlinear_val_mse'].append(val_lossNonlinear[-1]) \n",
    "    \n",
    "\n",
    "    linear_factors = modellinear.encoder(val_data).detach().cpu().numpy()\n",
    "    results['linear_factors'].append(linear_factors)\n",
    "\n",
    "    nonlinear_factors = modelNonlinear.encoder(val_data).detach().cpu().numpy()\n",
    "    results['nonlinear_factors'].append(nonlinear_factors)\n",
    "    \n",
    "    \n",
    "    # compute and store factor correlations \n",
    "    results['linear_analysis'].append(\n",
    "        aligned_corr(modellinear, val_tensor, F_true_tensor)\n",
    "    )\n",
    "\n",
    "    results['nonlinear_analysis'].append(\n",
    "        aligned_corr(modelNonlinear, val_tensor, F_true_tensor)\n",
    "    )\n",
    "\n",
    "print(f\"  Linear Train MSE:    {np.mean(results['linear_train_mse']):.8f} ± {np.std(results['linear_train_mse']):.8f}\")\n",
    "print(f\"  Linear Val MSE:      {np.mean(results['linear_val_mse']):.8f} ± {np.std(results['linear_val_mse']):.8f}\")\n",
    "print(f\"  Nonlinear Train MSE:    {np.mean(results['nonlinear_train_mse']):.8f} ± {np.std(results['nonlinear_train_mse']):.8f}\")\n",
    "print(f\"  Nonlinear Val MSE:      {np.mean(results['nonlinear_val_mse']):.8f} ± {np.std(results['nonlinear_val_mse']):.8f}\")\n",
    "\n",
    "\n",
    "def print_results_summary(results):\n",
    "    # factor analysis results\n",
    "    linear_factors = np.array(results['linear_analysis'])  \n",
    "    nonlinear_factors = np.array(results['nonlinear_analysis'])\n",
    "     \n",
    "    \n",
    "    print(f\"\\nPer-Factor Correlations (mean across runs):\")\n",
    "    print(f\"{'Factor':<8} {'Linear':<12} {'Nonlinear':<12}\")\n",
    "    \n",
    "    for i in range(linear_factors.shape[1]):\n",
    "        factor_linear_mean = linear_factors[:, i].mean()\n",
    "        factor_linear_std = linear_factors[:, i].std()\n",
    "        factor_nonlinear_mean = nonlinear_factors[:,i].mean()\n",
    "        factor_nonlinear_std = nonlinear_factors[:,i].std()\n",
    "\n",
    "        print(f\"{i+1:<8} {factor_linear_mean:.4f}±{factor_linear_std:.4f} {factor_nonlinear_mean:.4f}+{factor_nonlinear_std:.4f}\")\n",
    "\n",
    "print_results_summary(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
