{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Optimal AutoEncoders (Linear, Afine Linear, Noisy Linear) for CT Scan Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import tifffile as tiff\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy as sp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a class for the dataset (according to pyTorch format) \n",
    "\n",
    "class CTScanDataset(Dataset):\n",
    "    def __init__(self, img_dir, length, mode, transform=None):\n",
    "        self.mode = mode\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "       # self.target_transform = target_transform\n",
    "        self.len = length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        slice_name = f\"slice{idx:05d}\"\n",
    "        img_path = os.path.join(self.img_dir, slice_name, self.mode, 'reconstruction.tif')\n",
    "        img_np = tiff.imread(img_path).astype(np.float32)\n",
    "        img_tensor = torch.from_numpy(img_np)\n",
    "        if self.transform:\n",
    "            img_tensor = self.transform(img_tensor)\n",
    "        return img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "img_dir = r'C:\\Users\\alexr\\OneDrive\\CMDS REU\\Data\\CT-Scan-6.9GB'\n",
    "mode = 'mode1'\n",
    "length = 1000\n",
    "dataset = CTScanDataset(img_dir, length, mode)\n",
    "\n",
    "# grab samples from the dataset\n",
    "flatten = True\n",
    "num_samples = 10\n",
    "X_raw = torch.stack([dataset[i] for i in range(1,num_samples)]) # grab numSamples images from MNIST -> shape of (1000, 1, 28, 28)\n",
    "X = X_raw.view(num_samples-1, -1) if flatten else X_raw\n",
    "\n",
    "r = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal AutoEncoder with Linear Map, assuming noiseless data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 4.00 TiB for an array with shape (1048576, 1048576) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m U, Sigma, V_t \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39msvd(X_np, full_matrices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# SVD of X\u001b[39;00m\n\u001b[0;32m      4\u001b[0m U_r \u001b[38;5;241m=\u001b[39m U[:, :r] \u001b[38;5;241m/\u001b[39m num_samples\n\u001b[1;32m----> 5\u001b[0m A \u001b[38;5;241m=\u001b[39m \u001b[43mU_r\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mU_r\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m''' reconstructed_X_sampleMean = A @ X_np\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03mdiff_sampleMean = (reconstructed_X_sampleMean - X_np) / sp.linalg.norm(reconstructed_X_sampleMean, ord='fro') # normalized error\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03merror_sampleMean = np.mean(sp.linalg.norm(diff_sampleMean, axis=1, ord=2)**2) '''\u001b[39;00m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 4.00 TiB for an array with shape (1048576, 1048576) and data type float32"
     ]
    }
   ],
   "source": [
    "# sample mean estimation method (same as SVD of X)\n",
    "X_np = X.detach().numpy().T\n",
    "U, Sigma, V_t = np.linalg.svd(X_np, full_matrices=False)  # SVD of X\n",
    "U_r = U[:, :r] / num_samples\n",
    "A = U_r @ U_r.T\n",
    "\n",
    "''' reconstructed_X_sampleMean = A @ X_np\n",
    "diff_sampleMean = (reconstructed_X_sampleMean - X_np) / sp.linalg.norm(reconstructed_X_sampleMean, ord='fro') # normalized error\n",
    "error_sampleMean = np.mean(sp.linalg.norm(diff_sampleMean, axis=1, ord=2)**2) '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second moment estimation method (SVD of L_x)\n",
    "\n",
    "gamma_X = (X_np @ X_np.T) # unbiased second moment\n",
    "gamma_X += 1e-5*np.eye(gamma_X.shape[0]) # add the regularization term to ensure SPD\n",
    "L_x = np.linalg.cholesky(gamma_X) # Cholesky decomposition\n",
    "\n",
    "U, Sigma, V_t = np.linalg.svd(L_x) # SVD\n",
    "U_r = U[:, :r] # first r left singular vectors of L_X\n",
    "A = U_r @ U_r.T # Echard-Young solution to the transformed minimization problem\n",
    "\n",
    "reconstructed_X_covariance = A @ X_np\n",
    "diff_covariance = (reconstructed_X_covariance - X_np) / sp.linalg.norm(reconstructed_X_sampleMean, ord='fro') # normalized error\n",
    "error_covariance = np.mean(sp.linalg.norm(diff_sampleMean, axis=1, ord=2)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal AutoEncoder with Afine Linear Map, assuming noiseless data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covariance estimation method \n",
    "\n",
    "Sigma_X = np.cov(X_np)\n",
    "mu_X = np.mean(X_np.T, axis=0)\n",
    "Sigma_X += 1e-5 * np.eye(Sigma_X.shape[0]) # add a regularization term to ensure SPD\n",
    "K_x = np.linalg.cholesky(Sigma_X)\n",
    "U, D, V_t = np.linalg.svd(K_x)\n",
    "U_r = U[:, :r]\n",
    "A = U_r @ U_r.T\n",
    "b = (np.eye(A.shape[0]) - A) @ mu_X\n",
    "b = np.tile(b[:, np.newaxis], (1, 999))\n",
    "\n",
    "reconstructed_X_afine = A @ X_np + b\n",
    "diff_afine = (reconstructed_X_afine - X_np) / sp.linalg.norm(reconstructed_X_sampleMean, ord='fro') # normalized error\n",
    "error_afine = np.mean(sp.linalg.norm(diff_afine, axis=1, ord=2)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal AutoEncoder with Linear Map, assuming Noisy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume that the noise is Gaussian, represented by eps. \n",
    "noise_amp = 1e-3\n",
    "eps = noise_amp * np.random.randn(X_np.shape[0], X_np.shape[1]) # generate noise with the same shape as X_np\n",
    "X_true = X_np - eps # approximate the true image by subtracting the noise\n",
    "\n",
    "gamma_X = (X_np @ X_np.T) # unbiased second moment\n",
    "gamma_X += 1e-5*np.eye(gamma_X.shape[0]) # add the regularization term to ensure SPD\n",
    "gamma_eps = (eps @ eps.T) + (noise_amp/2) # calculate gamma_eps\n",
    "\n",
    "M = gamma_X @ np.linalg.inv(gamma_X @ gamma_eps)\n",
    "U, Sigma, V_t = np.linalg.svd(M)\n",
    "U_r = U[: , :r]\n",
    "A = U_r @ U_r.T\n",
    "\n",
    "reconstructed_X_noisy = A @ X_np \n",
    "diff_noisy = (reconstructed_X_noisy - X_np) / sp.linalg.norm(reconstructed_X_sampleMean, ord='fro') # normalized error\n",
    "error_noisy = np.mean(sp.linalg.norm(diff_noisy, axis=1, ord=2)**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualizing Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5, 5)\n",
    "size = (1024, 1024) # common size for CT scan images\n",
    "\n",
    "for i in range(5):\n",
    "    img_original = X_np[:, i]\n",
    "    img_linear_sampleMean = reconstructed_X_sampleMean[:, i]\n",
    "   ''' img_linear_covariance = reconstructed_X_covariance[:, i]\n",
    "    img_noisy = reconstructed_X_noisy[:, i]\n",
    "    img_afine = reconstructed_X_afine[:, i] '''\n",
    "\n",
    "    ax[0,i].imshow(img_original.reshape(size), cmap='gray')\n",
    "    ax[0,i].axis('off')\n",
    "    ax[1,i].imshow(img_linear_sampleMean.reshape(size), cmap='gray')\n",
    "    ax[1,i].axis('off')\n",
    "   ''' ax[2,i].imshow(img_linear_covariance.reshape(size), cmap='gray')\n",
    "    ax[2,i].axis('off')\n",
    "    ax[3,i].imshow(img_noisy.reshape(size), cmap='gray')\n",
    "    ax[3,i].axis('off')\n",
    "    ax[4,i].imshow(img_afine.reshape(size), cmap='gray')\n",
    "    ax[4,i].axis('off') '''\n",
    "\n",
    "print(f'Error with Linear Map, sample mean Estimation: {error_sampleMean}')\n",
    "print(f'Error with Linear Map, second moment Estimation: {error_covariance}')\n",
    "print(f'Error with Affine Linear Map: {error_afine}')\n",
    "print(f'Error with Linear Map, nosiy data: {error_noisy}')\n",
    "# plt.tight_layout()\n",
    "plt.show()\n",
    "fig.set_size_inches(15, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
