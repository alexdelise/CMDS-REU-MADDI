{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd1f43e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from scipy import stats\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e29fb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_optimal_A_b_mu(X_np, r):\n",
    "    mu = np.mean(X_np, axis=0)\n",
    "    cov = (X_np - mu).T @ (X_np - mu)\n",
    "    U, S, _ = np.linalg.svd(cov)\n",
    "    Ur = U[:, :r]  # shape (d, r)\n",
    "    \n",
    "    A = Ur.T  # encoder projects from d->r: (r, d)\n",
    "    b = np.zeros(r)  # no bias needed in latent space\n",
    "    \n",
    "    return A, b, mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47a682f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python packaged affine autoencoder\n",
    "\n",
    "class ClassicAffineAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, bottleneck_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(input_dim, bottleneck_dim, bias=True)\n",
    "        self.decoder = nn.Linear(bottleneck_dim, input_dim, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ac4312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimalAffineAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, r, A, b, mu):\n",
    "        super().__init__()\n",
    "        self.A = torch.tensor(A, dtype=torch.float32)\n",
    "        self.b = torch.tensor(b, dtype=torch.float32)\n",
    "        self.mu = torch.tensor(mu, dtype=torch.float32)\n",
    "\n",
    "    def encoder(self, x):\n",
    "        # Return centered projected latent space: Ur^T(x - mu)\n",
    "        x_centered = x - self.mu  # center by mean\n",
    "        UrT = self.A.T  # A = Ur Ur^T ⇒ A.T = Ur^T Ur ⇒ encoder ~ Ur^T\n",
    "        return x_centered @ UrT  # [batch_size, d] x [d, r] → [batch_size, r]\n",
    "    \n",
    "    def decoder(self, z):\n",
    "        # Reconstruct from latent space: Ur * z + mu\n",
    "        return z @ self.A + self.mu  # [batch_size, r] x [r, d] → [batch_size, d]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Complete encode-decode cycle\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "640f5f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a combination modeel that allows for training after instantiation of optimal theoretical weights. \n",
    "class OptimalTrainableAffineAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, r, A_init, b_init, mu_init):\n",
    "        super().__init__()\n",
    "        self.A = nn.Parameter(torch.tensor(A_init, dtype=torch.float32))  # (r, d)\n",
    "        self.b = nn.Parameter(torch.tensor(b_init, dtype=torch.float32))  # (r,)\n",
    "        self.mu = nn.Parameter(torch.tensor(mu_init, dtype=torch.float32))  # (d,)\n",
    "\n",
    "    def encoder(self, x):\n",
    "        x_centered = x - self.mu  # (batch, d)\n",
    "        return x_centered @ self.A.T + self.b  # (batch, r)\n",
    "\n",
    "    def decoder(self, z):\n",
    "        return z @ self.A + self.mu  # (batch, d)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e435baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 3. Modify train_autoencoder to accept loaders instead of raw tensor\n",
    "def train_autoencoder(model, train_loader, val_loader, num_epochs=300, lr=1e-3):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon = model(batch)\n",
    "            loss = criterion(recon, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item() * batch.size(0)\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader.dataset)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                recon = model(batch)\n",
    "                loss = criterion(recon, batch)\n",
    "                total_val_loss += loss.item() * batch.size(0)\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "\n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4a2b82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation function only code for the optimal affine autoencoder\n",
    "def valOnlyOptimalAffineAutoencoder(model, val_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = batch.to(device)\n",
    "            recon = model(batch)\n",
    "            loss = criterion(recon, batch)\n",
    "            total_loss += loss.item() * batch.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader.dataset)\n",
    "    return avg_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c95e1648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (2000, 10)\n",
      "Starting 100-run experiment...\n",
      "Run 1/100Run 2/100Run 3/100Run 4/100Run 5/100Run 6/100Run 7/100Run 8/100Run 9/100Run 10/100Run 11/100Run 12/100Run 13/100Run 14/100Run 15/100Run 16/100Run 17/100Run 18/100Run 19/100Run 20/100Run 21/100Run 22/100Run 23/100Run 24/100Run 25/100Run 26/100Run 27/100Run 28/100Run 29/100Run 30/100Run 31/100Run 32/100Run 33/100Run 34/100Run 35/100Run 36/100Run 37/100Run 38/100Run 39/100Run 40/100Run 41/100Run 42/100Run 43/100Run 44/100Run 45/100Run 46/100Run 47/100Run 48/100Run 49/100Run 50/100Run 51/100Run 52/100Run 53/100Run 54/100Run 55/100Run 56/100Run 57/100Run 58/100Run 59/100Run 60/100Run 61/100Run 62/100Run 63/100Run 64/100Run 65/100Run 66/100Run 67/100Run 68/100Run 69/100Run 70/100Run 71/100Run 72/100Run 73/100Run 74/100Run 75/100Run 76/100Run 77/100Run 78/100Run 79/100Run 80/100Run 81/100Run 82/100Run 83/100Run 84/100Run 85/100Run 86/100Run 87/100Run 88/100Run 89/100Run 90/100Run 91/100Run 92/100Run 93/100Run 94/100Run 95/100Run 96/100Run 97/100Run 98/100Run 99/100Run 100/100"
     ]
    }
   ],
   "source": [
    "# get data ready\n",
    "X_df = pd.read_csv(\"assetReturns_garch.csv\")\n",
    "X_np = X_df.to_numpy().astype(np.float32)\n",
    "X_tensor = torch.tensor(X_np)\n",
    "\n",
    "print(f\"Data shape: {X_np.shape}\")\n",
    "\n",
    "# set dims and latent space size \n",
    "input_dim = X_np.shape[1]\n",
    "r = 3  # bottleneck dimension\n",
    "\n",
    "# compute the optimal params (only once, outside the loop)\n",
    "A, b, mu = compute_optimal_A_b_mu(X_np, r)\n",
    "\n",
    "# Initialize results storage\n",
    "results = {\n",
    "    'classic_mse': [],\n",
    "    'optimal_mse': [],\n",
    "    'trainable_mse': [],\n",
    "    'classic_train_loss': [],\n",
    "    'classic_val_loss': [],\n",
    "    'trainable_train_loss': [],\n",
    "    'trainable_val_loss': [],\n",
    "    'classic_factors': [],\n",
    "    'optimal_factors': [],\n",
    "    'trainable_factors': [],\n",
    "    'classic_train_histories': [],\n",
    "    'trainable_train_histories': [],\n",
    "    'classic_val_histories': [],\n",
    "    'trainable_val_histories': []\n",
    "}\n",
    "\n",
    "print(\"Starting 100-run experiment...\")\n",
    "\n",
    "# Run 100 experiments\n",
    "for run in range(100):\n",
    "    print(f\"Run {run + 1}/100\", end=\"\")\n",
    "    \n",
    "    # Set different seed for each run\n",
    "    torch.manual_seed(run)\n",
    "    np.random.seed(run)\n",
    "    \n",
    "    # split data into training and validation with current seed\n",
    "    val_split = 0.2\n",
    "    total_size = len(X_tensor)\n",
    "    val_size = int(val_split * total_size)\n",
    "    train_size = total_size - val_size\n",
    "    \n",
    "    train_data, val_data = random_split(X_tensor, [train_size, val_size])\n",
    "    \n",
    "    # create the data loaders \n",
    "    batch_size = 64\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # initialize models for this run\n",
    "    model_classic = ClassicAffineAutoencoder(input_dim, r)\n",
    "    model_optimal = OptimalAffineAutoencoder(input_dim, r, A, b, mu)\n",
    "    model_optimal_trainable = OptimalTrainableAffineAutoencoder(input_dim, r, A, b, mu)\n",
    "    \n",
    "    # Train classic model\n",
    "    model_classic, losses_classic_train, losses_classic_val = train_autoencoder(\n",
    "        model_classic, \n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        70,\n",
    "        0.001\n",
    "    )\n",
    "    \n",
    "    # Validation only on optimal model\n",
    "    losses_optimal_val = valOnlyOptimalAffineAutoencoder(model_optimal, val_loader)\n",
    "    \n",
    "    # Train optimal trainable model\n",
    "    model_optimal_trainable, losses_trainable_train, losses_trainable_val = train_autoencoder(\n",
    "        model_optimal_trainable,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        20,\n",
    "        0.001\n",
    "    )\n",
    "    \n",
    "    # Store results for this run\n",
    "    results['classic_mse'].append(min(losses_classic_val))\n",
    "    results['optimal_mse'].append(losses_optimal_val)\n",
    "    results['trainable_mse'].append(min(losses_trainable_val))\n",
    "\n",
    "    results['classic_train_loss'].append(losses_classic_train[-1])\n",
    "    results['classic_val_loss'].append(losses_classic_val[-1])\n",
    "    results['trainable_train_loss'].append(losses_trainable_train[-1])\n",
    "    results['trainable_val_loss'].append(losses_trainable_val[-1])\n",
    "    \n",
    "    # Store full training histories\n",
    "    results['classic_train_histories'].append(losses_classic_train)\n",
    "    results['classic_val_histories'].append(losses_classic_val)\n",
    "    results['trainable_train_histories'].append(losses_trainable_train)\n",
    "    results['trainable_val_histories'].append(losses_trainable_val)\n",
    "\n",
    "        # --------------------------------------------\n",
    "    # PCA BASELINE: Fit on training data, evaluate on val\n",
    "    # --------------------------------------------\n",
    "\n",
    "    # Fit PCA on training data (NumPy)\n",
    "    X_train_np = X_tensor[train_data.indices].numpy()\n",
    "    X_val_np = val_tensor.numpy()  # already selected earlier\n",
    "\n",
    "    pca = PCA(n_components=r)\n",
    "    pca.fit(X_train_np)\n",
    "\n",
    "    # Transform validation data\n",
    "    Z_pca = pca.transform(X_val_np)\n",
    "\n",
    "    # Align to true latent factors using Procrustes\n",
    "    F = F_true_tensor.numpy()\n",
    "    R_pca, _ = orthogonal_procrustes(Z_pca, F)\n",
    "    Z_pca_aligned = Z_pca @ R_pca\n",
    "\n",
    "    # Compute correlation per factor\n",
    "    corr_pca = np.abs([\n",
    "        np.corrcoef(Z_pca_aligned[:, i], F[:, i])[0, 1]\n",
    "        for i in range(F.shape[1])\n",
    "    ])\n",
    "    \n",
    "    # Compute reconstruction MSE on validation data\n",
    "    X_val_reconstructed = pca.inverse_transform(Z_pca)\n",
    "    mse_pca = np.mean((X_val_np - X_val_reconstructed) ** 2)\n",
    "\n",
    "    # Store results\n",
    "    results.setdefault('pca_mse', []).append(mse_pca)\n",
    "    results.setdefault('pca_factors', []).append(corr_pca)\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # FACTOR ANALYSIS: Procrustes-aligned correlations\n",
    "    # --------------------------------------------\n",
    "\n",
    "    # Load ground truth latent factors (full set) and slice val rows\n",
    "    F_true_full = pd.read_csv(\"latentFactors_garch.csv\").to_numpy().astype(np.float32)\n",
    "    F_true_tensor = torch.tensor(F_true_full[val_data.indices])\n",
    "\n",
    "    # Get validation data as tensor\n",
    "    val_tensor = X_tensor[val_data.indices]\n",
    "\n",
    "    # Helper function to align & compute correlation\n",
    "    def aligned_corr(model, X_val, F_true_val):\n",
    "        with torch.no_grad():\n",
    "            Z = model.encoder(X_val).cpu().numpy()\n",
    "            F = F_true_val.cpu().numpy()\n",
    "        R, _ = orthogonal_procrustes(Z, F)\n",
    "        Z_aligned = Z @ R\n",
    "        return np.abs([\n",
    "            np.corrcoef(Z_aligned[:, i], F[:, i])[0, 1]\n",
    "            for i in range(F.shape[1])\n",
    "        ])\n",
    "\n",
    "    # Compute and store factor correlations\n",
    "    results['classic_factors'].append(\n",
    "        aligned_corr(model_classic, val_tensor, F_true_tensor)\n",
    "    )\n",
    "    results['optimal_factors'].append(\n",
    "        aligned_corr(model_optimal, val_tensor, F_true_tensor)\n",
    "    )\n",
    "    results['trainable_factors'].append(\n",
    "        aligned_corr(model_optimal_trainable, val_tensor, F_true_tensor)\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18e5ab3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " VALIDATION MSE RESULTS\n",
      "Classic Autoencoder:\n",
      "  Mean MSE: 1.409329 ± 0.046787\n",
      "  Median:   1.401503\n",
      "  Min/Max:  1.322890  1.604322\n",
      "\n",
      "Optimal Autoencoder:\n",
      "  Mean MSE: 1.381532 ± 0.033371\n",
      "  Median:   1.380051\n",
      "  Min/Max:  1.304812  1.493806\n",
      "\n",
      "Trainable Optimal Autoencoder:\n",
      "  Mean MSE: 1.383313 ± 0.033396\n",
      "  Median:   1.381695\n",
      "  Min/Max:  1.307124  1.495925\n",
      "\n",
      "PCA Baseline:\n",
      "  Mean MSE: 1.374383 ± 0.093616\n",
      "  Median:   1.381638\n",
      "  Min/Max:  0.503196  1.493646\n",
      "\n",
      " PERFORMANCE COMPARISON\n",
      "Optimal vs Classic:     +1.97% improvement\n",
      "Trainable vs Classic:   +1.85% improvement\n",
      "Trainable vs Optimal:   -0.13% improvement\n",
      "PCA vs Classic:         +2.48% improvement\n",
      "PCA vs Optimal:         +0.52% improvement\n",
      "PCA vs Trainable:       +0.65% improvement\n",
      "\n",
      " FINAL TRAINING/VALIDATION LOSSES\n",
      "Classic Autoencoder:\n",
      "  Final Train Loss: 1.397694 ± 0.036883\n",
      "  Final Val Loss:   1.409364 ± 0.046766\n",
      "  Train/Val Ratio:  0.992\n",
      "\n",
      "Trainable Optimal Autoencoder:\n",
      "  Final Train Loss: 1.382268 ± 0.008342\n",
      "  Final Val Loss:   1.391408 ± 0.033445\n",
      "  Train/Val Ratio:  0.993\n",
      "\n",
      " FACTOR RECOVERY ANALYSIS\n",
      "Factor Correlations (mean ± std across all factors and runs):\n",
      "  Classic:           0.7313 ± 0.0853\n",
      "  Optimal:           0.7492 ± 0.0577\n",
      "  Trainable Optimal: 0.7491 ± 0.0578\n",
      "  PCA:               0.7490 ± 0.0579\n",
      "\n",
      "Per-Factor Correlations:\n",
      "Factor   Classic    Optimal    Trainable  PCA       \n",
      "1        0.7924     0.8013     0.8012     0.8012    \n",
      "2        0.7638     0.7730     0.7729     0.7727    \n",
      "3        0.6378     0.6734     0.6731     0.6729    \n",
      "\n",
      " SUMMARY TABLE\n",
      "       Model  MSE_Mean   MSE_Std  Factor_Corr_Mean  Factor_Corr_Std\n",
      "0    Classic  1.409329  0.046787          0.731322         0.085325\n",
      "1    Optimal  1.381532  0.033371          0.749238         0.057676\n",
      "2  Trainable  1.383313  0.033396          0.749066         0.057789\n",
      "3        PCA  1.374383  0.093616          0.748955         0.057861\n"
     ]
    }
   ],
   "source": [
    "def print_results_summary(results):\n",
    "    # MSE Results (Validation Loss)\n",
    "    print(\"\\n VALIDATION MSE RESULTS\")\n",
    "    \n",
    "    classic_mse = np.array(results['classic_mse'])\n",
    "    optimal_mse = np.array(results['optimal_mse'])\n",
    "    trainable_mse = np.array(results['trainable_mse'])\n",
    "    pca_mse = np.array(results['pca_mse'])  # NEW\n",
    "\n",
    "    print(f\"Classic Autoencoder:\")\n",
    "    print(f\"  Mean MSE: {classic_mse.mean():.6f} ± {classic_mse.std():.6f}\")\n",
    "    print(f\"  Median:   {np.median(classic_mse):.6f}\")\n",
    "    print(f\"  Min/Max:  {classic_mse.min():.6f}  {classic_mse.max():.6f}\")\n",
    "    \n",
    "    print(f\"\\nOptimal Autoencoder:\")\n",
    "    print(f\"  Mean MSE: {optimal_mse.mean():.6f} ± {optimal_mse.std():.6f}\")\n",
    "    print(f\"  Median:   {np.median(optimal_mse):.6f}\")\n",
    "    print(f\"  Min/Max:  {optimal_mse.min():.6f}  {optimal_mse.max():.6f}\")\n",
    "    \n",
    "    print(f\"\\nTrainable Optimal Autoencoder:\")\n",
    "    print(f\"  Mean MSE: {trainable_mse.mean():.6f} ± {trainable_mse.std():.6f}\")\n",
    "    print(f\"  Median:   {np.median(trainable_mse):.6f}\")\n",
    "    print(f\"  Min/Max:  {trainable_mse.min():.6f}  {trainable_mse.max():.6f}\")\n",
    "\n",
    "    print(f\"\\nPCA Baseline:\")\n",
    "    print(f\"  Mean MSE: {pca_mse.mean():.6f} ± {pca_mse.std():.6f}\")\n",
    "    print(f\"  Median:   {np.median(pca_mse):.6f}\")\n",
    "    print(f\"  Min/Max:  {pca_mse.min():.6f}  {pca_mse.max():.6f}\")\n",
    "    \n",
    "    # Performance Comparison\n",
    "    print(f\"\\n PERFORMANCE COMPARISON\")\n",
    "    \n",
    "    classic_vs_optimal = (classic_mse.mean() - optimal_mse.mean()) / classic_mse.mean() * 100\n",
    "    classic_vs_trainable = (classic_mse.mean() - trainable_mse.mean()) / classic_mse.mean() * 100\n",
    "    optimal_vs_trainable = (optimal_mse.mean() - trainable_mse.mean()) / optimal_mse.mean() * 100\n",
    "    pca_vs_classic = (classic_mse.mean() - pca_mse.mean()) / classic_mse.mean() * 100\n",
    "    pca_vs_optimal = (optimal_mse.mean() - pca_mse.mean()) / optimal_mse.mean() * 100\n",
    "    pca_vs_trainable = (trainable_mse.mean() - pca_mse.mean()) / trainable_mse.mean() * 100\n",
    "    \n",
    "    print(f\"Optimal vs Classic:     {classic_vs_optimal:+.2f}% improvement\")\n",
    "    print(f\"Trainable vs Classic:   {classic_vs_trainable:+.2f}% improvement\")\n",
    "    print(f\"Trainable vs Optimal:   {optimal_vs_trainable:+.2f}% improvement\")\n",
    "    print(f\"PCA vs Classic:         {pca_vs_classic:+.2f}% improvement\")\n",
    "    print(f\"PCA vs Optimal:         {pca_vs_optimal:+.2f}% improvement\")\n",
    "    print(f\"PCA vs Trainable:       {pca_vs_trainable:+.2f}% improvement\")\n",
    "    \n",
    "    # Final Training/Validation Losses\n",
    "    print(f\"\\n FINAL TRAINING/VALIDATION LOSSES\")\n",
    "    \n",
    "    classic_train_final = np.array(results['classic_train_loss'])\n",
    "    classic_val_final = np.array(results['classic_val_loss'])\n",
    "    trainable_train_final = np.array(results['trainable_train_loss'])\n",
    "    trainable_val_final = np.array(results['trainable_val_loss'])\n",
    "    \n",
    "    print(f\"Classic Autoencoder:\")\n",
    "    print(f\"  Final Train Loss: {classic_train_final.mean():.6f} ± {classic_train_final.std():.6f}\")\n",
    "    print(f\"  Final Val Loss:   {classic_val_final.mean():.6f} ± {classic_val_final.std():.6f}\")\n",
    "    print(f\"  Train/Val Ratio:  {(classic_train_final.mean()/classic_val_final.mean()):.3f}\")\n",
    "    \n",
    "    print(f\"\\nTrainable Optimal Autoencoder:\")\n",
    "    print(f\"  Final Train Loss: {trainable_train_final.mean():.6f} ± {trainable_train_final.std():.6f}\")\n",
    "    print(f\"  Final Val Loss:   {trainable_val_final.mean():.6f} ± {trainable_val_final.std():.6f}\")\n",
    "    print(f\"  Train/Val Ratio:  {(trainable_train_final.mean()/trainable_val_final.mean()):.3f}\")\n",
    "    \n",
    "    # Factor Analysis Results\n",
    "    print(f\"\\n FACTOR RECOVERY ANALYSIS\")\n",
    "\n",
    "    classic_factors = np.array(results['classic_factors'])\n",
    "    optimal_factors = np.array(results['optimal_factors'])\n",
    "    trainable_factors = np.array(results['trainable_factors'])\n",
    "    pca_factors = np.array(results['pca_factors'])  # NEW\n",
    "    \n",
    "    print(f\"Factor Correlations (mean ± std across all factors and runs):\")\n",
    "    print(f\"  Classic:           {classic_factors.mean():.4f} ± {classic_factors.std():.4f}\")\n",
    "    print(f\"  Optimal:           {optimal_factors.mean():.4f} ± {optimal_factors.std():.4f}\")\n",
    "    print(f\"  Trainable Optimal: {trainable_factors.mean():.4f} ± {trainable_factors.std():.4f}\")\n",
    "    print(f\"  PCA:               {pca_factors.mean():.4f} ± {pca_factors.std():.4f}\")\n",
    "    \n",
    "    print(f\"\\nPer-Factor Correlations:\")\n",
    "    print(f\"{'Factor':<8} {'Classic':<10} {'Optimal':<10} {'Trainable':<10} {'PCA':<10}\")\n",
    "    \n",
    "    for i in range(classic_factors.shape[1]):\n",
    "        factor_classic = classic_factors[:, i].mean()\n",
    "        factor_optimal = optimal_factors[:, i].mean()\n",
    "        factor_trainable = trainable_factors[:, i].mean()\n",
    "        factor_pca = pca_factors[:, i].mean()\n",
    "        print(f\"{i+1:<8} {factor_classic:<10.4f} {factor_optimal:<10.4f} {factor_trainable:<10.4f} {factor_pca:<10.4f}\")\n",
    "     \n",
    "    # Factor correlation tests\n",
    "    classic_factors_flat = classic_factors.flatten()\n",
    "    optimal_factors_flat = optimal_factors.flatten()\n",
    "    trainable_factors_flat = trainable_factors.flatten()\n",
    "    pca_factors_flat = pca_factors.flatten()\n",
    " \n",
    "    # Summary Table\n",
    "    print(f\"\\n SUMMARY TABLE\")\n",
    "    \n",
    "    summary_df = pd.DataFrame({\n",
    "        'Model': ['Classic', 'Optimal', 'Trainable', 'PCA'],\n",
    "        'MSE_Mean': [\n",
    "            classic_mse.mean(), \n",
    "            optimal_mse.mean(), \n",
    "            trainable_mse.mean(), \n",
    "            pca_mse.mean()\n",
    "        ],\n",
    "        'MSE_Std': [\n",
    "            classic_mse.std(), \n",
    "            optimal_mse.std(), \n",
    "            trainable_mse.std(), \n",
    "            pca_mse.std()\n",
    "        ],\n",
    "        'Factor_Corr_Mean': [\n",
    "            classic_factors.mean(), \n",
    "            optimal_factors.mean(), \n",
    "            trainable_factors.mean(),\n",
    "            pca_factors.mean()\n",
    "        ],\n",
    "        'Factor_Corr_Std': [\n",
    "            classic_factors.std(), \n",
    "            optimal_factors.std(), \n",
    "            trainable_factors.std(),\n",
    "            pca_factors.std()\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    print(summary_df.round(6))\n",
    "print_results_summary(results)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e661e4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "\n",
    "\n",
    "def compare_latent_factors(model, X_tensor, F_true_path, val_indices=None):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        Z_learned = model.encoder(X_tensor).detach().cpu().numpy()\n",
    "    Z_true_full = pd.read_csv(F_true_path).to_numpy()\n",
    "    \n",
    "    if val_indices is not None:\n",
    "        Z_true = Z_true_full[val_indices, :]\n",
    "    else:\n",
    "        Z_true = Z_true_full\n",
    "    \n",
    "    min_samples = min(Z_learned.shape[0], Z_true.shape[0])\n",
    "    min_factors = min(Z_learned.shape[1], Z_true.shape[1])\n",
    "    \n",
    "    Z_learned = Z_learned[:min_samples, :min_factors]\n",
    "    Z_true = Z_true[:min_samples, :min_factors]\n",
    "    \n",
    "    Z_learned_centered = Z_learned - Z_learned.mean(axis=0)\n",
    "    Z_true_centered = Z_true - Z_true.mean(axis=0)\n",
    "    \n",
    "    R, _ = orthogonal_procrustes(Z_learned_centered, Z_true_centered)\n",
    "    Z_aligned = Z_learned_centered @ R\n",
    "    \n",
    "    print(\"Latent Factor Correlations (after Procrustes alignment):\")\n",
    "    correlations = []\n",
    "    for i in range(Z_aligned.shape[1]):\n",
    "        corr, _ = pearsonr(Z_aligned[:, i], Z_true_centered[:, i])\n",
    "        correlations.append(corr)\n",
    "        print(f\"  Factor {i+1}: correlation = {corr:.4f}\")\n",
    "    \n",
    "    return Z_aligned, Z_true_centered, correlations\n",
    "\n",
    "def plot_training_curves(losses_train_baseline, losses_val_baseline, \n",
    "                         losses_val_optimal, losses_train_trainable, \n",
    "                         losses_val_trainable):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot 1: Training curves\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(losses_train_baseline, label='Classic AE - Train', linestyle='-', color='teal')\n",
    "    plt.plot(losses_val_baseline, label='Classic AE - Val', linestyle='--', color='teal')\n",
    "    plt.plot(losses_train_trainable, label='Trainable AE - Train', linestyle='-', color='magenta')\n",
    "    plt.plot(losses_val_trainable, label='Trainable AE - Val', linestyle='--', color='magenta')\n",
    " \n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss (MSE)\")\n",
    "    plt.title(\"Training vs Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot 2: Final comparison\n",
    "    plt.subplot(1, 2, 2)\n",
    "    models = ['Classic', 'Optimal', 'Trainable']\n",
    "    train_final = [losses_train_baseline[-1], 0, losses_train_trainable[-1]]\n",
    "    val_final = [losses_val_baseline[-1], losses_val_optimal, losses_val_trainable[-1]]\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, train_final, width, label='Train', alpha=0.8, color='pink')\n",
    "    plt.bar(x + width/2, val_final, width, label='Validation', alpha=0.8, color='lightblue')\n",
    "    plt.xlabel('Model Type')\n",
    "    plt.ylabel('Final Loss')\n",
    "    plt.title('Final Loss Comparison')\n",
    "    plt.xticks(x, models)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_models(model_baseline, model_optimal, model_trainable, X_tensor, A_opt, b_opt, \n",
    "                   F_true_path, losses_baseline_train, losses_baseline_val,\n",
    "                   losses_optimal_val, losses_trainable_train, losses_trainable_val,\n",
    "                   val_indices=None):\n",
    "\n",
    "    # 2. Latent Factor Correlation\n",
    "    print(\"LATENT FACTOR ANALYSIS:\")\n",
    "    print(\"\\nClassic AE Factors:\")\n",
    "    try:\n",
    "        _, _, corr_base = compare_latent_factors(model_baseline, X_tensor, F_true_path, val_indices)\n",
    "        avg_corr_base = np.mean(np.abs(corr_base))\n",
    "    except Exception as e:\n",
    "        print(f\"   Error in factor analysis: {e}\")\n",
    "        avg_corr_base = 0\n",
    "    \n",
    "    print(f\"   Average |correlation|: {avg_corr_base:.4f}\")\n",
    "    \n",
    "    print(\"\\nOptimal AE Factors:\")\n",
    "    try:\n",
    "        _, _, corr_opt = compare_latent_factors(model_optimal, X_tensor, F_true_path, val_indices)\n",
    "        avg_corr_opt = np.mean(np.abs(corr_opt))\n",
    "    except Exception as e:\n",
    "        print(f\"   Error in factor analysis: {e}\")\n",
    "        avg_corr_opt = 0\n",
    "    \n",
    "    print(f\"   Average |correlation|: {avg_corr_opt:.4f}\")\n",
    "\n",
    "    print(\"\\nTrainable AE Factors:\")\n",
    "    try:\n",
    "        _, _, corr_train = compare_latent_factors(model_trainable, X_tensor, F_true_path, val_indices)\n",
    "        avg_corr_train = np.mean(np.abs(corr_train))\n",
    "    except Exception as e:\n",
    "        print(f\"   Error in factor analysis: {e}\")\n",
    "        avg_corr_train = 0\n",
    "\n",
    "    print(f\"   Average |correlation|: {avg_corr_train:.4f}\")\n",
    "  \n",
    "    # 5. Plot training curves\n",
    "    print(\"PLOTTING TRAINING CURVES...\")\n",
    "    plot_training_curves(losses_baseline_train, losses_val_baseline=losses_baseline_val,\n",
    "                         losses_val_optimal=losses_optimal_val,\n",
    "                         losses_train_trainable=losses_trainable_train,\n",
    "                         losses_val_trainable=losses_trainable_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4435a663",
   "metadata": {},
   "source": [
    "Theoretical autoencoder always outperforms on reconstruction MSE indicating a true optimal solution \n",
    "- theoretical has no training and classic has 300 epochs of training\n",
    "- however i think that in order to best learn super noisy financial data, i think you need to train the optimal one and that will tailor it to your noisy dataset -> i gave the optimal solution ~50 epochs and its latent factor predictions were better than the classic autoencoder \n",
    "Question: I have tried training the optimal for about 20-50 epochs and it showed better performance than a classic that was trained for 300 epochs for nonlinear and GARCH data. Wondering if this is useful for CT data? should I try denoising in the latent space? because the latent factors are slightly worse than the classic but im guessing thats because we let the classic train and the optimal has no training at all. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
