{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cd1f43e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "import random\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bcbf9eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4e29fb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_optimal_A_b_mu(X_np, r):\n",
    "    mu = np.mean(X_np, axis=0)\n",
    "    cov = (X_np - mu).T @ (X_np - mu)\n",
    "    U, S, _ = np.linalg.svd(cov)\n",
    "    Ur = U[:, :r]  # shape (d, r)\n",
    "    \n",
    "    A = Ur @ Ur.T  # encoder projects from d->r: (r, d)\n",
    "    b = np.zeros(r)  # no bias needed in latent space\n",
    "    \n",
    "    return A, b, mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "47a682f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python packaged affine autoencoder\n",
    "\"\"\"A\n",
    "\n",
    "class ClassicAffineAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, bottleneck_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(input_dim, bottleneck_dim, bias=True)\n",
    "        self.decoder = nn.Linear(bottleneck_dim, input_dim, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "        \"\"\"\n",
    "class ClassicAffineAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, r):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.r = r\n",
    "        \n",
    "        # The projection matrix A (10x10)\n",
    "        self.projection = nn.Linear(input_dim, input_dim, bias=True)\n",
    "        \n",
    "        # Decoder from r-dimensional space\n",
    "        self.decoder = nn.Linear(r, input_dim, bias=True)\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        \"\"\"Extract r-dimensional factors from projected data\"\"\"\n",
    "        projected = self.projection(x)  # Apply A matrix\n",
    "        # Take first r dimensions or use SVD to get factors\n",
    "        return projected[:, :self.r]  # Simple approach: take first r dims\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7ac4312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimalAffineAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, r, A, b, mu):\n",
    "        super().__init__()\n",
    "        self.A = torch.tensor(A, dtype=torch.float32)\n",
    "        self.b = torch.tensor(b, dtype=torch.float32)\n",
    "        self.mu = torch.tensor(mu, dtype=torch.float32)\n",
    "\n",
    "    def encoder(self, x):\n",
    "        # Return centered projected latent space: Ur^T(x - mu)\n",
    "        x_centered = x - self.mu  # center by mean\n",
    "        UrT = self.A  # A = Ur Ur^T ⇒ A.T = Ur^T Ur ⇒ encoder ~ Ur^T\n",
    "        #return x_centered @ UrT  # [batch_size, d] x [d, r] → [batch_size, r]\n",
    "        return x_centered @ UrT  # i changed this switched the order\n",
    "    def decoder(self, z):\n",
    "        # Reconstruct from latent space: Ur * z + mu\n",
    "        return z @ self.A + self.mu # [batch_size, r] x [r, d] → [batch_size, d]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Complete encode-decode cycle\n",
    "        z = self.encoder(x)\n",
    "    \n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c4b31a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NONLINEAR AUTOENCODER \n",
    "class NonlinearAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, bottleneck_dim, hidden_dim=5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder: input -> hidden -> hidden -> bottleneck\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, bottleneck_dim)\n",
    "        )\n",
    "        \n",
    "        # Decoder: bottleneck -> hidden -> hidden -> output\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(bottleneck_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "640f5f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a combination modeel that allows for training after instantiation of optimal theoretical weights. \n",
    "class OptimalTrainableAffineAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, r, A_init, b_init, mu_init):\n",
    "        super().__init__()\n",
    "        self.A = nn.Parameter(torch.tensor(A_init, dtype=torch.float32))  # (r, d)\n",
    "        self.b = nn.Parameter(torch.tensor(b_init, dtype=torch.float32))  # (r,)\n",
    "        self.mu = nn.Parameter(torch.tensor(mu_init, dtype=torch.float32))  # (d,)\n",
    "\n",
    "    def encoder(self, x):\n",
    "        x_centered = x - self.mu  # (batch, d)\n",
    "        return x_centered @ self.A + self.b  # (batch, r)\n",
    "\n",
    "    def decoder(self, z):\n",
    "        return z @ self.A + self.mu  # (batch, d) I changed this too\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7e435baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Modify train_autoencoder to accept loaders instead of raw tensor\n",
    "def train_autoencoder(model, train_loader, val_loader, num_epochs, lr=1e-3):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon = model(batch)\n",
    "            loss = criterion(recon, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item() * batch.size(0)\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader.dataset)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                recon = model(batch)\n",
    "                loss = criterion(recon, batch)\n",
    "                total_val_loss += loss.item() * batch.size(0)\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "\n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a4a2b82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation function only code for the optimal affine autoencoder\n",
    "def valOnlyOptimalAffineAutoencoder(model, val_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = batch.to(device)\n",
    "            recon = model(batch)\n",
    "            loss = criterion(recon, batch)\n",
    "            total_loss += loss.item() * batch.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader.dataset)\n",
    "    return avg_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "18e5ab3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (2000, 10)\n",
      "PCA baseline MSE: 0.000016\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "the shapes of A and B differ ((400, 10) vs (400, 3))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 187\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;66;03m# Compute and store factor correlations\u001b[39;00m\n\u001b[1;32m    183\u001b[0m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassic_factors\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    184\u001b[0m     aligned_corr(model_classic, val_tensor, F_true_tensor)\n\u001b[1;32m    185\u001b[0m )\n\u001b[1;32m    186\u001b[0m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimal_factors\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 187\u001b[0m     aligned_corr(model_optimal, val_tensor, F_true_tensor)\n\u001b[1;32m    188\u001b[0m )\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m#results['trainable_factors'].append(\u001b[39;00m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;66;03m#aligned_corr(model_optimal_trainable, val_tensor, F_true_tensor)\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m#)\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m#results['nonlinear_factors'].append(\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m#aligned_corr(model_nonlinear, val_tensor, F_true_tensor)\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m#)\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_results_summary\u001b[39m(results):\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;66;03m# MSE Results (Validation Loss)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[77], line 175\u001b[0m, in \u001b[0;36maligned_corr\u001b[0;34m(model, X_val, F_true_val)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Z \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencoder(X_val)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    174\u001b[0m     F \u001b[38;5;241m=\u001b[39m F_true_val\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m--> 175\u001b[0m R, _ \u001b[38;5;241m=\u001b[39m orthogonal_procrustes(Z, F)\n\u001b[1;32m    176\u001b[0m Z_aligned \u001b[38;5;241m=\u001b[39m Z \u001b[38;5;241m@\u001b[39m R\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mabs([\n\u001b[1;32m    178\u001b[0m     np\u001b[38;5;241m.\u001b[39mcorrcoef(Z_aligned[:, i], F[:, i])[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(F\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    180\u001b[0m ])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/scipy/linalg/_procrustes.py:85\u001b[0m, in \u001b[0;36morthogonal_procrustes\u001b[0;34m(A, B, check_finite)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexpected ndim to be 2, but observed \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m A\u001b[38;5;241m.\u001b[39mndim)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m B\u001b[38;5;241m.\u001b[39mshape:\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe shapes of A and B differ (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m vs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mB\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Be clever with transposes, with the intention to save memory.\u001b[39;00m\n\u001b[1;32m     87\u001b[0m u, w, vt \u001b[38;5;241m=\u001b[39m svd(B\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mdot(A)\u001b[38;5;241m.\u001b[39mT)\n",
      "\u001b[0;31mValueError\u001b[0m: the shapes of A and B differ ((400, 10) vs (400, 3))"
     ]
    }
   ],
   "source": [
    "# get data ready\n",
    "X_df = pd.read_csv(\"assetReturns_garch.csv\")\n",
    "X_np = X_df.to_numpy().astype(np.float32)\n",
    "X_tensor = torch.tensor(X_np)\n",
    "\n",
    "print(f\"Data shape: {X_np.shape}\")\n",
    "\n",
    "# set dims and latent space size \n",
    "input_dim = X_np.shape[1]\n",
    "r = 3  # bottleneck dimension\n",
    "\n",
    "# compute the optimal params (only once, outside the loop)\n",
    "A, b, mu = compute_optimal_A_b_mu(X_np, r)\n",
    "\n",
    "# Initialize results stoage\n",
    "\n",
    "results = {\n",
    "    'classic_mse': [],\n",
    "    'optimal_mse': [],\n",
    "    'trainable_mse': [],\n",
    "    'nonlinear_mse': [],    \n",
    "    'classic_train_loss': [],\n",
    "    'classic_val_loss': [],\n",
    "    'trainable_train_loss': [],\n",
    "    'trainable_val_loss': [],\n",
    "    'nonlinear_train_loss': [],\n",
    "    'nonlinear_val_loss': [],\n",
    "    'classic_factors': [],\n",
    "    'optimal_factors': [],\n",
    "    'trainable_factors': [],\n",
    "    'nonlinear_factors': [],    \n",
    "    'classic_train_histories': [],\n",
    "    'trainable_train_histories': [],\n",
    "    'classic_val_histories': [],\n",
    "    'trainable_val_histories': [],\n",
    "    'nonlinear_train_histories': [],\n",
    "    'nonlinear_val_histories': []\n",
    "}\n",
    "  \n",
    "    # split data into training and validation with current seed\n",
    "\n",
    "n_samples = X_tensor.shape[0]\n",
    "train_size = int(0.8 * n_samples)\n",
    "val_size = n_samples - train_size\n",
    "\n",
    "# Slice chronologically\n",
    "train_data = X_tensor[:train_size]\n",
    "train_indices = train_data.indices\n",
    "val_data = X_tensor[train_size:]\n",
    "val_indices = np.arange(train_size, len(X_np))\n",
    "val_dates = X_df.index[val_indices]\n",
    "\n",
    "# create the data loaders\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # initialize models for this run\n",
    "model_classic = ClassicAffineAutoencoder(input_dim, r)\n",
    "model_optimal = OptimalAffineAutoencoder(input_dim, r, A, b, mu)\n",
    "#model_optimal_trainable = OptimalTrainableAffineAutoencoder(input_dim, r, A, b, mu)\n",
    "#model_nonlinear = NonlinearAutoencoder(input_dim, r, hidden_dim=128)\n",
    "    \n",
    "    # Train classic model\n",
    "model_classic, losses_classic_train, losses_classic_val = train_autoencoder(\n",
    "    model_classic, \n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    70,\n",
    "    0.001\n",
    "    )\n",
    "    \n",
    "    # Validation only on optimal model\n",
    "losses_optimal_val = valOnlyOptimalAffineAutoencoder(model_optimal, val_loader)\n",
    "    \n",
    "    # Train optimal trainable model\n",
    "\"\"\"\n",
    "    model_optimal_trainable, losses_trainable_train, losses_trainable_val = train_autoencoder(\n",
    "    model_optimal_trainable,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    20,\n",
    "    0.001\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "model_nonlinear, losses_nonlinear_train, losses_nonlinear_val = train_autoencoder(\n",
    "    model_nonlinear,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    70,\n",
    "    0.001\n",
    ")\n",
    "\"\"\"\n",
    "    \n",
    "    # Store results for this run\n",
    "results['classic_mse'].append(min(losses_classic_val))\n",
    "results['optimal_mse'].append(losses_optimal_val)\n",
    "#results['trainable_mse'].append(min(losses_trainable_val))\n",
    "#results['nonlinear_mse'].append(min(losses_nonlinear_val))\n",
    "\n",
    "results['classic_train_loss'].append(losses_classic_train[-1])\n",
    "results['classic_val_loss'].append(losses_classic_val[-1])\n",
    "#results['trainable_train_loss'].append(losses_trainable_train[-1])\n",
    "#results['trainable_val_loss'].append(losses_trainable_val[-1])\n",
    "#results['nonlinear_train_loss'].append(losses_nonlinear_train[-1])\n",
    "#results['nonlinear_val_loss'].append(losses_nonlinear_val[-1])\n",
    "\n",
    "    # Store full training histories\n",
    "results['classic_train_histories'].append(losses_classic_train)\n",
    "results['classic_val_histories'].append(losses_classic_val)\n",
    "#results['trainable_train_histories'].append(losses_trainable_train)\n",
    "#results['trainable_val_histories'].append(losses_trainable_val)\n",
    "#results['nonlinear_train_histories'].append(losses_nonlinear_train)\n",
    "#results['nonlinear_val_histories'].append(losses_nonlinear_val)\n",
    "\n",
    "    # Convert to NumPy arrays\n",
    "X_train_np = train_data.numpy()\n",
    "X_val_np = val_data.numpy()\n",
    "\n",
    "    # Load ground truth latent factors (full set) and slice val rows\n",
    "F_true_full = pd.read_csv(\"latentFactors_garch.csv\").to_numpy().astype(np.float32)\n",
    "F_true_tensor = torch.tensor(F_true_full[val_indices])\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # PCA BASELINE: Fit on training data, evaluate on val\n",
    "    # --------------------------------------------\n",
    "\n",
    "    # Fit PCA on training data\n",
    "pca = PCA(n_components=r)\n",
    "pca.fit(X_train_np)\n",
    "\n",
    "    # Reconstruct validation data\n",
    "X_val_recon = pca.inverse_transform(pca.transform(X_val_np))\n",
    "\n",
    "    # Compute MSE on validation set\n",
    "pca_mse = np.mean((X_val_np - X_val_recon) ** 2)\n",
    "print(f\"PCA baseline MSE: {pca_mse:.6f}\")\n",
    "    \n",
    "    # Transform validation data\n",
    "Z_pca = pca.transform(X_val_np)\n",
    "\n",
    "    # Align to true latent factors using Procrustes\n",
    "F = F_true_tensor.numpy()\n",
    "R_pca, _ = orthogonal_procrustes(Z_pca, F)\n",
    "Z_pca_aligned = Z_pca @ R_pca\n",
    "\n",
    "    # Compute correlation per factor\n",
    "corr_pca = np.abs([\n",
    "        np.corrcoef(Z_pca_aligned[:, i], F[:, i])[0, 1]\n",
    "        for i in range(F.shape[1])\n",
    "])\n",
    "    \n",
    "    # Compute reconstruction MSE on validation data\n",
    "X_val_reconstructed = pca.inverse_transform(Z_pca)\n",
    "mse_pca = np.mean((X_val_np - X_val_reconstructed) ** 2)\n",
    "\n",
    "    # Store results\n",
    "results.setdefault('pca_mse', []).append(mse_pca)\n",
    "results.setdefault('pca_factors', []).append(corr_pca)\n",
    "\n",
    " # FACTOR ANALYSIS: Procrustes-aligned correlations\n",
    "  \n",
    "    # Get validation data as tensor\n",
    "val_tensor = X_tensor[val_indices]\n",
    "\n",
    "    # Helper function to align & compute correlation\n",
    "def aligned_corr(model, X_val, F_true_val):\n",
    "    with torch.no_grad():\n",
    "        Z = model.encoder(X_val).cpu().numpy()\n",
    "        F = F_true_val.cpu().numpy()\n",
    "    R, _ = orthogonal_procrustes(Z, F)\n",
    "    Z_aligned = Z @ R\n",
    "    return np.abs([\n",
    "        np.corrcoef(Z_aligned[:, i], F[:, i])[0, 1]\n",
    "        for i in range(F.shape[1])\n",
    "    ])\n",
    "\n",
    "    # Compute and store factor correlations\n",
    "results['classic_factors'].append(\n",
    "    aligned_corr(model_classic, val_tensor, F_true_tensor)\n",
    ")\n",
    "results['optimal_factors'].append(\n",
    "    aligned_corr(model_optimal, val_tensor, F_true_tensor)\n",
    ")\n",
    "#results['trainable_factors'].append(\n",
    "    #aligned_corr(model_optimal_trainable, val_tensor, F_true_tensor)\n",
    "#)\n",
    "#results['nonlinear_factors'].append(\n",
    "    #aligned_corr(model_nonlinear, val_tensor, F_true_tensor)\n",
    "#)\n",
    "\n",
    "def print_results_summary(results):\n",
    "    # MSE Results (Validation Loss)\n",
    "    print(\"\\n VALIDATION MSE RESULTS\")\n",
    "    \n",
    "    classic_mse = np.array(results['classic_mse'])\n",
    "    optimal_mse = np.array(results['optimal_mse'])\n",
    "    #trainable_mse = np.array(results['trainable_mse'])\n",
    "    #nonlinear_mse = np.array(results['nonlinear_mse'])  # NEW\n",
    "    pca_mse = np.array(results['pca_mse'])  # NEW\n",
    "\n",
    "    print(f\"Classic Autoencoder:\")\n",
    "    print(f\"  Mean MSE: {classic_mse.mean():.8f} \")\n",
    "    print(f\"  Median:   {np.median(classic_mse):.8f}\")\n",
    "    print(f\"  Min/Max:  {classic_mse.min():.8f}  \")\n",
    "    \n",
    "    print(f\"\\nOptimal Autoencoder:\")\n",
    "    print(f\"  Mean MSE: {optimal_mse.mean():.8f} \")\n",
    "    print(f\"  Median:   {np.median(optimal_mse):.8f}\")\n",
    "    print(f\"  Min/Max:  {optimal_mse.min():.8f}  {optimal_mse.max():.8f}\")\n",
    "    \n",
    "    #print(f\"\\nTrainable Optimal Autoencoder:\")\n",
    "    #print(f\"  Mean MSE: {trainable_mse.mean():.8f} \")\n",
    "    #print(f\"  Median:   {np.median(trainable_mse):.8f}\")\n",
    "    #print(f\"  Min/Max:  {trainable_mse.min():.8f}  {trainable_mse.max():.8f}\")\n",
    "\n",
    "    #print(f\"\\nNonlinear Autoencoder:\")\n",
    "    #print(f\"  Mean MSE: {nonlinear_mse.mean():.8f} \")\n",
    "    #print(f\"  Median:   {np.median(nonlinear_mse):.8f}\")\n",
    "    #print(f\"  Min/Max:  {nonlinear_mse.min():.8f}  {nonlinear_mse.max():.8f}\")\n",
    "\n",
    "    print(f\"\\nPCA Baseline:\")\n",
    "    print(f\"  Mean MSE: {pca_mse.mean():.8f} \")\n",
    "    print(f\"  Median:   {np.median(pca_mse):.8f}\")\n",
    "    print(f\"  Min/Max:  {pca_mse.min():.8f}  {pca_mse.max():.8f}\")\n",
    "    \n",
    "    # Final Training/Validation Losses\n",
    "    print(f\"\\n FINAL TRAINING/VALIDATION LOSSES\")\n",
    "    \n",
    "    classic_train_final = np.array(results['classic_train_loss'])\n",
    "    classic_val_final = np.array(results['classic_val_loss'])\n",
    "    #trainable_train_final = np.array(results['trainable_train_loss'])\n",
    "    #trainable_val_final = np.array(results['trainable_val_loss'])\n",
    "    #nonlinear_train_final = np.array(results['nonlinear_train_loss'])  # NEW\n",
    "    #nonlinear_val_final = np.array(results['nonlinear_val_loss'])  # NEW            \n",
    "    \n",
    "    print(f\"Classic Autoencoder:\")\n",
    "    print(f\"  Final Train Loss: {classic_train_final.mean():.8f} \")\n",
    "    print(f\"  Final Val Loss:   {classic_val_final.mean():.8f} \")\n",
    "    print(f\"  Train/Val Ratio:  {(classic_train_final.mean()/classic_val_final.mean()):.4f}\")\n",
    "    \n",
    "    #print(f\"\\nTrainable Optimal Autoencoder:\")\n",
    "    #print(f\"  Final Train Loss: {trainable_train_final.mean():.8f} \")\n",
    "    #print(f\"  Final Val Loss:   {trainable_val_final.mean():.8f} \")\n",
    "    #print(f\"  Train/Val Ratio:  {(trainable_train_final.mean()/trainable_val_final.mean()):.4f}\")\n",
    "\n",
    "    #print(f\"\\nNonlinear Autoencoder:\")\n",
    "    #print(f\"  Final Train Loss: {nonlinear_train_final.mean():.8f} \")\n",
    "    #print(f\"  Final Val Loss:   {nonlinear_val_final.mean():.8f} \")\n",
    "    #print(f\"  Train/Val Ratio:  {(nonlinear_train_final.mean()/nonlinear_val_final.mean()):.4f}\")       \n",
    "    \n",
    "    # Factor Analysis Results\n",
    "    print(f\"\\n FACTOR RECOVERY ANALYSIS\")\n",
    "\n",
    "    classic_factors = np.array(results['classic_factors'])\n",
    "    optimal_factors = np.array(results['optimal_factors'])\n",
    "   #trainable_factors = np.array(results['trainable_factors'])\n",
    "    #nonlinear_factors = np.array(results['nonlinear_factors'])  # NEW\n",
    "    pca_factors = np.array(results['pca_factors'])  # NEW\n",
    "    \n",
    "    print(f\"Factor Correlations:\")\n",
    "    print(f\"  Classic:           {classic_factors.mean():.4f}\")\n",
    "    print(f\"  Optimal:           {optimal_factors.mean():.4f} \")\n",
    "    #print(f\"  Trainable Optimal: {trainable_factors.mean():.4f} \")\n",
    "    #print(f\"  Nonlinear:        {nonlinear_factors.mean():.4f} \")\n",
    "    print(f\"  PCA:               {pca_factors.mean():.4f} \")\n",
    "    \n",
    "    print(f\"\\nPer-Factor Correlations:\")\n",
    "    print(f\"{'Factor':<8} {'Classic':<10} {'Optimal':<10} {'Trainable':<10} {'PCA':<10}\")\n",
    "    \n",
    "    for i in range(classic_factors.shape[1]):\n",
    "        factor_classic = classic_factors[:, i].mean()\n",
    "        factor_optimal = optimal_factors[:, i].mean()\n",
    "        #factor_trainable = trainable_factors[:, i].mean()\n",
    "        factor_pca = pca_factors[:, i].mean()\n",
    "        #factor_nonlinear = nonlinear_factors[:, i].mean()  # NEW\n",
    "        #print(f\"{i+1:<8} {factor_classic:<10.4f} {factor_optimal:<10.4f} {factor_trainable:<10.4f} {factor_pca:<10.4f} {factor_nonlinear:<10.4f}\")\n",
    "\n",
    "    # Summary Table\n",
    "    print(f\"\\n SUMMARY TABLE\")\n",
    "    \n",
    "    summary_df = pd.DataFrame({\n",
    "        'Model': ['Classic', 'Optimal', 'Trainable', 'PCA', 'Nonlinear'],\n",
    "        'MSE_Mean': [\n",
    "            classic_mse.mean(), \n",
    "            optimal_mse.mean(), \n",
    "            #trainable_mse.mean(), \n",
    "            pca_mse.mean(),\n",
    "            #nonlinear_mse.mean()  # NEW\n",
    "        ],\n",
    "\n",
    "        'Factor_Corr_Mean': [\n",
    "            classic_factors.mean(), \n",
    "            optimal_factors.mean(), \n",
    "            #trainable_factors.mean(),\n",
    "            pca_factors.mean(),\n",
    "            #nonlinear_factors.mean()\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    print(summary_df.round(6))\n",
    "print_results_summary(results)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb40b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/opt/anaconda3/lib/python3.12/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed run 10/100\n",
      "Completed run 20/100\n",
      "Completed run 30/100\n",
      "Completed run 40/100\n",
      "Completed run 50/100\n",
      "Completed run 60/100\n",
      "Completed run 70/100\n",
      "Completed run 80/100\n",
      "Completed run 90/100\n",
      "Completed run 100/100\n",
      "\n",
      "============================================================\n",
      "RESULTS SUMMARY\n",
      "============================================================\n",
      "\n",
      "MSE RESULTS:\n",
      "  Linear Train MSE:    0.00001867 ± 0.00000089\n",
      "  Linear Val MSE:      0.00001944 ± 0.00000109\n",
      "  Nonlinear Train MSE: 0.00002553 ± 0.00000157\n",
      "  Nonlinear Val MSE:   0.00002658 ± 0.00000172\n",
      "\n",
      "FACTOR RECOVERY ANALYSIS\n",
      "\n",
      "Overall Factor Correlations (mean across all factors and runs):\n",
      "  Classic:    0.2052 ± 0.1830\n",
      "  Nonlinear:  nan ± nan\n",
      "\n",
      "Per-Factor Correlations (mean across runs):\n",
      "Factor   Classic      Nonlinear   \n",
      "-----------------------------------\n",
      "1        0.3908±0.2062   nan±nan\n",
      "2        0.1299±0.0517   nan±nan\n",
      "3        0.0950±0.0548   nan±nan\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "X_val = val_data.numpy()\n",
    "r = 3  # number of latent factors\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "results = {\n",
    "    'linear_train_mse': [],\n",
    "    'linear_val_mse': [],\n",
    "    'nonlinear_train_mse': [],\n",
    "    'nonlinear_val_mse': [],\n",
    "    'classic_factors': [],\n",
    "    'nonlinear_factors': [],\n",
    "    'classic_analysis': [],\n",
    "    'nonlinear_analysis': []\n",
    "}\n",
    "\n",
    "# Get validation data as tensor (move this before the loop)\n",
    "val_tensor = X_tensor[val_indices]\n",
    "\n",
    "# Helper function to align & compute correlation\n",
    "def aligned_corr(model, X_val, F_true_val):\n",
    "    with torch.no_grad():\n",
    "        Z = model.encoder(X_val).cpu().numpy()\n",
    "        F = F_true_val.cpu().numpy()\n",
    "    R, _ = orthogonal_procrustes(Z, F)\n",
    "    Z_aligned = Z @ R\n",
    "    return np.abs([\n",
    "        np.corrcoef(Z_aligned[:, i], F[:, i])[0, 1]\n",
    "        for i in range(F.shape[1])\n",
    "    ])\n",
    "\n",
    "# Run 100 iterations\n",
    "for run in range(100):\n",
    "    seed = 42 + run  # Different seed for each run\n",
    "    set_seed(seed)\n",
    "    \n",
    "    # Print progress every 10 runs\n",
    "    if (run + 1) % 10 == 0:\n",
    "        print(f\"Completed run {run + 1}/100\")\n",
    "    \n",
    "    # Init model and train model\n",
    "    modellinear = ClassicAffineAutoencoder(input_dim, r).to(device)\n",
    "    modelnonlinear = NonlinearAutoencoder(input_dim, r).to(device)\n",
    "    \n",
    "    modellinear, train_losslinear, val_losslinear = train_autoencoder(\n",
    "        modellinear, train_loader, val_loader, num_epochs=150, lr=0.001)\n",
    "    modelnonlinear, train_lossnonlinear, val_lossnonlinear = train_autoencoder(\n",
    "        modelnonlinear, train_loader, val_loader, num_epochs=100, lr=0.001)\n",
    "    \n",
    "    # Store final MSE values (assuming losses are MSE)\n",
    "    results['linear_train_mse'].append(train_losslinear[-1])  # Final training MSE\n",
    "    results['linear_val_mse'].append(val_losslinear[-1])      # Final validation MSE\n",
    "    results['nonlinear_train_mse'].append(train_lossnonlinear[-1])  # Final training MSE\n",
    "    results['nonlinear_val_mse'].append(val_lossnonlinear[-1])      # Final validation MSE\n",
    "    \n",
    "    # Get factors for this run\n",
    "    classic_factors = modellinear.encoder(val_data).detach().cpu().numpy()\n",
    "    nonlinear_factors = modelnonlinear.encoder(val_data).detach().cpu().numpy()\n",
    "    \n",
    "    # Store factors for this run\n",
    "    results['classic_factors'].append(classic_factors)\n",
    "    results['nonlinear_factors'].append(nonlinear_factors)\n",
    "    \n",
    "    # Compute and store factor correlations (MOVED INSIDE THE LOOP)\n",
    "    results['classic_analysis'].append(\n",
    "        aligned_corr(modellinear, val_tensor, F_true_tensor)\n",
    "    )\n",
    "    results['nonlinear_analysis'].append(\n",
    "        aligned_corr(modelnonlinear, val_tensor, F_true_tensor)\n",
    "    )\n",
    "\n",
    "# After all runs are complete, compute summary statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Print MSE results\n",
    "print(\"\\nMSE RESULTS:\")\n",
    "print(f\"  Linear Train MSE:    {np.mean(results['linear_train_mse']):.8f} ± {np.std(results['linear_train_mse']):.8f}\")\n",
    "print(f\"  Linear Val MSE:      {np.mean(results['linear_val_mse']):.8f} ± {np.std(results['linear_val_mse']):.8f}\")\n",
    "print(f\"  Nonlinear Train MSE: {np.mean(results['nonlinear_train_mse']):.8f} ± {np.std(results['nonlinear_train_mse']):.8f}\")\n",
    "print(f\"  Nonlinear Val MSE:   {np.mean(results['nonlinear_val_mse']):.8f} ± {np.std(results['nonlinear_val_mse']):.8f}\")\n",
    "\n",
    "def print_results_summary(results):\n",
    "    # Factor Analysis Results\n",
    "    print(f\"\\nFACTOR RECOVERY ANALYSIS\")\n",
    "    \n",
    "    # Use the correct keys - these contain correlation results\n",
    "    classic_factors = np.array(results['classic_analysis'])  # Shape: (100, 3)\n",
    "    nonlinear_factors = np.array(results['nonlinear_analysis'])  # Shape: (100, 3)\n",
    "    \n",
    "    print(f\"\\nOverall Factor Correlations (mean across all factors and runs):\")\n",
    "    print(f\"  Classic:    {classic_factors.mean():.4f} ± {classic_factors.std():.4f}\")\n",
    "    print(f\"  Nonlinear:  {nonlinear_factors.mean():.4f} ± {nonlinear_factors.std():.4f}\")\n",
    "    \n",
    "    print(f\"\\nPer-Factor Correlations (mean across runs):\")\n",
    "    print(f\"{'Factor':<8} {'Classic':<12} {'Nonlinear':<12}\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    for i in range(classic_factors.shape[1]):\n",
    "        factor_classic_mean = classic_factors[:, i].mean()\n",
    "        factor_classic_std = classic_factors[:, i].std()\n",
    "        factor_nonlinear_mean = nonlinear_factors[:, i].mean()\n",
    "        factor_nonlinear_std = nonlinear_factors[:, i].std()\n",
    "        print(f\"{i+1:<8} {factor_classic_mean:.4f}±{factor_classic_std:.4f}   {factor_nonlinear_mean:.4f}±{factor_nonlinear_std:.4f}\")\n",
    "\n",
    "# Call the function\n",
    "print_results_summary(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de48eb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Very low variance in encoded factors: [5.1301186e-12 4.3520743e-14 8.8817842e-16]\n",
      "\n",
      "Run 1 Debug Info:\n",
      "  Classic correlations: [0.06767839 0.10384607 0.03067196]\n",
      "  Nonlinear correlations: [nan nan nan]\n",
      "  Classic factors - mean: [ 0.35990378 -0.49302375  0.21532026], std: [0.00661348 0.00365904 0.00460045]\n",
      "  Nonlinear factors - mean: [-0.5608257   0.16102599  0.25119022], std: [2.2649765e-06 2.0861626e-07 2.9802322e-08]\n",
      "\n",
      "Run 2 Debug Info:\n",
      "  Classic correlations: [0.03021932 0.09925218 0.06766467]\n",
      "  Nonlinear correlations: [0.62704009 0.05655566 0.1443164 ]\n",
      "  Classic factors - mean: [-0.15627536 -0.38007554  0.28854406], std: [0.00462682 0.00541145 0.00418584]\n",
      "  Nonlinear factors - mean: [-0.2991768   0.474788    0.29565907], std: [4.803569e-05 9.585928e-04 7.269078e-04]\n",
      "\n",
      "Run 3 Debug Info:\n",
      "  Classic correlations: [0.4432214  0.20031793 0.14000445]\n",
      "  Nonlinear correlations: [0.67800499 0.11785022 0.17414877]\n",
      "  Classic factors - mean: [-0.38652635  0.2551032   0.27147216], std: [0.00590612 0.0052268  0.00454902]\n",
      "  Nonlinear factors - mean: [-0.08521908 -0.11870618  0.17397092], std: [0.01087652 0.00181572 0.0200013 ]\n",
      "Completed run 10/100\n",
      "Warning: Very low variance in encoded factors: [1.7408297e-13 2.2204460e-16 8.2622797e-13]\n",
      "Completed run 20/100\n",
      "Warning: Very low variance in encoded factors: [2.3403778e-08 2.4017878e-12 5.2360422e-10]\n",
      "Completed run 30/100\n",
      "Warning: Very low variance in encoded factors: [4.2987836e-13 1.9984014e-13 6.2372330e-13]\n",
      "Warning: Very low variance in encoded factors: [2.2204460e-16 1.3877788e-15 1.2789769e-13]\n",
      "Warning: Very low variance in encoded factors: [5.5511151e-13 7.9936058e-13 3.5527137e-15]\n",
      "Completed run 40/100\n",
      "Warning: Very low variance in encoded factors: [2.6867397e-14 2.5899283e-12 3.1974423e-14]\n",
      "Warning: Very low variance in encoded factors: [1.8505229e-10 8.4339292e-11 3.1261660e-12]\n",
      "Completed run 50/100\n",
      "Warning: Very low variance in encoded factors: [1.4930279e-12 1.8673951e-13 2.7853275e-12]\n",
      "Warning: Very low variance in encoded factors: [5.5511151e-17 4.1056047e-13 1.7985613e-14]\n",
      "Warning: Very low variance in encoded factors: [7.194245e-14 0.000000e+00 2.877698e-13]\n",
      "Completed run 60/100\n",
      "Completed run 70/100\n",
      "Completed run 80/100\n",
      "Warning: Very low variance in encoded factors: [8.5353946e-13 6.8001160e-16 1.3877788e-13]\n",
      "Warning: Very low variance in encoded factors: [1.2825296e-12 3.1974423e-14 1.0746959e-13]\n",
      "Completed run 90/100\n",
      "Warning: Very low variance in encoded factors: [3.4694470e-14 1.5667467e-12 1.8353374e-15]\n",
      "Warning: Very low variance in encoded factors: [0.0000000e+00 4.4964032e-15 8.5353946e-13]\n",
      "Completed run 100/100\n",
      "\n",
      "============================================================\n",
      "RESULTS SUMMARY\n",
      "============================================================\n",
      "\n",
      "MSE RESULTS:\n",
      "  Linear Train MSE:    0.00001867 ± 0.00000089\n",
      "  Linear Val MSE:      0.00001944 ± 0.00000109\n",
      "  Nonlinear Train MSE: 0.00002553 ± 0.00000157\n",
      "  Nonlinear Val MSE:   0.00002658 ± 0.00000172\n",
      "\n",
      "FACTOR RECOVERY ANALYSIS\n",
      "\n",
      "Overall Factor Correlations (mean across all factors and runs):\n",
      "  Classic:    0.2052 ± 0.1830\n",
      "  Nonlinear:  nan ± nan\n",
      "\n",
      "Per-Factor Correlations (mean across runs):\n",
      "Factor   Classic      Nonlinear   \n",
      "-----------------------------------\n",
      "1        0.3908±0.2062   nan±nan\n",
      "2        0.1299±0.0517   nan±nan\n",
      "3        0.0950±0.0548   nan±nan\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "X_val = val_data.numpy()\n",
    "r = 3  # number of latent factors\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "results = {\n",
    "    'linear_train_mse': [],\n",
    "    'linear_val_mse': [],\n",
    "    'nonlinear_train_mse': [],\n",
    "    'nonlinear_val_mse': [],\n",
    "    'classic_factors': [],\n",
    "    'nonlinear_factors': [],\n",
    "    'classic_analysis': [],\n",
    "    'nonlinear_analysis': []\n",
    "}\n",
    "\n",
    "# Get validation data as tensor (move this before the loop)\n",
    "val_tensor = X_tensor[val_indices]\n",
    "\n",
    "# Helper function to align & compute correlation\n",
    "def aligned_corr(model, X_val, F_true_val):\n",
    "    with torch.no_grad():\n",
    "        Z = model.encoder(X_val).cpu().numpy()\n",
    "        F = F_true_val.cpu().numpy()\n",
    "    \n",
    "    # Check for problematic values\n",
    "    if np.any(np.isnan(Z)) or np.any(np.isinf(Z)):\n",
    "        print(f\"Warning: NaN or Inf values in encoded factors Z\")\n",
    "        return np.full(F.shape[1], np.nan)\n",
    "    \n",
    "    # Check variance of encoded factors\n",
    "    z_vars = np.var(Z, axis=0)\n",
    "    if np.any(z_vars < 1e-10):\n",
    "        print(f\"Warning: Very low variance in encoded factors: {z_vars}\")\n",
    "        return np.full(F.shape[1], np.nan)\n",
    "    \n",
    "    R, _ = orthogonal_procrustes(Z, F)\n",
    "    Z_aligned = Z @ R\n",
    "    \n",
    "    correlations = []\n",
    "    for i in range(F.shape[1]):\n",
    "        # Check if either variable has zero variance\n",
    "        if np.var(Z_aligned[:, i]) < 1e-10 or np.var(F[:, i]) < 1e-10:\n",
    "            correlations.append(np.nan)\n",
    "        else:\n",
    "            corr = np.corrcoef(Z_aligned[:, i], F[:, i])[0, 1]\n",
    "            correlations.append(np.abs(corr) if not np.isnan(corr) else np.nan)\n",
    "    \n",
    "    return np.array(correlations)\n",
    "\n",
    "# Run 100 iterations\n",
    "for run in range(100):\n",
    "    seed = 42 + run  # Different seed for each run\n",
    "    set_seed(seed)\n",
    "    \n",
    "    # Print progress every 10 runs\n",
    "    if (run + 1) % 10 == 0:\n",
    "        print(f\"Completed run {run + 1}/100\")\n",
    "    \n",
    "    # Init model and train model\n",
    "    modellinear = ClassicAffineAutoencoder(input_dim, r).to(device)\n",
    "    modelnonlinear = NonlinearAutoencoder(input_dim, r).to(device)\n",
    "    \n",
    "    modellinear, train_losslinear, val_losslinear = train_autoencoder(\n",
    "        modellinear, train_loader, val_loader, num_epochs=150, lr=0.001)\n",
    "    modelnonlinear, train_lossnonlinear, val_lossnonlinear = train_autoencoder(\n",
    "        modelnonlinear, train_loader, val_loader, num_epochs=100, lr=0.001)\n",
    "    \n",
    "    # Store final MSE values (assuming losses are MSE)\n",
    "    results['linear_train_mse'].append(train_losslinear[-1])  # Final training MSE\n",
    "    results['linear_val_mse'].append(val_losslinear[-1])      # Final validation MSE\n",
    "    results['nonlinear_train_mse'].append(train_lossnonlinear[-1])  # Final training MSE\n",
    "    results['nonlinear_val_mse'].append(val_lossnonlinear[-1])      # Final validation MSE\n",
    "    \n",
    "    # Get factors for this run\n",
    "    classic_factors = modellinear.encoder(val_data).detach().cpu().numpy()\n",
    "    nonlinear_factors = modelnonlinear.encoder(val_data).detach().cpu().numpy()\n",
    "    \n",
    "    # Store factors for this run\n",
    "    results['classic_factors'].append(classic_factors)\n",
    "    results['nonlinear_factors'].append(nonlinear_factors)\n",
    "    \n",
    "    # Compute and store factor correlations (MOVED INSIDE THE LOOP)\n",
    "    classic_corr = aligned_corr(modellinear, val_tensor, F_true_tensor)\n",
    "    nonlinear_corr = aligned_corr(modelnonlinear, val_tensor, F_true_tensor)\n",
    "    \n",
    "    # Debug: Print info for first few runs\n",
    "    if run < 3:\n",
    "        print(f\"\\nRun {run + 1} Debug Info:\")\n",
    "        print(f\"  Classic correlations: {classic_corr}\")\n",
    "        print(f\"  Nonlinear correlations: {nonlinear_corr}\")\n",
    "        \n",
    "        # Check the actual factor values\n",
    "        with torch.no_grad():\n",
    "            classic_factors_debug = modellinear.encoder(val_tensor).cpu().numpy()\n",
    "            nonlinear_factors_debug = modelnonlinear.encoder(val_tensor).cpu().numpy()\n",
    "        \n",
    "        print(f\"  Classic factors - mean: {classic_factors_debug.mean(axis=0)}, std: {classic_factors_debug.std(axis=0)}\")\n",
    "        print(f\"  Nonlinear factors - mean: {nonlinear_factors_debug.mean(axis=0)}, std: {nonlinear_factors_debug.std(axis=0)}\")\n",
    "    \n",
    "    results['classic_analysis'].append(classic_corr)\n",
    "    results['nonlinear_analysis'].append(nonlinear_corr)\n",
    "\n",
    "# After all runs are complete, compute summary statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Print MSE results\n",
    "print(\"\\nMSE RESULTS:\")\n",
    "print(f\"  Linear Train MSE:    {np.mean(results['linear_train_mse']):.8f} ± {np.std(results['linear_train_mse']):.8f}\")\n",
    "print(f\"  Linear Val MSE:      {np.mean(results['linear_val_mse']):.8f} ± {np.std(results['linear_val_mse']):.8f}\")\n",
    "print(f\"  Nonlinear Train MSE: {np.mean(results['nonlinear_train_mse']):.8f} ± {np.std(results['nonlinear_train_mse']):.8f}\")\n",
    "print(f\"  Nonlinear Val MSE:   {np.mean(results['nonlinear_val_mse']):.8f} ± {np.std(results['nonlinear_val_mse']):.8f}\")\n",
    "\n",
    "def print_results_summary(results):\n",
    "    # Factor Analysis Results\n",
    "    print(f\"\\nFACTOR RECOVERY ANALYSIS\")\n",
    "    \n",
    "    # Use the correct keys - these contain correlation results\n",
    "    classic_factors = np.array(results['classic_analysis'])  # Shape: (100, 3)\n",
    "    nonlinear_factors = np.array(results['nonlinear_analysis'])  # Shape: (100, 3)\n",
    "    \n",
    "    print(f\"\\nOverall Factor Correlations (mean across all factors and runs):\")\n",
    "    print(f\"  Classic:    {classic_factors.mean():.4f} ± {classic_factors.std():.4f}\")\n",
    "    print(f\"  Nonlinear:  {nonlinear_factors.mean():.4f} ± {nonlinear_factors.std():.4f}\")\n",
    "    \n",
    "    print(f\"\\nPer-Factor Correlations (mean across runs):\")\n",
    "    print(f\"{'Factor':<8} {'Classic':<12} {'Nonlinear':<12}\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    for i in range(classic_factors.shape[1]):\n",
    "        factor_classic_mean = classic_factors[:, i].mean()\n",
    "        factor_classic_std = classic_factors[:, i].std()\n",
    "        factor_nonlinear_mean = nonlinear_factors[:, i].mean()\n",
    "        factor_nonlinear_std = nonlinear_factors[:, i].std()\n",
    "        print(f\"{i+1:<8} {factor_classic_mean:.4f}±{factor_classic_std:.4f}   {factor_nonlinear_mean:.4f}±{factor_nonlinear_std:.4f}\")\n",
    "\n",
    "# Call the function\n",
    "print_results_summary(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
