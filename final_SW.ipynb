{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import Normalize\n",
    "from torch import nn\n",
    "import torch.optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    eta, u, v = x[:,0,:,:], x[:,1,:,:], x[:,2,:,:]\n",
    "    u_max, u_min = u.max(), u.min()\n",
    "    v_max, v_min = v.max(), v.min()\n",
    "    eta_max, eta_min = eta.max(), eta.min()\n",
    "    \n",
    "    eta_norm = 2 * (eta - eta_min) / (eta_max - eta_min) - 1\n",
    "    u_norm = 2 * (u - u_min) / (u_max - u_min) - 1\n",
    "    v_norm = 2 * (v - v_min) / (v_max - v_min) - 1\n",
    "\n",
    "    u_norm = u_norm.unsqueeze(1)\n",
    "    v_norm = v_norm.unsqueeze(1)\n",
    "    eta_norm = eta_norm.unsqueeze(1)\n",
    "\n",
    "    x_norm = torch.cat((eta_norm, u_norm, v_norm), dim=1)\n",
    "\n",
    "    return x_norm, [u_max, u_min, v_max, v_min, eta_max, eta_min]\n",
    "\n",
    "def denormalize(x_norm, max_min_vals):\n",
    "    eta_norm, u_norm, v_norm = x_norm[:,0,:,:], x_norm[:,1,:,:], x_norm[:,2,:,:]\n",
    "    u_max, u_min, v_max, v_min, eta_max, eta_min  = max_min_vals\n",
    "\n",
    "    u = ((u_norm + 1) / 2) * (u_max - u_min) + u_min\n",
    "    v = ((v_norm + 1) / 2) * (v_max - v_min) + v_min\n",
    "    eta = ((eta_norm + 1) / 2) * (eta_max - eta_min) + eta_min\n",
    "\n",
    "    u = u.unsqueeze(1)\n",
    "    v = v.unsqueeze(1)\n",
    "    eta = eta.unsqueeze(1)\n",
    "    x = torch.cat((eta, u, v), dim=1)\n",
    "    \n",
    "    return x\n",
    "    \n",
    "class dataset(Dataset): \n",
    "    def __init__(self, data, ic, length):\n",
    "        self.data = data\n",
    "        self.len = length\n",
    "        self.ic = ic\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    def __getitem__(self, idx):\n",
    "        dataPoint = self.data[idx, :, :, :]\n",
    "        initial_condition = self.ic[idx, :, :, :]\n",
    "        return dataPoint, initial_condition\n",
    "\n",
    "def training_loop(dataLoader, model, device, optimizer, loss_fn, epochs, path, batchSize, Mse_loss_fn):\n",
    "    for epoch in range(epochs):\n",
    "        for batch in dataLoader:\n",
    "            data, ic = batch\n",
    "            data = data.to(device)\n",
    "            ic = ic.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(data).view(batchSize, 3, 64, 64)\n",
    "            loss = loss_fn(pred, ic, Mse_loss_fn)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'epoch {epoch} : loss {loss.item()}') \n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "def loss_function(output, target, Mse_loss_fn):\n",
    "    # define a custom loss function for the model to train. Both output and pred have shapes [batchSize, 3, 64, 64].\n",
    "    # goal is to calculate channel wise-loss, and encourage sparsity in u and v \n",
    "    # MSELoss_fn must be a object of class nn.MSELoss() to calculate loss in eta\n",
    "\n",
    "    u_target, u_output = target[:, 1, :, :] , output[:, 1, :, :]\n",
    "    v_target, v_output = target[:, 2, :, :] , output[:, 2, :, :]\n",
    "    eta_target, eta_output = target[:, 0, :, :] , output[:, 0, :, :]\n",
    "\n",
    "    eta_error = Mse_loss_fn(eta_target, eta_output)\n",
    "    # u_error = lr * L1loss_fn(u_target, u_output) \n",
    "    # v_error = lr * L1loss_fn(v_target, v_output)\n",
    "    u_error = Mse_loss_fn(u_target, u_output) \n",
    "    v_error = Mse_loss_fn(v_target, v_output)\n",
    "\n",
    "    total_err = torch.stack([eta_error, u_error, v_error]).mean()\n",
    "\n",
    "    return total_err\n",
    "\n",
    "def plot_figs(idx, X1, X2):  \n",
    "    # function used to plot actual vs predicted initial conditions for 1 datapoint\n",
    "    x = np.linspace(0, 1, 64)\n",
    "    y = np.linspace(0, 1, 64)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "\n",
    "    eta1 = X1[idx, 0, :, :].cpu()\n",
    "    u1 = X1[idx, 1, :, :].cpu()\n",
    "    v1 = X1[idx, 2, :, :].cpu()\n",
    "    eta2 = X2[idx, 0, :, :].cpu()\n",
    "    u2 = X2[idx, 1, :, :].cpu()\n",
    "    v2 = X2[idx, 2, :, :].cpu()\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 8), subplot_kw={'projection': '3d'})\n",
    "\n",
    "    titles = [\n",
    "        \"Actual η\", \"Actual u\", \"Actual v\",\n",
    "        \"Predicted η\", \"Predicted u\", \"Predicted v\"\n",
    "    ]\n",
    "    Zs = [eta1, u1, v1, eta2, u2, v2]\n",
    "    cmaps = ['viridis', 'viridis', 'plasma', 'viridis', 'viridis', 'plasma']\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.plot_surface(X, Y, Zs[i], cmap=cmaps[i])\n",
    "        if i==1 or i==2 or i==4 or i==5:\n",
    "            ax.set_zlim(-1e-4, 1e-4)\n",
    "        ax.set_title(titles[i], fontsize=18, pad=10)\n",
    "        ax.set_xlabel('x', fontsize=12, labelpad=5)\n",
    "        ax.set_ylabel('y', fontsize=12, labelpad=5)\n",
    "        ax.tick_params(axis='both', labelsize=10)\n",
    "\n",
    "    # Try to reduce whitespace (tight_layout often has limited effect on 3D)\n",
    "    #plt.tight_layout(pad=2.0)\n",
    "    plt.subplots_adjust(hspace=0.3, wspace=0.1)  # manual tuning\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InverseCNNUpsampled(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # ----encoder----\n",
    "            nn.Conv2d(in_channels, 15, kernel_size=3, padding=1), # 3x64x64 --> 15x64x64\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0), # 15x64x64 --> 15x32x32\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(15, 30, kernel_size=3, padding=1), # 15x32x32 --> 30x32x32\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0), # 30x32x32 --> 30x16x16\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(30, 60, kernel_size=3, padding=1), # 30x16x16 --> 60x16x16\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0), # 60x16x16 --> 60x8x8\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(60, 128, kernel_size=3, padding=1), # 60x8x8 --> 128x8x8\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            # -----decoder-----\n",
    "            nn.Conv2d(in_channels=128, out_channels=60, kernel_size=3, stride=1, padding=1), # 128x8x8 --> 60x8x8\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Upsample(scale_factor=2), # 60x8x8 --> 60x16x16\n",
    "            nn.Conv2d(in_channels=60, out_channels=30, kernel_size=3, stride=1, padding=1), # 60x16x16 --> 30x16x16\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Upsample(scale_factor=2), # 30x16x16 --> 30x32x32\n",
    "            nn.Conv2d(in_channels=30, out_channels=15, kernel_size=3, stride=1, padding=1),  # 30x32x32 -> 15x32x32\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Upsample(scale_factor=2), # 15x32x32 --> 15x64x64\n",
    "            nn.ConvTranspose2d(in_channels=15, out_channels=3, kernel_size=3, stride=1, padding=1),  # 15x64x64 --> 3x64x64\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if createSWEdata.py is being run for the first time, run the first block of code to \n",
    "# load data into single files\n",
    "# if single files have already been created, then run only the second block of data to load the files\n",
    "\n",
    "# --- creating single files of data ---\n",
    "ic_names = [\"Gaussian Bump\", \"2 Gaussian Bumps\", \"Sinusoidal Wave Pattern\", \"Flat Conditions\"]\n",
    "num_dataPoints = 4 * 2500\n",
    "ic_set = torch.zeros(size=(num_dataPoints, 3, 64, 64)) # rows are eta,u,v for each datapoint. Columns are per datapoint. \n",
    "data_set = torch.zeros(size=(num_dataPoints, 3, 64, 64)) # each datapoint has shape 3x64x64 (3 channels over 64x64 grid)\n",
    "case = 64\n",
    "batches = 2500\n",
    "\n",
    "i=1\n",
    "for ic_name in ic_names:\n",
    "    x = torch.load(f'new_data/train_x1_{ic_name}_allBatch_Close.pt')[:2500, :, :, :]\n",
    "    print(x.shape)\n",
    "    ic = torch.load(f'new_data/train_ic_{ic_name}_allBatch.pt')[:2500, :, :, :]\n",
    "    print(x.shape)\n",
    "    data_set[batches*(i-1): batches*i] = x\n",
    "    ic_set[batches*(i-1): batches*i] = ic\n",
    "    i = i+1\n",
    "# save full datasets \n",
    "torch.save(data_set, f\"new_data/training_data_10000.pt\")\n",
    "torch.save(ic_set, f\"new_data/ic_training_data_10000.pt\")\n",
    "\n",
    "#\n",
    "# --- loading the data into variables ---\n",
    "#\n",
    "\n",
    "data_set = torch.load('new_data/training_data_10000.pt').clone()\n",
    "ic_set = torch.load('new_data/ic_training_data_10000.pt').clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 10000\n",
    "batchSize = 125\n",
    "# --- normalization of each channel using min and max, with output range [-1,1]\n",
    "dataset_normalized, min_max_vals = normalize(data_set)\n",
    "ic_normalized, min_max_vals_ic = normalize(ic_set)\n",
    "\n",
    "DataSet = dataset(dataset_normalized, ic_normalized, size)\n",
    "dataLoader = DataLoader(DataSet, shuffle=True, batch_size=batchSize, num_workers=0)\n",
    "loss_fn = nn.MSELoss(reduction='none')\n",
    "\n",
    "model_inverse = InverseCNNUpsampled(in_channels=3)\n",
    "model_inverse = model_inverse.to(device)\n",
    "\n",
    "# if model has already been stored, can directly load model using block of code below and skip training\n",
    "''' \n",
    "state_dict= torch.load(\"inv_model_upsampled.pth\", map_location='mps')\n",
    "model_inverse.load_state_dict(state_dict) \n",
    "'''\n",
    "\n",
    "optimizer_inverse = torch.optim.Adam(model_inverse.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- training loop ---\n",
    "\n",
    "epochs = 15\n",
    "path = 'inv_model.pth' # define path to store model states\n",
    "Mse_loss_fn = nn.MSELoss()\n",
    "\n",
    "training_loop(dataLoader, model_inverse, device, optimizer_inverse, loss_function, epochs, path, batchSize, Mse_loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- compute reconstructions and errors\n",
    "\n",
    "test_dataLoader = DataLoader(DataSet, shuffle=False, batch_size=batchSize)\n",
    "model = model_inverse\n",
    "model.eval()\n",
    "ics = torch.zeros(size=(size, 3, 64, 64)) # store initial conditions in new variable, in same order as reconstrctions\n",
    "reconstructions = torch.zeros(size=(size, 3, 64, 64))\n",
    "data_stored = torch.zeros(size=(size, 3, 64, 64)) # store data (input conditions) in a new variable, in same order as reconstructions\n",
    "i=0\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataLoader:\n",
    "        data, ic = batch\n",
    "        data, ic = data.to(device), ic.to(device)\n",
    "        pred = model(data).view(batchSize, 3, 64, 64)\n",
    "        reconstructions[i:batchSize+i, :, :, :] = pred\n",
    "        ics[i:batchSize+i, :, :, :] = ic\n",
    "        data_stored[i:batchSize+i, :, :, :] = data\n",
    "        i += batchSize\n",
    "\n",
    "reconstruction_denormalized = denormalize(reconstructions, min_max_vals_ic)\n",
    "ics_denormalized = denormalize(ics, min_max_vals_ic)\n",
    "data_denormalized = denormalize(data_stored, min_max_vals)\n",
    "\n",
    "total_err = torch.norm(reconstruction_denormalized - ics_denormalized) / torch.norm(ics_denormalized)\n",
    "eta_err = torch.norm(reconstruction_denormalized[:,0,:,:] - ics_denormalized[:,0,:,:] ) / torch.norm(ics_denormalized[:,0,:,:] )\n",
    "u_err = torch.norm(reconstruction_denormalized[:,1,:,:]  - ics_denormalized[:,1,:,:] ) / torch.norm(ics_denormalized[:,1,:,:] )\n",
    "v_err = torch.norm(reconstruction_denormalized[:,2,:,:]  - ics_denormalized[:,2,:,:] ) / torch.norm(ics_denormalized[:,2,:,:] )\n",
    "\n",
    "print(f'Total Error : {err}')\n",
    "print(f'eta error : {eta_err} , u error : {u_err} , v error : {v_err}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# --- plot initial conditions only, for 4 different data points ---\n",
    "#\n",
    "\n",
    "fig, axes = plt.subplots(2, 6, figsize=(15, 10), subplot_kw={'projection': '3d'})\n",
    "x = np.linspace(0, 1, 64)\n",
    "y = np.linspace(0, 1, 64)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "indices = [0,3000,6000,9000]\n",
    "gaussian = ics_denormalized[indices[0], :, :, :]\n",
    "two_gaussian = ics_denormalized[indices[1], :, :, :]\n",
    "sinusoidal = ics_denormalized[indices[2], :, :, :]\n",
    "flat = ics_denormalized[indices[3], :, :, :]\n",
    "\n",
    "print(Zs[1].shape)\n",
    "\n",
    "Zs = [gaussian[0,:,:], gaussian[1,:,:], gaussian[2,:,:], two_gaussian[0,:,:], two_gaussian[1,:,:], two_gaussian[2,:,:],\n",
    "    sinusoidal[0,:,:], sinusoidal[1,:,:], sinusoidal[2,:,:], flat[0,:,:], flat[1,:,:], flat[2,:,:] ]\n",
    "titles = [ \"η\", \"u\", \"v\",  \"η\", \"u\", \"v\",  \"η\", \"u\", \"v\",  \"η\", \"u\", \"v\" ]\n",
    "ics = [\"IC: Gaussian Bump \\nu\",\"IC: 2 Gaussian Bumps \\nu\", \"IC: Sinusoidal Wave \\nu\",\"IC: Flat Conditions \\nu\"]\n",
    "cmaps = ['viridis', 'viridis', 'plasma', 'viridis', 'viridis', 'plasma',\n",
    "        'viridis', 'viridis', 'plasma', 'viridis', 'viridis', 'plasma']\n",
    "\n",
    "k=0\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.plot_surface(X, Y, Zs[i], cmap=cmaps[i])\n",
    "    ax.set_title(titles[i], fontsize=20, pad=5)\n",
    "    if ((i-1)%3==0):\n",
    "        ax.set_title(ics[k], fontsize=20, pad=10)\n",
    "        k += 1\n",
    "    ax.set_xlabel('x', fontsize=12, labelpad=5)\n",
    "    ax.set_ylabel('y', fontsize=12, labelpad=5)\n",
    "    ax.tick_params(axis='both', labelsize=5)\n",
    "    ax.axis('tight')\n",
    "    \n",
    "plt.subplots_adjust(hspace=0.1, wspace=0.1)  # manual tuning\n",
    "\n",
    "plt.tight_layout()  # Adjusts spacing to prevent overlap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# --- plot predictions vs actual initial conditions for 1 datapoint ---\n",
    "#\n",
    "idx = 3000\n",
    "original = ics_denormalized\n",
    "predicted = reconstruction_denormalized\n",
    "\n",
    "plot_figs(idx, original, predicted)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
