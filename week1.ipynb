{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67b54735",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0cb8bb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "tensor_transform = transforms.ToTensor()\n",
    "dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=tensor_transform)\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb88c780",
   "metadata": {},
   "source": [
    "# Part 1: Data of Known Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5044ba",
   "metadata": {},
   "source": [
    "### Data Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5bab7744",
   "metadata": {},
   "outputs": [],
   "source": [
    "numSamples = 1000 # must be large for X to be SPD\n",
    "dim = 784 # dimension of the data (chosen based on the 28x28 MNIST dataset)\n",
    "r = 100 # maximum rank of the mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5587894",
   "metadata": {},
   "source": [
    "### Theoretical Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d94b1304",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(numSamples, dim)\n",
    "# mu = 0 by default in numpy Gaussian distribution\n",
    "\n",
    "gamma_X = np.eye(dim)  # known covariance for N(0, I)\n",
    "L_X = np.linalg.cholesky(gamma_X)  # Cholesky decomposition\n",
    "\n",
    "U, Sigma, V_T = np.linalg.svd(L_X)  # SVD\n",
    "U_r = U[:, :r]  # first r left singular vectors of L_X\n",
    "\n",
    "A_opt = U_r @ U_r.T  # Eckart-Young solution to the transformed minimization problem\n",
    "\n",
    "# Minimized expected error\n",
    "error_opt = np.linalg.norm((A_opt - np.eye(dim)) @ L_X, ord='fro')**2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f4757a",
   "metadata": {},
   "source": [
    "### Computational, Autoencoder Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7be0e96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "# Create the autoencoder class with one hidden layer\n",
    "class LinearAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, bottleneck_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(input_dim, bottleneck_dim, bias=False)\n",
    "        self.decoder = nn.Linear(bottleneck_dim, input_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon\n",
    "    \n",
    "model = LinearAutoencoder(input_dim=dim, bottleneck_dim=r)\n",
    "\n",
    "# Training\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "X_loader = torch.utils.data.DataLoader(X_tensor, batch_size=128, shuffle=True)\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in X_loader:\n",
    "        optimizer.zero_grad()\n",
    "        recon = model(batch)\n",
    "        loss = criterion(recon, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * batch.size(0)\n",
    "    avg_loss = total_loss / numSamples\n",
    "    #if epoch % 10 == 0:\n",
    "        #print(f\"Epoch {epoch:02d}, MSE: {avg_loss:.6f}\")\n",
    "\n",
    "# Reconstruct learned projection matrix A\n",
    "W = model.decoder.weight @ model.encoder.weight  # A_learned = W_decoder * W_encoder\n",
    "A_learned = W.detach().numpy()  # shape: (784, 784)\n",
    "\n",
    "error_learned = np.linalg.norm((A_learned - np.eye(dim)) @ L_X, ord='fro')**2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8502a850",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2ba4ed09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal A error: 684.000\n",
      "Learned A error: 686.677\n"
     ]
    }
   ],
   "source": [
    "print(f\"Optimal A error: {error_opt:.3f}\")\n",
    "print(f\"Learned A error: {error_learned:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e76ffe7",
   "metadata": {},
   "source": [
    "# Part 2: Data from MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebe5db3",
   "metadata": {},
   "source": [
    "### Data Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cdee8874",
   "metadata": {},
   "outputs": [],
   "source": [
    "numSamples = 1000 # must be large for X to be SPD\n",
    "dim = 784 # dimension of the data (chosen based on the 28x28 MNIST dataset)\n",
    "r = 100 # maximum rank of the mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faff1dd",
   "metadata": {},
   "source": [
    "### Theoretical Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "30304bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Windows\\Temp\\ipykernel_4028\\3743025898.py:9: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  gamma_X += 1e-5*np.eye(gamma_X.shape[0]) # add the regularization term to ensure SPD\n",
      "C:\\Windows\\Temp\\ipykernel_4028\\3743025898.py:18: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  error_opt = np.linalg.norm((A_opt - np.eye(dim)) @ L_X,ord='fro')**2\n"
     ]
    }
   ],
   "source": [
    "flatten = True\n",
    "\n",
    "X_raw = torch.stack([dataset[i][0] for i in range(numSamples)]) # grab numSamples images from MNIST\n",
    "X = X_raw.view(numSamples, -1) if flatten else X_raw\n",
    "mu = X.mean(dim=0, keepdim=True) # must now calculate the sample mean\n",
    "X = X - mu\n",
    "\n",
    "gamma_X = (X.T @ X) / (numSamples - 1) # unbiased second moment (covariance matrix in this case??)\n",
    "gamma_X += 1e-5*np.eye(gamma_X.shape[0]) # add the regularization term to ensure SPD\n",
    "L_X = np.linalg.cholesky(gamma_X) # Cholesky decomposition\n",
    "\n",
    "U, Sigma, V_T = np.linalg.svd(L_X) # SVD\n",
    "U_r = U[:, :r] # first r left singular vectors of L_X\n",
    "\n",
    "A_opt = U_r @ U_r.T # Echard-Young solution to the transformed minimization problem\n",
    "\n",
    "# Minimized expected error\n",
    "error_opt = np.linalg.norm((A_opt - np.eye(dim)) @ L_X,ord='fro')**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bcf525",
   "metadata": {},
   "source": [
    "### Computational, Autoencoder Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c1ea19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Windows\\Temp\\ipykernel_4028\\2912211445.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X, dtype=torch.float32) # updates X_tensor to account for the new data\n"
     ]
    }
   ],
   "source": [
    "X_tensor = torch.tensor(X, dtype=torch.float32) # updates X_tensor to account for the new data\n",
    "\n",
    "model = LinearAutoencoder(input_dim=dim, bottleneck_dim=r)\n",
    "\n",
    "# Training\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "X_loader = torch.utils.data.DataLoader(X_tensor, batch_size=128, shuffle=True)\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in X_loader:\n",
    "        optimizer.zero_grad()\n",
    "        recon = model(batch)\n",
    "        loss = criterion(recon, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * batch.size(0)\n",
    "    avg_loss = total_loss / numSamples\n",
    "    #if epoch % 10 == 0:\n",
    "        #print(f\"Epoch {epoch:02d}, MSE: {avg_loss:.6f}\")\n",
    "\n",
    "# Reconstruct learned projection matrix A\n",
    "W = model.decoder.weight @ model.encoder.weight  # A_learned = W_decoder * W_encoder\n",
    "A_learned = W.detach().numpy()  # shape: (784, 784)\n",
    "L_X = L_X.detach().numpy() # L_X is a tensor since above we made X a tensor, thus we must turn it back into a ndarray\n",
    "\n",
    "error_learned = np.linalg.norm((A_learned - np.eye(dim)) @ L_X, ord='fro')**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9c581a",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "624112e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal A error: 3.667\n",
      "Learned A error: 5.077\n"
     ]
    }
   ],
   "source": [
    "print(f\"Optimal A error: {error_opt:.3f}\")\n",
    "print(f\"Learned A error: {error_learned:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
