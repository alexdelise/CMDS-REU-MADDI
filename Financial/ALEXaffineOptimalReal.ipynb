{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a977f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['PYTHONHASHSEED'] = '42'\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import random_split, DataLoader, TensorDataset\n",
    "from scipy import stats\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "from sklearn.decomposition import PCA\n",
    "import requests, zipfile, io\n",
    "from zipfile import ZipFile\n",
    "from collections import Counter\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1f4bfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9721e8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data for 33 stocks...\n",
      "Date range: 2015-01-01 to 2024-12-31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10 Failed downloads:\n",
      "['V', 'AMZN', 'PFE', 'MRK', 'GS', 'MSFT', 'AAPL', 'NVDA', 'JNJ', 'JPM']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed for batch 1: Empty DataFrame returned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10 Failed downloads:\n",
      "['JPM', 'NVDA', 'V', 'AAPL', 'JNJ', 'PFE', 'MSFT', 'AMZN', 'MRK', 'GS']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 2 failed for batch 1: Empty DataFrame returned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10 Failed downloads:\n",
      "['V', 'GS', 'MSFT', 'AAPL', 'MRK', 'JPM', 'PFE', 'NVDA', 'JNJ', 'AMZN']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 3 failed for batch 1: Empty DataFrame returned\n",
      "Batch failed: ['AAPL', 'MSFT', 'NVDA', 'JPM', 'GS', 'V', 'JNJ', 'PFE', 'MRK', 'AMZN']\n",
      "Completed batch 1/4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 204\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    199\u001b[0m     framework \u001b[38;5;241m=\u001b[39m EnhancedStockDataFramework(\n\u001b[0;32m    200\u001b[0m         start_date\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2015-01-01\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    201\u001b[0m         end_date\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2024-12-31\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    202\u001b[0m     )\n\u001b[1;32m--> 204\u001b[0m     success \u001b[38;5;241m=\u001b[39m \u001b[43mframework\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_complete_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline completed successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 182\u001b[0m, in \u001b[0;36mEnhancedStockDataFramework.run_complete_pipeline\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_complete_pipeline\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    180\u001b[0m     tickers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_stock_universe()\n\u001b[1;32m--> 182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtickers\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    183\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPIPELINE FAILED: Could not download any data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    184\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[15], line 56\u001b[0m, in \u001b[0;36mEnhancedStockDataFramework.download_data\u001b[1;34m(self, tickers, max_retries, batch_size, pause)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_retries):\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 56\u001b[0m         df \u001b[38;5;241m=\u001b[39m \u001b[43myf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtickers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgroup_by\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mticker\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m            \u001b[49m\u001b[43mauto_adjust\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m            \u001b[49m\u001b[43mthreads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m df\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m     67\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmpty DataFrame returned\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\alexr\\Downloads\\Lib\\site-packages\\yfinance\\utils.py:104\u001b[0m, in \u001b[0;36mlog_indent_decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEntering \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m IndentationContext():\n\u001b[1;32m--> 104\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExiting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\alexr\\Downloads\\Lib\\site-packages\\yfinance\\multi.py:162\u001b[0m, in \u001b[0;36mdownload\u001b[1;34m(tickers, start, end, actions, threads, ignore_tz, group_by, auto_adjust, back_adjust, repair, keepna, progress, period, interval, prepost, proxy, rounding, timeout, session, multi_level_index)\u001b[0m\n\u001b[0;32m    158\u001b[0m _multitasking\u001b[38;5;241m.\u001b[39mset_max_threads(threads)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, ticker \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tickers):\n\u001b[0;32m    160\u001b[0m     _download_one_threaded(ticker, period\u001b[38;5;241m=\u001b[39mperiod, interval\u001b[38;5;241m=\u001b[39minterval,\n\u001b[0;32m    161\u001b[0m                            start\u001b[38;5;241m=\u001b[39mstart, end\u001b[38;5;241m=\u001b[39mend, prepost\u001b[38;5;241m=\u001b[39mprepost,\n\u001b[1;32m--> 162\u001b[0m                            actions\u001b[38;5;241m=\u001b[39mactions, auto_adjust\u001b[38;5;241m=\u001b[39mauto_adjust,\n\u001b[0;32m    163\u001b[0m                            back_adjust\u001b[38;5;241m=\u001b[39mback_adjust, repair\u001b[38;5;241m=\u001b[39mrepair, keepna\u001b[38;5;241m=\u001b[39mkeepna,\n\u001b[0;32m    164\u001b[0m                            progress\u001b[38;5;241m=\u001b[39m(progress \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m),\n\u001b[0;32m    165\u001b[0m                            rounding\u001b[38;5;241m=\u001b[39mrounding, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shared\u001b[38;5;241m.\u001b[39m_DFS) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(tickers):\n\u001b[0;32m    167\u001b[0m     _time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class EnhancedStockDataFramework:\n",
    "    def __init__(self, start_date=\"2015-01-01\", end_date=\"2024-12-31\"):\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.raw_data = None\n",
    "        self.processed_data = None\n",
    "        self.failed_downloads = []\n",
    "        self.successful_tickers = []\n",
    "        \n",
    "    def get_stock_universe(self):\n",
    "        tickers = [\n",
    "            # tech\n",
    "            'AAPL', 'MSFT', 'NVDA',\n",
    "            # finance\n",
    "            'JPM', 'GS', 'V',\n",
    "            # health and pharma \n",
    "            'JNJ', 'PFE', 'MRK',\n",
    "            # consumer discretionary \n",
    "            'AMZN', 'TSLA', 'HD',\n",
    "            # consumer staples\n",
    "            'PG', 'KO', 'PEP',\n",
    "            # industrial and manufacturing\n",
    "            'BA', 'CAT', 'GE',\n",
    "            # energy and utilities\n",
    "            'XOM', 'CVX', 'NEE',\n",
    "            # materials and basic industries \n",
    "            'LIN', 'SHW', 'FCX',\n",
    "            # real estate and REITs\n",
    "            'AMT', 'SPG', 'PLD',\n",
    "            # communication services\n",
    "            'GOOGL', 'META', 'DIS',\n",
    "            # emerging growth and others\n",
    "            'SHOP', 'PLTR', 'SNOW'\n",
    "        ]\n",
    "        return tickers\n",
    "\n",
    "    \n",
    "    def download_data(self, tickers, max_retries=3, batch_size=10, pause=3):\n",
    "        print(f\"Downloading data for {len(tickers)} stocks...\")\n",
    "        print(f\"Date range: {self.start_date} to {self.end_date}\")\n",
    "\n",
    "        all_data = {}\n",
    "        failed_downloads = []\n",
    "\n",
    "        for i in range(0, len(tickers), batch_size):\n",
    "            batch = tickers[i:i+batch_size]\n",
    "            success = False\n",
    "\n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    df = yf.download(\n",
    "                        tickers=batch,\n",
    "                        start=self.start_date,\n",
    "                        end=self.end_date,\n",
    "                        group_by='ticker',\n",
    "                        auto_adjust=False,\n",
    "                        threads=True,\n",
    "                        progress=False\n",
    "                    )\n",
    "\n",
    "                    if df.empty:\n",
    "                        raise ValueError(\"Empty DataFrame returned\")\n",
    "\n",
    "                    for t in batch:\n",
    "                        try:\n",
    "                            close = df[t]['Close']\n",
    "                            if not close.empty and len(close) > 252:\n",
    "                                all_data[t] = close\n",
    "                            else:\n",
    "                                print(f\"Insufficient data for {t}\")\n",
    "                                failed_downloads.append(t)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Ticker failed in batch: {t} — {e}\")\n",
    "                            failed_downloads.append(t)\n",
    "\n",
    "                    success = True\n",
    "                    break\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Attempt {attempt + 1} failed for batch {i//batch_size+1}: {e}\")\n",
    "                    time.sleep(pause)\n",
    "\n",
    "            if not success:\n",
    "                print(f\"Batch failed: {batch}\")\n",
    "                failed_downloads.extend(batch)\n",
    "\n",
    "            print(f\"Completed batch {i//batch_size + 1}/{(len(tickers) + batch_size - 1) // batch_size}\")\n",
    "            time.sleep(pause)\n",
    "\n",
    "        self.raw_data = pd.DataFrame(all_data)\n",
    "        self.failed_downloads = list(set(failed_downloads))\n",
    "        self.successful_tickers = list(all_data.keys())\n",
    "\n",
    "        if not self.raw_data.empty:\n",
    "            print(f\"\\n✅ Successfully downloaded {len(all_data)} tickers\")\n",
    "            if self.failed_downloads:\n",
    "                print(f\"❌ Failed: {len(self.failed_downloads)} tickers\")\n",
    "                print(f\"Examples: {self.failed_downloads[:10]}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"ERROR: No data downloaded.\")\n",
    "            return False\n",
    "\n",
    "    def clean_data(self, min_trading_days=252, max_zero_days=5):\n",
    "        if self.raw_data is None or self.raw_data.empty:\n",
    "            print(\"ERROR: No raw data available for cleaning\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"Initial data shape: {self.raw_data.shape}\")\n",
    "        \n",
    "        self.raw_data = self.raw_data.dropna(thresh=min_trading_days, axis=1)\n",
    "        print(f\"After removing stocks with insufficient data: {self.raw_data.shape}\")\n",
    "        \n",
    "        zero_counts = (self.raw_data <= 0).sum()\n",
    "        valid_stocks = zero_counts[zero_counts <= max_zero_days].index\n",
    "        self.raw_data = self.raw_data[valid_stocks]\n",
    "        print(f\"After removing stocks with excessive zero prices: {self.raw_data.shape}\")\n",
    "        \n",
    "        self.raw_data = self.raw_data.fillna(method='ffill', limit=5)\n",
    "        self.raw_data = self.raw_data.dropna()\n",
    "\n",
    "        if self.raw_data.empty:\n",
    "            print(\"ERROR: No data remaining after cleaning!\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"Final cleaned data shape: {self.raw_data.shape}\")\n",
    "        return True\n",
    "\n",
    "    def calculate_returns(self):\n",
    "        if self.raw_data is None or self.raw_data.empty:\n",
    "            print(\"ERROR: No data available for return calculation\")\n",
    "            return False\n",
    "\n",
    "        returns = self.raw_data.pct_change().dropna()\n",
    "        extreme_threshold = 0.5\n",
    "        returns_clean = returns.clip(lower=-extreme_threshold, upper=extreme_threshold)\n",
    "\n",
    "        mean_returns = returns_clean.mean()\n",
    "        volatility = returns_clean.std() * np.sqrt(252)\n",
    "\n",
    "        self.processed_data = {\n",
    "            'returns': returns_clean,\n",
    "            'prices': self.raw_data,\n",
    "            'mean_daily_return': mean_returns.mean(),\n",
    "            'mean_daily_volatility': volatility.mean(),\n",
    "            'num_stocks': len(returns_clean.columns),\n",
    "            'num_days': len(returns_clean),\n",
    "            'date_range': f\"{returns_clean.index[0].strftime('%Y-%m-%d')} to {returns_clean.index[-1].strftime('%Y-%m-%d')}\"\n",
    "        }\n",
    "\n",
    "        return True\n",
    "\n",
    "    def generate_summary(self):\n",
    "        if self.processed_data is None:\n",
    "            print(\"ERROR: No processed data available\")\n",
    "            return\n",
    "\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ENHANCED STOCK DATA FRAMEWORK - SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"Successful downloads: {self.processed_data['num_stocks']} stocks\")\n",
    "        print(f\"Failed downloads: {len(self.failed_downloads)} stocks\")\n",
    "        print(f\"Date range: {self.processed_data['date_range']}\")\n",
    "        print(f\"Trading days: {self.processed_data['num_days']}\")\n",
    "        \n",
    "        print(f\"\\nData Quality Metrics:\")\n",
    "        print(f\"Mean daily return: {self.processed_data['mean_daily_return']:.6f}\")\n",
    "        print(f\"Mean daily volatility: {self.processed_data['mean_daily_volatility']:.4f}\")\n",
    "        \n",
    "        if self.failed_downloads:\n",
    "            print(f\"\\n Failed tickers: {self.failed_downloads[:10]}\")\n",
    "\n",
    "    def run_complete_pipeline(self):\n",
    "        tickers = self.get_stock_universe()\n",
    "        \n",
    "        if not self.download_data(tickers):\n",
    "            print(\"PIPELINE FAILED: Could not download any data\")\n",
    "            return False\n",
    "        \n",
    "        if not self.clean_data():\n",
    "            print(\"PIPELINE FAILED: No data remaining after cleaning\")\n",
    "            return False\n",
    "        \n",
    "        if not self.calculate_returns():\n",
    "            print(\"PIPELINE FAILED: Could not calculate returns\")\n",
    "            return False\n",
    "        \n",
    "        self.generate_summary()\n",
    "        return True\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    framework = EnhancedStockDataFramework(\n",
    "        start_date=\"2015-01-01\",\n",
    "        end_date=\"2024-12-31\"\n",
    "    )\n",
    "    \n",
    "    success = framework.run_complete_pipeline()\n",
    "    \n",
    "    if success:\n",
    "        print(\"Pipeline completed successfully!\")\n",
    "        print(f\"Access your data via: framework.processed_data\")\n",
    "        \n",
    "        returns = framework.processed_data['returns']\n",
    "        prices = framework.processed_data['prices']\n",
    "        \n",
    "        print(f\"\\nReturns shape: {returns.shape}\")\n",
    "        print(f\"Prices shape: {prices.shape}\")\n",
    "    else:\n",
    "        print(\"Pipeline failed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f02ca6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max() iterable argument is empty",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 103\u001b[0m\n\u001b[0;32m    100\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_axisbelow(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# Set y-axis to start at 0 and add some padding at the top\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_ylim(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcounts\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1.1\u001b[39m)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# Ensure integer ticks on y-axis\u001b[39;00m\n\u001b[0;32m    106\u001b[0m ax\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mset_major_locator(plt\u001b[38;5;241m.\u001b[39mMaxNLocator(integer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "\u001b[1;31mValueError\u001b[0m: max() iterable argument is empty"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Set publication-quality plot parameters\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12,\n",
    "    'axes.linewidth': 1.2,\n",
    "    'axes.labelsize': 12,\n",
    "    'axes.titlesize': 13,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 10,\n",
    "    'figure.titlesize': 14,\n",
    "    'lines.linewidth': 1.5,\n",
    "    'axes.grid': False,\n",
    "    'axes.axisbelow': True,\n",
    "    'figure.facecolor': 'white',\n",
    "    'axes.facecolor': 'white'\n",
    "})\n",
    "\n",
    "# Function to get market cap category for a ticker\n",
    "def get_market_cap_category(ticker):\n",
    "    try:\n",
    "        stock = yf.Ticker(ticker)\n",
    "        info = stock.info\n",
    "        market_cap = info.get('marketCap', None)\n",
    "        if market_cap is None:\n",
    "            return 'Unknown'\n",
    "        elif market_cap >= 10e9:\n",
    "            return 'Large Cap (≥ $10B)'\n",
    "        elif 2e9 <= market_cap < 10e9:\n",
    "            return 'Mid Cap ($2B–$10B)'\n",
    "        elif market_cap < 2e9:\n",
    "            return 'Small Cap (< $2B)'\n",
    "        else:\n",
    "            return 'Unknown'\n",
    "    except Exception as e:\n",
    "        return 'Unknown'\n",
    "\n",
    "# Map sectors (same as before)\n",
    "sector_map = {\n",
    "    **dict.fromkeys(['AAPL', 'MSFT', 'NVDA'], 'Tech Giants'),\n",
    "    **dict.fromkeys(['JPM', 'GS', 'V'], 'Financial Services'),\n",
    "    **dict.fromkeys(['JNJ', 'PFE', 'MRK'], 'Healthcare & Pharma'),\n",
    "    **dict.fromkeys(['AMZN', 'TSLA', 'HD'], 'Consumer Discretionary'),\n",
    "    **dict.fromkeys(['PG', 'KO', 'PEP'], 'Consumer Staples'),\n",
    "    **dict.fromkeys(['BA', 'CAT', 'GE'], 'Industrial & Manufacturing'),\n",
    "    **dict.fromkeys(['XOM', 'CVX', 'NEE'], 'Energy & Utilities'),\n",
    "    **dict.fromkeys(['LIN', 'SHW', 'FCX'], 'Materials & Basic Industries'),\n",
    "    **dict.fromkeys(['AMT', 'SPG', 'PLD'], 'Real Estate & REITs'),\n",
    "    **dict.fromkeys(['GOOGL', 'META', 'DIS'], 'Communication Services'),\n",
    "    **dict.fromkeys(['SHOP', 'PLTR', 'SNOW'], 'Emerging Growth & Others')\n",
    "}\n",
    "\n",
    "\n",
    "# Get market caps and sectors for successfully downloaded tickers\n",
    "sectors = []\n",
    "for ticker in framework.successful_tickers:\n",
    "    sectors.append(sector_map.get(ticker, 'Unknown'))\n",
    "\n",
    "# Count distribution\n",
    "sector_counts = Counter(sectors)\n",
    "\n",
    "# Create publication-quality figure\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Sort sectors by count for better visualization\n",
    "sorted_sectors = sorted(sector_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "sector_names = [item[0] for item in sorted_sectors]\n",
    "counts = [item[1] for item in sorted_sectors]\n",
    "\n",
    "# Create bars with professional styling\n",
    "bars = ax.bar(range(len(sector_names)), counts, \n",
    "              color='#D3869B', alpha=0.8, linewidth=0.5)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xlabel('Industry Sector', fontsize=14, fontweight='normal')\n",
    "ax.set_ylabel('Number of Stocks', fontsize=14, fontweight='normal')\n",
    "\n",
    "# Set x-axis labels\n",
    "ax.set_xticks(range(len(sector_names)))\n",
    "ax.set_xticklabels(sector_names, rotation=45, ha='right', fontsize=10)\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for i, (bar, count) in enumerate(zip(bars, counts)):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "            f'{count}', ha='center', va='bottom', fontsize=10, fontweight='normal')\n",
    "\n",
    "# Customize grid\n",
    "#ax.grid(True, alpha=0.3, linestyle='-', linewidth=0.5, axis='y')\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "# Set y-axis to start at 0 and add some padding at the top\n",
    "ax.set_ylim(0, max(counts) * 1.1)\n",
    "\n",
    "# Ensure integer ticks on y-axis\n",
    "ax.yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "\n",
    "# Adjust layout to prevent label cutoff\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save high-quality versions\n",
    "plt.savefig('figure_sector_distribution.png', dpi=300, bbox_inches='tight', \n",
    "            facecolor='white', edgecolor='none')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Reset matplotlib parameters\n",
    "plt.rcdefaults()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a10502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets the fama french factors from the site for the date range \n",
    "\n",
    "def get_daily_fama_french_factors(start_date='2021-11-11', end_date='2024-12-31'):\n",
    "    \"\"\"\n",
    "    Downloads and processes the Fama-French 3-factor daily data from Ken French's website.\n",
    "    \"\"\"\n",
    "    url = \"https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Research_Data_Factors_daily_CSV.zip\"\n",
    "    r = requests.get(url)\n",
    "    z = ZipFile(io.BytesIO(r.content))\n",
    "    \n",
    "    # Correct filename from ZIP\n",
    "    with z.open(\"F-F_Research_Data_Factors_daily.csv\") as f:\n",
    "        df_raw = pd.read_csv(f, skiprows=3)\n",
    "\n",
    "    # Drop footer rows that contain non-date strings\n",
    "    df_raw = df_raw[df_raw.iloc[:, 0].str.match(r'^\\d{6,8}$')].copy()\n",
    "\n",
    "    # Rename columns\n",
    "    df_raw.columns = ['Date', 'Mkt-RF', 'SMB', 'HML', 'RF']\n",
    "    df_raw['Date'] = pd.to_datetime(df_raw['Date'], format='%Y%m%d')\n",
    "    df_raw.set_index('Date', inplace=True)\n",
    "\n",
    "    # Convert percent to decimal\n",
    "    df_raw = df_raw.astype(float) / 100.0\n",
    "\n",
    "    # Filter by date range\n",
    "    mask = (df_raw.index >= pd.to_datetime(start_date)) & (df_raw.index <= pd.to_datetime(end_date))\n",
    "    return df_raw.loc[mask]\n",
    "\n",
    "# Example usage\n",
    "ff_factors = get_daily_fama_french_factors('2021-11-11', '2024-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407da410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now its time put iin all of the frameworks for autoencoders. \n",
    "def compute_optimal_A_b_mu(X_np, r):\n",
    "    mu = np.mean(X_np, axis=0)\n",
    "    cov = (X_np - mu).T @ (X_np - mu)\n",
    "    U, S, _ = np.linalg.svd(cov)\n",
    "    Ur = U[:, :r]  # shape (d, r)\n",
    "    \n",
    "    A = Ur.T  # encoder projects from d->r: (r, d)\n",
    "    b = np.zeros(r)  # no bias needed in latent space\n",
    "    return A, b, mu\n",
    "\n",
    "\n",
    "# python packaged affine autoencoder\n",
    "\n",
    "class ClassicAffineAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, bottleneck_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(input_dim, bottleneck_dim, bias=True)\n",
    "        self.decoder = nn.Linear(bottleneck_dim, input_dim, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "    \n",
    "\n",
    "class OptimalAffineAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, r, A, b, mu):\n",
    "        super().__init__()\n",
    "        self.A = torch.tensor(A, dtype=torch.float32)\n",
    "        self.b = torch.tensor(b, dtype=torch.float32)\n",
    "        self.mu = torch.tensor(mu, dtype=torch.float32)\n",
    "\n",
    "    def encoder(self, x):\n",
    "        # Return centered projected latent space: Ur^T(x - mu)\n",
    "        x_centered = x - self.mu  # center by mean\n",
    "        UrT = self.A.T  # A = Ur Ur^T ⇒ A.T = Ur^T Ur ⇒ encoder ~ Ur^T\n",
    "        return x_centered @ UrT  # [batch_size, d] x [d, r] → [batch_size, r]\n",
    "    \n",
    "    def decoder(self, z):\n",
    "        # Reconstruct from latent space: Ur * z + mu\n",
    "        return z @ self.A + self.mu  # [batch_size, r] x [r, d] → [batch_size, d]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Complete encode-decode cycle\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)\n",
    "# this is a combination modeel that allows for training after instantiation of optimal theoretical weights. \n",
    "\n",
    "class OptimalTrainableAffineAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, r, A_init, b_init, mu_init):\n",
    "        super().__init__()\n",
    "        self.A = nn.Parameter(torch.tensor(A_init, dtype=torch.float32))  # (r, d)\n",
    "        self.b = nn.Parameter(torch.tensor(b_init, dtype=torch.float32))  # (r,)\n",
    "        self.mu = nn.Parameter(torch.tensor(mu_init, dtype=torch.float32))  # (d,)\n",
    "\n",
    "    def encoder(self, x):\n",
    "        x_centered = x - self.mu  # (batch, d)\n",
    "        return x_centered @ self.A.T + self.b  # (batch, r)\n",
    "\n",
    "    def decoder(self, z):\n",
    "        return z @ self.A + self.mu  # (batch, d)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)\n",
    "\n",
    "# NONLINEAR AUTOENCODER \n",
    "class NonlinearAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, bottleneck_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder: input -> hidden -> hidden -> bottleneck\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, bottleneck_dim)\n",
    "        )\n",
    "        \n",
    "        # Decoder: bottleneck -> hidden -> hidden -> output\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(bottleneck_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "\n",
    "def train_autoencoder(model, train_loader, val_loader, num_epochs=300, lr=1e-3):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon = model(batch)\n",
    "            loss = criterion(recon, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item() * batch.size(0)\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader.dataset)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                recon = model(batch)\n",
    "                loss = criterion(recon, batch)\n",
    "                total_val_loss += loss.item() * batch.size(0)\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(avg_val_loss)\n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "\n",
    "\n",
    "# validation function only code for the optimal affine autoencoder\n",
    "def valOnlyOptimalAffineAutoencoder(model, val_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = batch.to(device)\n",
    "            recon = model(batch)\n",
    "            loss = criterion(recon, batch)\n",
    "            total_loss += loss.item() * batch.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader.dataset)\n",
    "    return avg_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead5b4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (787, 194)\n",
      "Data type: <class 'pandas.core.frame.DataFrame'>\n",
      "Tensor shape: torch.Size([787, 194])\n",
      "Returns index dtype: datetime64[ns, America/New_York], tz: America/New_York\n",
      "FF factors index dtype: datetime64[ns], tz: None\n",
      "Number of common dates after tz fix: 787\n",
      "\n",
      "Fama-French (GT) Factor Analysis:\n",
      "  Explained Variance Ratio: [0.18158941 0.0213758  0.04943703]\n",
      "  Cumulative Variance: [0.18158941 0.20296521 0.25240223]\n",
      "  Rotated Explained Variance Ratio: [0.12571907 0.01839108 0.10343888]\n",
      "  Rotated Cumulative Variance: [0.12571907 0.14411016 0.24754904]\n",
      "FINAL RESULTS SUMMARY\n",
      "classic_mse: mean = 0.00040797,  min = 0.00040797, max = 0.00040797\n",
      "optimal_mse: mean = 0.00028766,  min = 0.00028766, max = 0.00028766\n",
      "trainable_mse: mean = 0.00029120,  min = 0.00029120, max = 0.00029120\n",
      "pca_mse: mean = 0.00029557,  min = 0.00029557, max = 0.00029557\n",
      "nonlinear_mse: mean = 0.00029912,  min = 0.00029912, max = 0.00029912\n",
      "FACTOR STABILITY ANALYSIS\n",
      "\n",
      "CLASSIC - Rotated Factor Stability:\n",
      "  Factor 1 (Market?): 0.047\n",
      "  Factor 2 (Size?):   0.000 \n",
      "  Factor 3 (Value?):  0.018 \n",
      "  Total Explained Variance: 0.065\n",
      "  Factor Balance (higher=better): 0.000\n",
      "\n",
      "OPTIMAL - Rotated Factor Stability:\n",
      "  Factor 1 (Market?): 0.126\n",
      "  Factor 2 (Size?):   0.137 \n",
      "  Factor 3 (Value?):  0.080 \n",
      "  Total Explained Variance: 0.342\n",
      "  Factor Balance (higher=better): 0.586\n",
      "\n",
      "TRAINABLE - Rotated Factor Stability:\n",
      "  Factor 1 (Market?): 0.116\n",
      "  Factor 2 (Size?):   0.141 \n",
      "  Factor 3 (Value?):  0.079 \n",
      "  Total Explained Variance: 0.336\n",
      "  Factor Balance (higher=better): 0.563\n",
      "\n",
      "PCA - Rotated Factor Stability:\n",
      "  Factor 1 (Market?): 0.109\n",
      "  Factor 2 (Size?):   0.144 \n",
      "  Factor 3 (Value?):  0.080 \n",
      "  Total Explained Variance: 0.333\n",
      "  Factor Balance (higher=better): 0.555\n",
      "\n",
      "FF - Rotated Factor Stability:\n",
      "  Factor 1 (Market?): 0.126\n",
      "  Factor 2 (Size?):   0.018 \n",
      "  Factor 3 (Value?):  0.103 \n",
      "  Total Explained Variance: 0.248\n",
      "  Factor Balance (higher=better): 0.146\n",
      "\n",
      "NONLINEAR - Rotated Factor Stability:\n",
      "  Factor 1 (Market?): 0.140\n",
      "  Factor 2 (Size?):   0.000 \n",
      "  Factor 3 (Value?):  0.105 \n",
      "  Total Explained Variance: 0.245\n",
      "  Factor Balance (higher=better): 0.003\n"
     ]
    }
   ],
   "source": [
    "# VARIMAX AND EXPLAINED VARIANCE FUNCTION DEFINITIONS \n",
    "def varimax_rotation(loadings, gamma=1.0, q=20, tol=1e-6):\n",
    "    loadings = np.array(loadings)\n",
    "    n_features, n_factors = loadings.shape\n",
    "    \n",
    "    T = np.eye(n_factors)\n",
    "    \n",
    "    for iteration in range(q):\n",
    "        T_old = T.copy()\n",
    "        \n",
    "        # Current rotated loadings\n",
    "        L = loadings @ T\n",
    "        \n",
    "        # Varimax criterion - maximize variance of squared loadings\n",
    "        # Compute gradient of varimax objective\n",
    "        u = n_features * L**3 - gamma * L @ np.diag(np.sum(L**2, axis=0))\n",
    "        \n",
    "        # SVD to find optimal rotation\n",
    "        A = loadings.T @ u\n",
    "        U, s, Vt = np.linalg.svd(A)\n",
    "        T = U @ Vt\n",
    "        \n",
    "        # Check convergence\n",
    "        if np.sum((T - T_old)**2) < tol:\n",
    "            break\n",
    "    \n",
    "    rotated_loadings = loadings @ T\n",
    "    return rotated_loadings, T\n",
    "\n",
    "# COMPUTE THE EXPLAINED VARIANCE FOR EACH FACTOR \n",
    "def compute_explained_variance(factors, original_data):\n",
    "    loadings = np.linalg.lstsq(factors, original_data, rcond=None)[0]  # shape: (n_factors, n_features)\n",
    "    \n",
    "    # Reconstruct data using each factor individually\n",
    "    total_var = np.var(original_data, axis=0, ddof=1).sum()\n",
    "    explained_vars = []\n",
    "    \n",
    "    for i in range(factors.shape[1]):\n",
    "        # Reconstruct using only factor i\n",
    "        reconstructed = np.outer(factors[:, i], loadings[i, :])\n",
    "        explained_var = np.var(reconstructed, axis=0, ddof=1).sum()\n",
    "        explained_vars.append(explained_var)\n",
    "    \n",
    "    explained_var_ratio = np.array(explained_vars) / total_var\n",
    "    cumulative_var = np.cumsum(explained_var_ratio)\n",
    "    \n",
    "    return explained_var_ratio, cumulative_var\n",
    "\n",
    "def analyze_factor_loadings(factors, original_data, method_name=\"\"):\n",
    "    # Compute factor loadings via regression: loadings = (factors^T * factors)^-1 * factors^T * data\n",
    "    factors = np.array(factors)\n",
    "    original_data = np.array(original_data)\n",
    "    \n",
    "    # Compute loadings: each column is the loading for one factor\n",
    "    loadings = np.linalg.lstsq(factors, original_data, rcond=None)[0].T  # shape: (n_features, n_factors)\n",
    "    \n",
    "    # Perform varimax rotation on loadings\n",
    "    rotated_loadings, rotation_matrix = varimax_rotation(loadings)\n",
    "    \n",
    "    # Rotate the factors accordingly\n",
    "    rotated_factors = factors @ rotation_matrix\n",
    "    \n",
    "    # Compute explained variance\n",
    "    explained_var_ratio, cumulative_var = compute_explained_variance(factors, original_data)\n",
    "    rotated_explained_var_ratio, rotated_cumulative_var = compute_explained_variance(rotated_factors, original_data)\n",
    "    \n",
    "    results = {\n",
    "        'loadings': loadings,\n",
    "        'rotated_loadings': rotated_loadings,\n",
    "        'rotation_matrix': rotation_matrix,\n",
    "        'rotated_factors': rotated_factors,\n",
    "        'explained_var_ratio': explained_var_ratio,\n",
    "        'cumulative_var': cumulative_var,\n",
    "        'rotated_explained_var_ratio': rotated_explained_var_ratio,\n",
    "        'rotated_cumulative_var': rotated_cumulative_var\n",
    "    }\n",
    "    \n",
    "    if method_name:\n",
    "        print(f\"\\n{method_name} Factor Analysis:\")\n",
    "        print(f\"  Explained Variance Ratio: {explained_var_ratio}\")\n",
    "        print(f\"  Cumulative Variance: {cumulative_var}\")\n",
    "        print(f\"  Rotated Explained Variance Ratio: {rotated_explained_var_ratio}\")\n",
    "        print(f\"  Rotated Cumulative Variance: {rotated_cumulative_var}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Add this to your main pipeline - insert after storing latent representations\n",
    "def add_factor_analysis_to_pipeline():\n",
    "    # Get validation data for this run\n",
    "    val_tensor = X_tensor[val_data.indices]\n",
    "    X_val_np = val_tensor.numpy()\n",
    "    \n",
    "    # Analyze each method\n",
    "    classic_analysis = analyze_factor_loadings(\n",
    "        results['classic_factors'][-1], X_val_np, \"Classic AE\"\n",
    "    )\n",
    "    optimal_analysis = analyze_factor_loadings(\n",
    "        results['optimal_factors'][-1], X_val_np, \"Optimal AE\"\n",
    "    )\n",
    "    trainable_analysis = analyze_factor_loadings(\n",
    "        results['trainable_factors'][-1], X_val_np, \"Trainable AE\"\n",
    "    )\n",
    "    pca_analysis = analyze_factor_loadings(\n",
    "        results['pca_factors'][-1], X_val_np, \"PCA\"\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    results.setdefault('classic_analysis', []).append(classic_analysis)\n",
    "    results.setdefault('optimal_analysis', []).append(optimal_analysis)\n",
    "    results.setdefault('trainable_analysis', []).append(trainable_analysis)\n",
    "    results.setdefault('pca_analysis', []).append(pca_analysis)\n",
    "\n",
    "\n",
    "\n",
    "# After all runs complete, analyze average results\n",
    "def analyze_average_factor_results(results):\n",
    "    \"\"\"\n",
    "    Analyze average factor analysis results across all runs\n",
    "    \"\"\"\n",
    "    methods = ['classic', 'optimal', 'trainable', 'pca']\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"AVERAGE FACTOR ANALYSIS RESULTS ACROSS ALL RUNS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for method in methods:\n",
    "        analysis_key = f'{method}_analysis'\n",
    "        if analysis_key in results:\n",
    "            analyses = results[analysis_key]\n",
    "            \n",
    "            # Average explained variance ratios\n",
    "            avg_explained_var = np.mean([a['explained_var_ratio'] for a in analyses], axis=0)\n",
    "            avg_cumulative_var = np.mean([a['cumulative_var'] for a in analyses], axis=0)\n",
    "            avg_rotated_explained_var = np.mean([a['rotated_explained_var_ratio'] for a in analyses], axis=0)\n",
    "            avg_rotated_cumulative_var = np.mean([a['rotated_cumulative_var'] for a in analyses], axis=0)\n",
    "            \n",
    "            print(f\"\\n{method.upper()} METHOD:\")\n",
    "            print(f\"  Average Explained Variance Ratio: {avg_explained_var}\")\n",
    "            print(f\"  Average Cumulative Variance: {avg_cumulative_var}\")\n",
    "            print(f\"  Average Rotated Explained Variance Ratio: {avg_rotated_explained_var}\")\n",
    "            print(f\"  Average Rotated Cumulative Variance: {avg_rotated_cumulative_var}\")\n",
    "            \n",
    "            # Show factor interpretability (how much variance is concentrated)\n",
    "            loading_concentration = np.mean([np.std(np.abs(a['rotated_loadings']), axis=0) for a in analyses], axis=0)\n",
    "            print(f\"  Loading Concentration (higher = more interpretable): {loading_concentration}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# THIS IS WHERE THE ACTUAL PIPELINE STARTS !!! \n",
    "X_np = framework.processed_data['returns']  \n",
    "X_tensor = torch.from_numpy(X_np.values).float()    \n",
    "\n",
    "n_samples = X_tensor.shape[0]\n",
    "train_size = int(0.8 * n_samples)\n",
    "val_size = n_samples - train_size\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "# Slice chronologically\n",
    "train_data = X_tensor[:train_size]\n",
    "val_data = X_tensor[train_size:]\n",
    "val_indices = np.arange(train_size, len(X_np))  \n",
    "val_dates = X_np.index[val_indices]\n",
    "\n",
    "# Create DataLoader for training and validation\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n",
    "val_loader   = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Data shape: {X_np.shape}\")\n",
    "print(f\"Data type: {type(X_np)}\")\n",
    "print(f\"Tensor shape: {X_tensor.shape}\")\n",
    "\n",
    "\n",
    "# Set dimensions\n",
    "input_dim = X_np.shape[1]\n",
    "r = 3  # bottleneck dimension\n",
    "\n",
    "# Compute optimal affine params once\n",
    "A, b, mu = compute_optimal_A_b_mu(X_np, r)\n",
    "\n",
    "# Initialize results\n",
    "results = {\n",
    "    'classic_mse': [],\n",
    "    'optimal_mse': [],\n",
    "    'trainable_mse': [],\n",
    "    'nonlinear_mse': [],\n",
    "    'classic_train_loss': [],\n",
    "    'classic_val_loss': [],\n",
    "    'trainable_train_loss': [],\n",
    "    'trainable_val_loss': [],\n",
    "    'nonlinear_train_loss': [],\n",
    "    'nonlinear_val_loss': [],\n",
    "    'classic_train_histories': [],\n",
    "    'classic_val_histories': [],\n",
    "    'trainable_train_histories': [],\n",
    "    'trainable_val_histories': [],\n",
    "    'nonlinear_train_histories': [],\n",
    "    'nonlinear_val_histories': [],\n",
    "    'classic_factors': [],\n",
    "    'optimal_factors': [],\n",
    "    'trainable_factors': [],\n",
    "    'nonlinear_factors': [],\n",
    "}\n",
    "    # Init models\n",
    "model_classic = ClassicAffineAutoencoder(input_dim, r)\n",
    "model_optimal = OptimalAffineAutoencoder(input_dim, r, A, b, mu)\n",
    "model_optimal_trainable = OptimalTrainableAffineAutoencoder(input_dim, r, A, b, mu)\n",
    "model_nonlinear = NonlinearAutoencoder(input_dim, r)\n",
    "\n",
    "    # Train Classic AE\n",
    "model_classic, losses_classic_train, losses_classic_val = train_autoencoder(\n",
    "    model_classic, train_loader, val_loader, num_epochs=100, lr=0.001\n",
    ")\n",
    "\n",
    "    # Validate Optimal AE\n",
    "loss_optimal_val = valOnlyOptimalAffineAutoencoder(model_optimal, val_loader)\n",
    "\n",
    "    # Train Trainable Optimal AE\n",
    "model_optimal_trainable, losses_trainable_train, losses_trainable_val = train_autoencoder(\n",
    "    model_optimal_trainable, train_loader, val_loader, num_epochs=20, lr=0.001\n",
    ")\n",
    "\n",
    "    # Train Nonlinear AE\n",
    "model_nonlinear, losses_nonlinear_train, losses_nonlinear_val = train_autoencoder(\n",
    "    model_nonlinear, train_loader, val_loader, num_epochs=100, lr=0.001\n",
    ")\n",
    "\n",
    "    # PCA baseline\n",
    "\n",
    "X_train_np = train_data.numpy()\n",
    "X_val_np = val_data.numpy()\n",
    "\n",
    "pca = PCA(n_components=r)\n",
    "pca.fit(X_train_np)\n",
    "\n",
    "Z_pca_val = pca.transform(X_val_np)  # latent\n",
    "X_pca_reconstructed = pca.inverse_transform(Z_pca_val)  # reconstruction\n",
    "\n",
    "mse_pca = np.mean((X_val_np - X_pca_reconstructed) ** 2)\n",
    "results.setdefault('pca_mse', []).append(mse_pca)\n",
    "results.setdefault('pca_factors', []).append(Z_pca_val)\n",
    "\n",
    "    # Save MSE results\n",
    "results['classic_mse'].append(min(losses_classic_val))\n",
    "results['optimal_mse'].append(loss_optimal_val)\n",
    "results['trainable_mse'].append(min(losses_trainable_val))\n",
    "results['nonlinear_mse'].append(min(losses_nonlinear_val))\n",
    "\n",
    "    # Save final losses\n",
    "\n",
    "results['classic_train_loss'].append(losses_classic_train[-1])\n",
    "results['classic_val_loss'].append(losses_classic_val[-1])\n",
    "results['trainable_train_loss'].append(losses_trainable_train[-1])\n",
    "results['trainable_val_loss'].append(losses_trainable_val[-1])\n",
    "results['nonlinear_train_loss'].append(losses_nonlinear_train[-1])\n",
    "results['nonlinear_val_loss'].append(losses_nonlinear_val[-1])\n",
    "\n",
    "    # Save loss histories\n",
    "results['classic_train_histories'].append(losses_classic_train)\n",
    "results['classic_val_histories'].append(losses_classic_val)\n",
    "results['trainable_train_histories'].append(losses_trainable_train)\n",
    "results['trainable_val_histories'].append(losses_trainable_val)\n",
    "results['nonlinear_train_histories'].append(losses_nonlinear_train)\n",
    "results['nonlinear_val_histories'].append(losses_nonlinear_val)\n",
    "    # Store latent representations (optional, unaligned)\n",
    "with torch.no_grad():\n",
    "    X_val_np = val_data.numpy()\n",
    "        \n",
    "        # Store latent representations\n",
    "    classic_factors = model_classic.encoder(val_data).cpu().numpy()\n",
    "    optimal_factors = model_optimal.encoder(val_data).cpu().numpy()\n",
    "    trainable_factors = model_optimal_trainable.encoder(val_data).cpu().numpy()\n",
    "    nonlinear_factors = model_nonlinear.encoder(val_data).cpu().numpy()\n",
    "\n",
    "    results['classic_factors'].append(classic_factors)\n",
    "    results['optimal_factors'].append(optimal_factors)\n",
    "    results['trainable_factors'].append(trainable_factors)\n",
    "    results['nonlinear_factors'].append(nonlinear_factors)\n",
    "\n",
    "        # Perform factor analysis with rotation\n",
    "        # For other runs, compute without printing\n",
    "    classic_analysis = analyze_factor_loadings(classic_factors, X_val_np, \"\")\n",
    "    optimal_analysis = analyze_factor_loadings(optimal_factors, X_val_np, \"\")\n",
    "    trainable_analysis = analyze_factor_loadings(trainable_factors, X_val_np, \"\")\n",
    "    pca_analysis = analyze_factor_loadings(results['pca_factors'][-1], X_val_np, \"\")\n",
    "    nonlinear_analysis = analyze_factor_loadings(nonlinear_factors, X_val_np, \"\")\n",
    "\n",
    "        # Store analysis results\n",
    "    results.setdefault('classic_analysis', []).append(classic_analysis)\n",
    "    results.setdefault('optimal_analysis', []).append(optimal_analysis)\n",
    "    results.setdefault('trainable_analysis', []).append(trainable_analysis)\n",
    "    results.setdefault('pca_analysis', []).append(pca_analysis)\n",
    "    results.setdefault('nonlinear_analysis', []).append(nonlinear_analysis)\n",
    "\n",
    "# Remove timezone info to match ff_factors index\n",
    "val_dates_naive = val_dates.tz_localize(None)\n",
    "\n",
    "ff_val = ff_factors.loc[val_dates_naive]\n",
    "ff_factors_latent = ff_val[['Mkt-RF', 'SMB', 'HML']].values\n",
    "\n",
    "# Drop RF column and keep only Mkt-RF, SMB, HML\n",
    "ff_factors_latent = ff_factors_latent[:, :3]\n",
    "print(f\"Returns index dtype: {X_np.index.dtype}, tz: {X_np.index.tz}\")\n",
    "print(f\"FF factors index dtype: {ff_val.index.dtype}, tz: {ff_val.index.tz}\")\n",
    "\n",
    "# If one has timezone and other doesn’t, convert to timezone naive:\n",
    "X_dates_naive = X_np.index.tz_localize(None) if X_np.index.tz else X_np.index\n",
    "ff_dates_naive = ff_factors.index.tz_localize(None) if ff_factors.index.tz else ff_factors.index\n",
    "\n",
    "common_dates = X_dates_naive.intersection(ff_dates_naive)\n",
    "print(f\"Number of common dates after tz fix: {len(common_dates)}\")\n",
    "\n",
    "# Store and analyze\n",
    "results.setdefault('ff_factors', []).append(ff_factors_latent)\n",
    "ff_analysis = analyze_factor_loadings(ff_factors_latent, X_val_np, \"Fama-French (GT)\")\n",
    "results.setdefault('ff_analysis', []).append(ff_analysis)\n",
    "results.setdefault('pca_factors', []).append(Z_pca_val)\n",
    "\n",
    "# 3. AFTER YOUR MAIN LOOP COMPLETES: Add this analysis\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "\n",
    "# Your existing results summary\n",
    "for key in ['classic_mse', 'optimal_mse', 'trainable_mse', 'pca_mse', 'nonlinear_mse']:\n",
    "    values = results[key]\n",
    "    print(f\"{key}: mean = {np.mean(values):.8f},  min = {np.min(values):.8f}, max = {np.max(values):.8f}\")\n",
    "\n",
    "\n",
    "# Compare factor stability across runs\n",
    "def compare_factor_stability(results):\n",
    "    print(\"FACTOR STABILITY ANALYSIS\")\n",
    "\n",
    "    methods = ['classic', 'optimal', 'trainable', 'pca', 'ff', 'nonlinear']\n",
    "    \n",
    "    for method in methods:\n",
    "        analysis_key = f'{method}_analysis'\n",
    "        if analysis_key in results:\n",
    "            analyses = results[analysis_key]\n",
    "            \n",
    "            # Look at consistency of explained variance across runs\n",
    "            explained_vars = [a['rotated_explained_var_ratio'] for a in analyses]\n",
    "            mean_explained = np.mean(explained_vars, axis=0)\n",
    "            std_explained = np.std(explained_vars, axis=0)\n",
    "            \n",
    "            print(f\"\\n{method.upper()} - Rotated Factor Stability:\")\n",
    "            print(f\"  Factor 1 (Market?): {mean_explained[0]:.3f}\")\n",
    "            print(f\"  Factor 2 (Size?):   {mean_explained[1]:.3f} \")\n",
    "            print(f\"  Factor 3 (Value?):  {mean_explained[2]:.3f} \")\n",
    "            \n",
    "            # Check if factors are well-separated\n",
    "            total_explained = np.sum(mean_explained)\n",
    "            print(f\"  Total Explained Variance: {total_explained:.3f}\")\n",
    "            \n",
    "            # Factor balance (ideally factors should be somewhat balanced)\n",
    "            max_factor = np.max(mean_explained)\n",
    "            min_factor = np.min(mean_explained)\n",
    "            balance_ratio = min_factor / max_factor\n",
    "            print(f\"  Factor Balance (higher=better): {balance_ratio:.3f}\")\n",
    "\n",
    "# Call the stability analysis\n",
    "compare_factor_stability(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dea81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed run 10/100\n",
      "Completed run 20/100\n",
      "Completed run 30/100\n",
      "Completed run 40/100\n",
      "Completed run 50/100\n",
      "Completed run 60/100\n",
      "Completed run 70/100\n",
      "Completed run 80/100\n",
      "Completed run 90/100\n",
      "Completed run 100/100\n",
      "\n",
      "============================================================\n",
      "AVERAGE FACTOR ANALYSIS RESULTS ACROSS ALL RUNS\n",
      "============================================================\n",
      "\n",
      "LINEAR METHOD:\n",
      "  Average Explained Variance Ratio: [0.04841822 0.05318192 0.05392538]\n",
      "  Std Explained Variance Ratio: [0.04872584 0.05492875 0.04914283]\n",
      "  Training MSE: 0.00049337 ± 0.00001052\n",
      "  Validation MSE: 0.00040017 ± 0.00000419\n",
      "\n",
      "NONLINEAR METHOD:\n",
      "  Average Explained Variance Ratio: [0.07546662 0.08004685 0.06692649]\n",
      "  Std Explained Variance Ratio: [0.06437017 0.06357637 0.05377892]\n",
      "  Training MSE: 0.00027245 ± 0.00000306\n",
      "  Validation MSE: 0.00030224 ± 0.00000461\n",
      "\n",
      "============================================================\n",
      "FACTOR STABILITY ANALYSIS\n",
      "============================================================\n",
      "\n",
      "LINEAR - Rotated Factor Stability:\n",
      "  Factor 1: 0.0484 ± 0.0487\n",
      "  Factor 2: 0.0532 ± 0.0549\n",
      "  Factor 3: 0.0539 ± 0.0491\n",
      "  Total Explained Variance: 0.1555\n",
      "  Factor Balance (higher=better): 0.8979\n",
      "  Balance Std: 0.0000\n",
      "\n",
      "NONLINEAR - Rotated Factor Stability:\n",
      "  Factor 1: 0.0755 ± 0.0644\n",
      "  Factor 2: 0.0800 ± 0.0636\n",
      "  Factor 3: 0.0669 ± 0.0538\n",
      "  Total Explained Variance: 0.2224\n",
      "  Factor Balance (higher=better): 0.8361\n",
      "  Balance Std: 0.0000\n",
      "\n",
      "============================================================\n",
      "CUMULATIVE VARIANCE ANALYSIS\n",
      "============================================================\n",
      "\n",
      "LINEAR - Average Cumulative Variance: [0.04841822 0.10160014 0.15552553]\n",
      "LINEAR - Std Cumulative Variance: [0.04872584 0.0552143  0.04453195]\n",
      "NONLINEAR - Average Cumulative Variance: [0.07546662 0.15551347 0.22243996]\n",
      "NONLINEAR - Std Cumulative Variance: [0.06437017 0.0649606  0.04621567]\n",
      "\n",
      "DETAILED CUMULATIVE VARIANCE:\n",
      "LINEAR:\n",
      "  Up to Factor 1: 0.0484 ± 0.0487\n",
      "  Up to Factor 2: 0.1016 ± 0.0552\n",
      "  Up to Factor 3: 0.1555 ± 0.0445\n",
      "NONLINEAR:\n",
      "  Up to Factor 1: 0.0755 ± 0.0644\n",
      "  Up to Factor 2: 0.1555 ± 0.0650\n",
      "  Up to Factor 3: 0.2224 ± 0.0462\n",
      "\n",
      "============================================================\n",
      "MSE PERFORMANCE COMPARISON\n",
      "============================================================\n",
      "\n",
      "TRAINING MSE:\n",
      "  Linear Model:    0.00049337 ± 0.00001052\n",
      "  Nonlinear Model: 0.00027245 ± 0.00000306\n",
      "\n",
      "VALIDATION MSE:\n",
      "  Linear Model:    0.00040017 ± 0.00000419\n",
      "  Nonlinear Model: 0.00030224 ± 0.00000461\n",
      "\n",
      "OVERFITTING ANALYSIS:\n",
      "\n",
      "============================================================\n",
      "SUMMARY COMPARISON\n",
      "============================================================\n",
      "Linear Model    - Total Variance Explained: 0.1555\n",
      "Nonlinear Model - Total Variance Explained: 0.2224\n",
      "Linear Model    - Most Important Factor: 0.0539\n",
      "Nonlinear Model - Most Important Factor: 0.0800\n",
      "Linear Model    - Factor Balance: 0.8979\n",
      "Nonlinear Model - Factor Balance: 0.8361\n",
      "Linear Model    - Validation MSE: 0.00040017\n",
      "Nonlinear Model - Validation MSE: 0.00030224\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "X_val = val_data.numpy()\n",
    "r = 3  # number of latent factors\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "results = {\n",
    "    'linear_analysis': [],\n",
    "    'nonlinear_analysis': [],\n",
    "    'linear_train_mse': [],\n",
    "    'linear_val_mse': [],\n",
    "    'nonlinear_train_mse': [],\n",
    "    'nonlinear_val_mse': []\n",
    "}\n",
    "\n",
    "# Run 100 iterations\n",
    "for run in range(100):\n",
    "    seed = 42 + run  # Different seed for each run\n",
    "    set_seed(seed)\n",
    "    \n",
    "    # Print progress every 10 runs\n",
    "    if (run + 1) % 10 == 0:\n",
    "        print(f\"Completed run {run + 1}/100\")\n",
    "    \n",
    "    # Init model and train model\n",
    "    modellinear = ClassicAffineAutoencoder(input_dim, r).to(device)\n",
    "    modelnonlinear = NonlinearAutoencoder(input_dim, r).to(device)\n",
    "    \n",
    "    modellinear, train_losslinear, val_losslinear = train_autoencoder(\n",
    "        modellinear, train_loader, val_loader, num_epochs=150, lr=0.001)\n",
    "    modelnonlinear, train_lossnonlinear, val_lossnonlinear = train_autoencoder(\n",
    "        modelnonlinear, train_loader, val_loader, num_epochs=100, lr=0.001)\n",
    "    \n",
    "    # Store final MSE values (assuming losses are MSE)\n",
    "    results['linear_train_mse'].append(train_losslinear[-1])  # Final training MSE\n",
    "    results['linear_val_mse'].append(val_losslinear[-1])      # Final validation MSE\n",
    "    results['nonlinear_train_mse'].append(train_lossnonlinear[-1])  # Final training MSE\n",
    "    results['nonlinear_val_mse'].append(val_lossnonlinear[-1])      # Final validation MSE\n",
    "    \n",
    "    # Get factors\n",
    "    classic_factors = modellinear.encoder(val_data).detach().cpu().numpy()\n",
    "    nonlinear_factors = modelnonlinear.encoder(val_data).detach().cpu().numpy()\n",
    "    \n",
    "    # Analyze factor loadings (without printing)\n",
    "    classic_analysis = analyze_factor_loadings(classic_factors, X_val, \"\")\n",
    "    nonlinear_analysis = analyze_factor_loadings(nonlinear_factors, X_val, \"\")\n",
    "    \n",
    "    # Store analysis results\n",
    "    results['linear_analysis'].append(classic_analysis)\n",
    "    results['nonlinear_analysis'].append(nonlinear_analysis)\n",
    "\n",
    "# Extract explained variance ratios from each analysis\n",
    "linear_explained_vars = [analysis['rotated_explained_var_ratio'] for analysis in results['linear_analysis']]\n",
    "nonlinear_explained_vars = [analysis['rotated_explained_var_ratio'] for analysis in results['nonlinear_analysis']]\n",
    "\n",
    "# Convert to numpy arrays\n",
    "linear_explained_vars = np.array(linear_explained_vars)\n",
    "nonlinear_explained_vars = np.array(nonlinear_explained_vars)\n",
    "\n",
    "# Convert MSE lists to numpy arrays\n",
    "linear_train_mse = np.array(results['linear_train_mse'])\n",
    "linear_val_mse = np.array(results['linear_val_mse'])\n",
    "nonlinear_train_mse = np.array(results['nonlinear_train_mse'])\n",
    "nonlinear_val_mse = np.array(results['nonlinear_val_mse'])\n",
    "\n",
    "# Compute statistics\n",
    "linear_mean = np.mean(linear_explained_vars, axis=0)\n",
    "linear_std = np.std(linear_explained_vars, axis=0)\n",
    "nonlinear_mean = np.mean(nonlinear_explained_vars, axis=0)\n",
    "nonlinear_std = np.std(nonlinear_explained_vars, axis=0)\n",
    "\n",
    "# Compute MSE statistics\n",
    "linear_train_mse_mean = np.mean(linear_train_mse)\n",
    "linear_train_mse_std = np.std(linear_train_mse)\n",
    "linear_val_mse_mean = np.mean(linear_val_mse)\n",
    "linear_val_mse_std = np.std(linear_val_mse)\n",
    "\n",
    "nonlinear_train_mse_mean = np.mean(nonlinear_train_mse)\n",
    "nonlinear_train_mse_std = np.std(nonlinear_train_mse)\n",
    "nonlinear_val_mse_mean = np.mean(nonlinear_val_mse)\n",
    "nonlinear_val_mse_std = np.std(nonlinear_val_mse)\n",
    "\n",
    "# Print results following your framework style\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AVERAGE FACTOR ANALYSIS RESULTS ACROSS ALL RUNS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nLINEAR METHOD:\")\n",
    "print(f\"  Average Explained Variance Ratio: {linear_mean}\")\n",
    "print(f\"  Std Explained Variance Ratio: {linear_std}\")\n",
    "print(f\"  Training MSE: {linear_train_mse_mean:.8f} ± {linear_train_mse_std:.8f}\")\n",
    "print(f\"  Validation MSE: {linear_val_mse_mean:.8f} ± {linear_val_mse_std:.8f}\")\n",
    "\n",
    "print(f\"\\nNONLINEAR METHOD:\")\n",
    "print(f\"  Average Explained Variance Ratio: {nonlinear_mean}\")\n",
    "print(f\"  Std Explained Variance Ratio: {nonlinear_std}\")\n",
    "print(f\"  Training MSE: {nonlinear_train_mse_mean:.8f} ± {nonlinear_train_mse_std:.8f}\")\n",
    "print(f\"  Validation MSE: {nonlinear_val_mse_mean:.8f} ± {nonlinear_val_mse_std:.8f}\")\n",
    "\n",
    "# Factor stability analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FACTOR STABILITY ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "methods = [('LINEAR', linear_mean, linear_std), ('NONLINEAR', nonlinear_mean, nonlinear_std)]\n",
    "\n",
    "for method_name, mean_explained, std_explained in methods:\n",
    "    print(f\"\\n{method_name} - Rotated Factor Stability:\")\n",
    "    print(f\"  Factor 1: {mean_explained[0]:.4f} ± {std_explained[0]:.4f}\")\n",
    "    print(f\"  Factor 2: {mean_explained[1]:.4f} ± {std_explained[1]:.4f}\")\n",
    "    print(f\"  Factor 3: {mean_explained[2]:.4f} ± {std_explained[2]:.4f}\")\n",
    "    \n",
    "    # Check if factors are well-separated\n",
    "    total_explained = np.sum(mean_explained)\n",
    "    print(f\"  Total Explained Variance: {total_explained:.4f}\")\n",
    "\n",
    "    # Factor balance (ideally factors should be somewhat balanced)\n",
    "    max_factor = np.max(mean_explained)\n",
    "    min_factor = np.min(mean_explained)\n",
    "    balance_ratio = min_factor / max_factor\n",
    "    balance_std = np.std(balance_ratio)\n",
    "    print(f\"  Factor Balance (higher=better): {balance_ratio:.4f}\")\n",
    "    print(f\"  Balance Std: {balance_std:.4f}\")\n",
    "\n",
    "# Additional analysis: Compare cumulative variance\n",
    "linear_cumulative = [analysis['rotated_cumulative_var'] for analysis in results['linear_analysis']]\n",
    "nonlinear_cumulative = [analysis['rotated_cumulative_var'] for analysis in results['nonlinear_analysis']]\n",
    "\n",
    "# Convert to numpy arrays for proper statistics\n",
    "linear_cumulative = np.array(linear_cumulative)\n",
    "nonlinear_cumulative = np.array(nonlinear_cumulative)\n",
    "\n",
    "linear_cumulative_mean = np.mean(linear_cumulative, axis=0)\n",
    "linear_cumulative_std = np.std(linear_cumulative, axis=0)\n",
    "nonlinear_cumulative_mean = np.mean(nonlinear_cumulative, axis=0)\n",
    "nonlinear_cumulative_std = np.std(nonlinear_cumulative, axis=0)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CUMULATIVE VARIANCE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nLINEAR - Average Cumulative Variance: {linear_cumulative_mean}\")\n",
    "print(f\"LINEAR - Std Cumulative Variance: {linear_cumulative_std}\")\n",
    "print(f\"NONLINEAR - Average Cumulative Variance: {nonlinear_cumulative_mean}\")\n",
    "print(f\"NONLINEAR - Std Cumulative Variance: {nonlinear_cumulative_std}\")\n",
    "\n",
    "# Detailed cumulative variance breakdown\n",
    "print(f\"\\nDETAILED CUMULATIVE VARIANCE:\")\n",
    "print(f\"LINEAR:\")\n",
    "for i in range(len(linear_cumulative_mean)):\n",
    "    print(f\"  Up to Factor {i+1}: {linear_cumulative_mean[i]:.4f} ± {linear_cumulative_std[i]:.4f}\")\n",
    "\n",
    "print(f\"NONLINEAR:\")\n",
    "for i in range(len(nonlinear_cumulative_mean)):\n",
    "    print(f\"  Up to Factor {i+1}: {nonlinear_cumulative_mean[i]:.4f} ± {nonlinear_cumulative_std[i]:.4f}\")\n",
    "\n",
    "# MSE Comparison Section\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MSE PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nTRAINING MSE:\")\n",
    "print(f\"  Linear Model:    {linear_train_mse_mean:.8f} ± {linear_train_mse_std:.8f}\")\n",
    "print(f\"  Nonlinear Model: {nonlinear_train_mse_mean:.8f} ± {nonlinear_train_mse_std:.8f}\")\n",
    "\n",
    "\n",
    "print(f\"\\nVALIDATION MSE:\")\n",
    "print(f\"  Linear Model:    {linear_val_mse_mean:.8f} ± {linear_val_mse_std:.8f}\")\n",
    "print(f\"  Nonlinear Model: {nonlinear_val_mse_mean:.8f} ± {nonlinear_val_mse_std:.8f}\")\n",
    "\n",
    "\n",
    "# Overfitting analysis\n",
    "print(f\"\\nOVERFITTING ANALYSIS:\")\n",
    "linear_overfitting = linear_val_mse_mean - linear_train_mse_mean\n",
    "nonlinear_overfitting = nonlinear_val_mse_mean - nonlinear_train_mse_mean\n",
    "\n",
    "\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Linear Model    - Total Variance Explained: {np.sum(linear_mean):.4f}\")\n",
    "print(f\"Nonlinear Model - Total Variance Explained: {np.sum(nonlinear_mean):.4f}\")\n",
    "\n",
    "print(f\"Linear Model    - Most Important Factor: {np.max(linear_mean):.4f}\")\n",
    "print(f\"Nonlinear Model - Most Important Factor: {np.max(nonlinear_mean):.4f}\")\n",
    "\n",
    "print(f\"Linear Model    - Factor Balance: {np.min(linear_mean)/np.max(linear_mean):.4f}\")\n",
    "print(f\"Nonlinear Model - Factor Balance: {np.min(nonlinear_mean)/np.max(nonlinear_mean):.4f}\")\n",
    "\n",
    "print(f\"Linear Model    - Validation MSE: {linear_val_mse_mean:.8f}\")\n",
    "print(f\"Nonlinear Model - Validation MSE: {nonlinear_val_mse_mean:.8f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
