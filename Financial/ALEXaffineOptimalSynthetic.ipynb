{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cd1f43e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "import random\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bcbf9eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e29fb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_optimal_A_b_mu(X_np: np.ndarray, r: int, eps: float = 1e-8):\n",
    "    # ---------- 1. mean & covariance ----------\n",
    "    mu  = X_np.mean(axis=0)                         # (d,)\n",
    "    X0  = X_np - mu\n",
    "    T   = X_np.shape[0]\n",
    "    S   = (X0.T @ X0) / (T - 1)                    # (d,d)\n",
    "\n",
    "    try:\n",
    "        K = np.linalg.cholesky(S + eps * np.eye(S.shape[0]))  # upper-triangular\n",
    "    except np.linalg.LinAlgError:                             # still not D\n",
    "        # fall back to eig-sqrt so the factor always exists\n",
    "        eigvals, V = np.linalg.eigh(S)\n",
    "        K = V @ np.diag(np.sqrt(np.maximum(eigvals, 0)))      # (d,d)\n",
    "\n",
    "    U, _, _ = np.linalg.svd(K, full_matrices=False)           # U ∈ ℝ^{d×d}\n",
    "    U_r   = U[:, :r]                                          # (d,r)\n",
    "\n",
    "    A_map = U_r @ U_r.T                                       # (d,d)\n",
    "    b     = (np.eye(S.shape[0]) - A_map) @ mu                 # (d,)\n",
    "\n",
    "    return A_map.astype(np.float32), U_r.astype(np.float32), b.astype(np.float32), mu.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "47a682f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python packaged affine autoencoder\n",
    "\"\"\"A\n",
    "\n",
    "class ClassicAffineAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, bottleneck_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(input_dim, bottleneck_dim, bias=True)\n",
    "        self.decoder = nn.Linear(bottleneck_dim, input_dim, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "        \"\"\"\n",
    "class ClassicAffineAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, r):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.r = r\n",
    "        \n",
    "        # The projection matrix A (10x10)\n",
    "        self.projection = nn.Linear(input_dim, input_dim, bias=True)\n",
    "        \n",
    "        # Decoder from r-dimensional space\n",
    "        self.decoder = nn.Linear(r, input_dim, bias=True)\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        \"\"\"Extract r-dimensional factors from projected data\"\"\"\n",
    "        projected = self.projection(x)  # Apply A matrix\n",
    "        # Take first r dimensions or use SVD to get factors\n",
    "        return projected[:, :self.r]  # Simple approach: take first r dims\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7ac4312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimalAffineAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, r, A_map, U_r, b_img, mu):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"A_map\", torch.tensor(A_map, dtype=torch.float32))   # (d,d)\n",
    "        self.register_buffer(\"Ur\",    torch.tensor(U_r,   dtype=torch.float32))   # (d,r)\n",
    "        self.register_buffer(\"b_img\", torch.tensor(b_img, dtype=torch.float32))   # (d,)  image‑space bias\n",
    "        self.register_buffer(\"mu\",    torch.tensor(mu,    dtype=torch.float32))   # (d,)\n",
    "\n",
    "    # -------- encoder / decoder pair used for reconstruction --------\n",
    "    def encoder(self, x):                      # x : (batch,d)\n",
    "        return (x - self.mu) @ self.Ur         # (batch,r)   NO bias\n",
    "\n",
    "    def decoder(self, z):                      # z : (batch,r)\n",
    "        return z @ self.Ur.T + self.mu         # (batch,d)\n",
    "\n",
    "    # full affine projector  x ↦ Âx + b  (rarely used here)\n",
    "    def full_map(self, x):\n",
    "        return x @ self.A_map + self.b_img\n",
    "\n",
    "    # -------- forward pass required by nn.Module --------\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c4b31a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NONLINEAR AUTOENCODER \n",
    "class NonlinearAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, bottleneck_dim, hidden_dim=5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder: input -> hidden -> hidden -> bottleneck\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, bottleneck_dim)\n",
    "        )\n",
    "        \n",
    "        # Decoder: bottleneck -> hidden -> hidden -> output\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(bottleneck_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "640f5f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimalTrainableAffineAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, r, U_r_init, mu_init):\n",
    "        super().__init__()\n",
    "        self.Ur = nn.Parameter(torch.tensor(U_r_init, dtype=torch.float32))   # (d,r)\n",
    "        self.mu = nn.Parameter(torch.tensor(mu_init, dtype=torch.float32))    # (d,)\n",
    "\n",
    "    def encoder(self, x):\n",
    "        return (x - self.mu) @ self.Ur\n",
    "\n",
    "    def decoder(self, z):\n",
    "        return z @ self.Ur.T + self.mu\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7e435baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Modify train_autoencoder to accept loaders instead of raw tensor\n",
    "def train_autoencoder(model, train_loader, val_loader, num_epochs, lr=1e-3):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon = model(batch)\n",
    "            loss = criterion(recon, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item() * batch.size(0)\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader.dataset)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                recon = model(batch)\n",
    "                loss = criterion(recon, batch)\n",
    "                total_val_loss += loss.item() * batch.size(0)\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "\n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a4a2b82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation function only code for the optimal affine autoencoder\n",
    "def valOnlyOptimalAffineAutoencoder(model, val_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = batch.to(device)\n",
    "            recon = model(batch)\n",
    "            loss = criterion(recon, batch)\n",
    "            total_loss += loss.item() * batch.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader.dataset)\n",
    "    return avg_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "18e5ab3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (2000, 10)\n",
      "PCA baseline MSE: 0.000016\n",
      "\n",
      "VALIDATION MSE\n",
      "  Classic AE :  0.00002010\n",
      "  Optimal AE :  0.00001494\n",
      "  PCA        :  0.00001562\n",
      "\n",
      "FINAL CLASSIC LOSSES\n",
      "  Train: 0.00001980 | Val: 0.00002010\n",
      "\n",
      "FACTOR RECOVERY\n",
      "  Classic mean | std : 0.2224 | 0.0425\n",
      "  Optimal mean | std : 0.3555 | 0.3188\n",
      "  PCA     mean | std : 0.3185 | 0.3446\n",
      "\n",
      "Per‑factor means:\n",
      "Factor   Classic   Optimal       PCA\n",
      "------------------------------------\n",
      "1         0.2821    0.8052    0.8057\n",
      "2         0.1862    0.1584    0.0886\n",
      "3         0.1988    0.1030    0.0613\n",
      "\n",
      "SUMMARY\n",
      "     Model  MSE_Mean  Factor_Corr_Mean\n",
      "0  Classic  0.000020          0.222354\n",
      "1  Optimal  0.000015          0.355529\n",
      "2      PCA  0.000016          0.318534\n"
     ]
    }
   ],
   "source": [
    "# %% -------------------------------------------------------------------- #\n",
    "#  0.  DATA  &  THEORY‑CONSISTENT  OPTIMAL  PARAMETERS\n",
    "# ----------------------------------------------------------------------- #\n",
    "X_df      = pd.read_csv(\"assetReturns_ff3factor.csv\")          # (T,d)\n",
    "X_np      = X_df.to_numpy(dtype=np.float32)\n",
    "X_tensor  = torch.tensor(X_np)\n",
    "\n",
    "print(f\"Data shape: {X_np.shape}\")                         # (T, d)\n",
    "\n",
    "input_dim = X_np.shape[1]\n",
    "r         = 3                                              # bottleneck width\n",
    "\n",
    "# --- new: returns projector A_map (d,d)  AND  frame U_r (d,r) ---\n",
    "A_map, U_r, b, mu = compute_optimal_A_b_mu(X_np, r)\n",
    "\n",
    "# %% -------------------------------------------------------------------- #\n",
    "#  1.  RESULTS  DICT  (place‑holders kept for future variants)\n",
    "# ----------------------------------------------------------------------- #\n",
    "results = {\n",
    "    'classic_mse':   [], 'optimal_mse':   [], 'pca_mse':      [],\n",
    "    'classic_train_loss': [], 'classic_val_loss': [],\n",
    "    'classic_factors':    [], 'optimal_factors':   [], 'pca_factors': []\n",
    "}\n",
    "\n",
    "# %% -------------------------------------------------------------------- #\n",
    "#  2.  TRAIN / VAL  SPLIT  (chronological)\n",
    "# ----------------------------------------------------------------------- #\n",
    "n_samples  = X_tensor.shape[0]\n",
    "train_size = int(0.8 * n_samples)\n",
    "\n",
    "train_data = X_tensor[:train_size]\n",
    "train_indices = np.arange(train_size)                      # <-- fixed\n",
    "val_data   = X_tensor[train_size:]\n",
    "val_indices = np.arange(train_size, n_samples)\n",
    "\n",
    "# dates if you ever need them\n",
    "val_dates = X_df.index[val_indices]\n",
    "\n",
    "# %% -------------------------------------------------------------------- #\n",
    "#  3.  DATALOADERS\n",
    "# ----------------------------------------------------------------------- #\n",
    "g = torch.Generator().manual_seed(seed)\n",
    "batch_size  = 64\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True,  generator=g)\n",
    "val_loader   = DataLoader(val_data,   batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# %% -------------------------------------------------------------------- #\n",
    "#  4.  MODEL INSTANTIATION\n",
    "# ----------------------------------------------------------------------- #\n",
    "model_classic = ClassicAffineAutoencoder(input_dim, r)\n",
    "model_optimal = OptimalAffineAutoencoder(input_dim, r, A_map, U_r, b, mu)\n",
    "\n",
    "# %% -------------------------------------------------------------------- #\n",
    "#  5.  TRAIN / EVAL\n",
    "# ----------------------------------------------------------------------- #\n",
    "# ----- Classic AE -----\n",
    "model_classic, losses_classic_train, losses_classic_val = train_autoencoder(\n",
    "    model_classic, train_loader, val_loader, num_epochs=70, lr=1e-3)\n",
    "\n",
    "# ----- Optimal analytic projector (no training) -----\n",
    "loss_optimal_val = valOnlyOptimalAffineAutoencoder(model_optimal, val_loader)\n",
    "\n",
    "# store scalar metrics\n",
    "results['classic_mse'].append(min(losses_classic_val))\n",
    "results['optimal_mse'].append(loss_optimal_val)\n",
    "results['classic_train_loss'].append(losses_classic_train[-1])\n",
    "results['classic_val_loss'].append(losses_classic_val[-1])\n",
    "\n",
    "# keep whole histories (optional diagnostics)\n",
    "results.setdefault('classic_train_histories', []).append(losses_classic_train)\n",
    "results.setdefault('classic_val_histories',   []).append(losses_classic_val)\n",
    "\n",
    "# %% -------------------------------------------------------------------- #\n",
    "#  6.  PCA  BASELINE\n",
    "# ----------------------------------------------------------------------- #\n",
    "X_train_np = train_data.numpy()\n",
    "X_val_np   = val_data.numpy()\n",
    "\n",
    "pca = PCA(n_components=r).fit(X_train_np)\n",
    "X_val_recon = pca.inverse_transform(pca.transform(X_val_np))\n",
    "mse_pca     = np.mean((X_val_np - X_val_recon) ** 2)\n",
    "print(f\"PCA baseline MSE: {mse_pca:.6f}\")\n",
    "\n",
    "# align PCA factors to true factors via Procrustes\n",
    "F_true_full   = pd.read_csv(\"latentFactors_ff3factor.csv\").to_numpy(dtype=np.float32)\n",
    "F_true_tensor = torch.tensor(F_true_full[val_indices])\n",
    "F_val         = F_true_tensor.numpy()\n",
    "\n",
    "Z_pca         = pca.transform(X_val_np)\n",
    "R_pca, _      = orthogonal_procrustes(Z_pca, F_val)\n",
    "Z_pca_aligned = Z_pca @ R_pca\n",
    "corr_pca      = np.abs([np.corrcoef(Z_pca_aligned[:, i], F_val[:, i])[0, 1]\n",
    "                        for i in range(r)])\n",
    "\n",
    "# store PCA stats\n",
    "results['pca_mse'].append(mse_pca)\n",
    "results['pca_factors'].append(corr_pca)\n",
    "\n",
    "# %% -------------------------------------------------------------------- #\n",
    "#  7.  ALIGNMENT HELPER  &  FACTOR  CORRELATIONS\n",
    "# ----------------------------------------------------------------------- #\n",
    "# ------------------------------------------------------------------ #\n",
    "#  Drop‑in replacement for aligned_corr\n",
    "# ------------------------------------------------------------------ #\n",
    "def aligned_corr(model, X_val, F_true_val):\n",
    "    \"\"\"\n",
    "    Align latent factors from `model` to F_true_val (Procrustes) and\n",
    "    return absolute per‑factor correlations.\n",
    "    Works even if the model has no nn.Parameters (buffers only).\n",
    "    \"\"\"\n",
    "    # --- find the device of the model ---\n",
    "    try:                                  # usual case: at least one Parameter\n",
    "        dev = next(model.parameters()).device\n",
    "    except StopIteration:                 # parameter‑less model (buffers only)\n",
    "        dev = next(model.buffers()).device\n",
    "\n",
    "    X_val = X_val.to(dev)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        Z = model.encoder(X_val).cpu().numpy()\n",
    "        F = F_true_val.cpu().numpy()\n",
    "\n",
    "    R, _ = orthogonal_procrustes(Z, F)\n",
    "    Z_aligned = Z @ R\n",
    "    return np.abs([\n",
    "        np.corrcoef(Z_aligned[:, i], F[:, i])[0, 1] for i in range(F.shape[1])\n",
    "    ])\n",
    "\n",
    "\n",
    "val_tensor = X_tensor[val_indices]                         # still CPU\n",
    "\n",
    "results['classic_factors'].append(\n",
    "    aligned_corr(model_classic, val_tensor, F_true_tensor))\n",
    "results['optimal_factors'].append(\n",
    "    aligned_corr(model_optimal, val_tensor, F_true_tensor))\n",
    "\n",
    "# %% -------------------------------------------------------------------- #\n",
    "#  8.  REPORT UTILITY\n",
    "# ----------------------------------------------------------------------- #\n",
    "def print_results_summary(res):\n",
    "    # ---------- MSE ----------\n",
    "    classic_mse = np.array(res['classic_mse'])\n",
    "    optimal_mse = np.array(res['optimal_mse'])\n",
    "    pca_mse     = np.array(res['pca_mse'])\n",
    "\n",
    "    print(\"\\nVALIDATION MSE\")\n",
    "    print(f\"  Classic AE :  {classic_mse.mean():.8f}\")\n",
    "    print(f\"  Optimal AE :  {optimal_mse.mean():.8f}\")\n",
    "    print(f\"  PCA        :  {pca_mse.mean():.8f}\")\n",
    "\n",
    "    # ---------- train / val losses ----------\n",
    "    tr_final = np.array(res['classic_train_loss'])\n",
    "    va_final = np.array(res['classic_val_loss'])\n",
    "    print(\"\\nFINAL CLASSIC LOSSES\")\n",
    "    print(f\"  Train: {tr_final.mean():.8f} | Val: {va_final.mean():.8f}\")\n",
    "\n",
    "    # ---------- factor correlations ----------\n",
    "    classic_f  = np.array(res['classic_factors'])\n",
    "    optimal_f  = np.array(res['optimal_factors'])\n",
    "    pca_f      = np.array(res['pca_factors'])\n",
    "\n",
    "    print(\"\\nFACTOR RECOVERY\")\n",
    "    print(f\"  Classic mean | std : {classic_f.mean():.4f} | {classic_f.std():.4f}\")\n",
    "    print(f\"  Optimal mean | std : {optimal_f.mean():.4f} | {optimal_f.std():.4f}\")\n",
    "    print(f\"  PCA     mean | std : {pca_f.mean():.4f} | {pca_f.std():.4f}\")\n",
    "\n",
    "    print(f\"\\nPer‑factor means:\")\n",
    "    header = f\"{'Factor':<6}{'Classic':>10}{'Optimal':>10}{'PCA':>10}\"\n",
    "    print(header + \"\\n\" + \"-\" * len(header))\n",
    "    for i in range(r):\n",
    "        print(f\"{i+1:<6}{classic_f[:, i].mean():>10.4f}\"\n",
    "                    f\"{optimal_f[:, i].mean():>10.4f}\"\n",
    "                    f\"{pca_f[:, i].mean():>10.4f}\")\n",
    "\n",
    "    # ---------- summary table ----------\n",
    "    summary_df = pd.DataFrame({\n",
    "        'Model': ['Classic', 'Optimal', 'PCA'],\n",
    "        'MSE_Mean': [classic_mse.mean(), optimal_mse.mean(), pca_mse.mean()],\n",
    "        'Factor_Corr_Mean': [classic_f.mean(), optimal_f.mean(), pca_f.mean()]\n",
    "    })\n",
    "    print(\"\\nSUMMARY\")\n",
    "    print(summary_df.round(6))\n",
    "\n",
    "# %% -------------------------------------------------------------------- #\n",
    "print_results_summary(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cfb40b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexr\\Downloads\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:2999: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\alexr\\Downloads\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3000: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed run 10/100\n",
      "Completed run 20/100\n",
      "Completed run 30/100\n",
      "Completed run 40/100\n",
      "Completed run 50/100\n",
      "Completed run 60/100\n",
      "Completed run 70/100\n",
      "Completed run 80/100\n",
      "Completed run 90/100\n",
      "Completed run 100/100\n",
      "\n",
      "============================================================\n",
      "RESULTS SUMMARY\n",
      "============================================================\n",
      "\n",
      "MSE RESULTS:\n",
      "  Linear   Train: 0.00001887 ± 0.00000100\n",
      "  Linear   Val  : 0.00001960 ± 0.00000120\n",
      "  Nonlinear Train: 0.00002541 ± 0.00000162\n",
      "  Nonlinear Val  : 0.00002642 ± 0.00000181\n",
      "\n",
      "FACTOR RECOVERY ANALYSIS\n",
      "\n",
      "Overall correlations:\n",
      "  Classic  : 0.1944 ± 0.1892\n",
      "  Nonlinear: nan ± nan\n",
      "\n",
      "Per‑factor means:\n",
      "Factor       Classic   Nonlinear\n",
      "--------------------------------\n",
      "1             0.3723         nan\n",
      "2             0.1263         nan\n",
      "3             0.0845         nan\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------- #\n",
    "#  REPRODUCIBLE SEEDS\n",
    "# ------------------------------------------------------------------------- #\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------- #\n",
    "#  DATA SHAPES & DEVICE\n",
    "# ------------------------------------------------------------------------- #\n",
    "X_val   = val_data.numpy()            # only for external use later\n",
    "r       = 3                           # number of latent factors\n",
    "device  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ------------------------------------------------------------------------- #\n",
    "#  RESULTS CONTAINERS\n",
    "# ------------------------------------------------------------------------- #\n",
    "results = {\n",
    "    'linear_train_mse':   [], 'linear_val_mse':   [],\n",
    "    'nonlinear_train_mse':[], 'nonlinear_val_mse':[],\n",
    "    'classic_factors':    [], 'nonlinear_factors':[],\n",
    "    'classic_analysis':   [], 'nonlinear_analysis':[]\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------------------- #\n",
    "#  CONSTANT VALIDATION TENSORS\n",
    "# ------------------------------------------------------------------------- #\n",
    "val_tensor        = X_tensor[val_indices]          # CPU master copy\n",
    "val_tensor_device = val_tensor.to(device)          # GPU copy for factor grab\n",
    "\n",
    "# ------------------------------------------------------------------------- #\n",
    "#  HELPER: ALIGN & CORRELATE  (robust to param‑less models)\n",
    "# ------------------------------------------------------------------------- #\n",
    "def aligned_corr(model, X_val, F_true_val):\n",
    "    \"\"\"Align latent factors from `model` to true factors and return abs corr.\"\"\"\n",
    "    try:                     # get model device from parameters if present\n",
    "        dev = next(model.parameters()).device\n",
    "    except StopIteration:    # fall back to buffers (e.g. analytic models)\n",
    "        dev = next(model.buffers()).device\n",
    "\n",
    "    X_val = X_val.to(dev)\n",
    "    with torch.no_grad():\n",
    "        Z = model.encoder(X_val).cpu().numpy()\n",
    "        F = F_true_val.cpu().numpy()\n",
    "\n",
    "    R, _ = orthogonal_procrustes(Z, F)\n",
    "    Z_aligned = Z @ R\n",
    "    return np.abs([\n",
    "        np.corrcoef(Z_aligned[:, i], F[:, i])[0, 1] for i in range(F.shape[1])\n",
    "    ])\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------- #\n",
    "#  MONTE‑CARLO RUNS\n",
    "# ------------------------------------------------------------------------- #\n",
    "for run in range(100):\n",
    "    seed = 42 + run\n",
    "    set_seed(seed)\n",
    "\n",
    "    if (run + 1) % 10 == 0:\n",
    "        print(f\"Completed run {run + 1}/100\")\n",
    "\n",
    "    # ---------- models ----------\n",
    "    modellinear    = ClassicAffineAutoencoder(input_dim, r).to(device)\n",
    "    modelnonlinear = NonlinearAutoencoder(input_dim, r).to(device)\n",
    "\n",
    "    modellinear,   train_losslinear,   val_losslinear   = train_autoencoder(\n",
    "        modellinear,   train_loader, val_loader, num_epochs=150, lr=1e-3)\n",
    "    modelnonlinear,train_lossnonlinear,val_lossnonlinear= train_autoencoder(\n",
    "        modelnonlinear,train_loader, val_loader, num_epochs=100, lr=1e-3)\n",
    "\n",
    "    # ---------- store MSE ----------\n",
    "    results['linear_train_mse'].append(train_losslinear[-1])\n",
    "    results['linear_val_mse']  .append(val_losslinear[-1])\n",
    "    results['nonlinear_train_mse'].append(train_lossnonlinear[-1])\n",
    "    results['nonlinear_val_mse']  .append(val_lossnonlinear[-1])\n",
    "\n",
    "    # ---------- factors on GPU, then back to CPU ----------\n",
    "    classic_factors   = modellinear   .encoder(val_tensor_device) \\\n",
    "                                     .detach().cpu().numpy()\n",
    "    nonlinear_factors = modelnonlinear.encoder(val_tensor_device) \\\n",
    "                                     .detach().cpu().numpy()\n",
    "\n",
    "    results['classic_factors'].append(classic_factors)\n",
    "    results['nonlinear_factors'].append(nonlinear_factors)\n",
    "\n",
    "    # ---------- correlations ----------\n",
    "    results['classic_analysis'].append(\n",
    "        aligned_corr(modellinear,   val_tensor, F_true_tensor))\n",
    "    results['nonlinear_analysis'].append(\n",
    "        aligned_corr(modelnonlinear,val_tensor, F_true_tensor))\n",
    "\n",
    "# ------------------------------------------------------------------------- #\n",
    "#  SUMMARY  (MSE + FACTOR CORR)\n",
    "# ------------------------------------------------------------------------- #\n",
    "print(\"\\n\" + \"=\"*60 + \"\\nRESULTS SUMMARY\\n\" + \"=\"*60)\n",
    "\n",
    "print(\"\\nMSE RESULTS:\")\n",
    "print(f\"  Linear   Train: {np.mean(results['linear_train_mse']):.8f} ± \"\n",
    "      f\"{np.std(results['linear_train_mse']):.8f}\")\n",
    "print(f\"  Linear   Val  : {np.mean(results['linear_val_mse']):.8f} ± \"\n",
    "      f\"{np.std(results['linear_val_mse']):.8f}\")\n",
    "print(f\"  Nonlinear Train: {np.mean(results['nonlinear_train_mse']):.8f} ± \"\n",
    "      f\"{np.std(results['nonlinear_train_mse']):.8f}\")\n",
    "print(f\"  Nonlinear Val  : {np.mean(results['nonlinear_val_mse']):.8f} ± \"\n",
    "      f\"{np.std(results['nonlinear_val_mse']):.8f}\")\n",
    "\n",
    "def print_results_summary(res):\n",
    "    print(\"\\nFACTOR RECOVERY ANALYSIS\")\n",
    "\n",
    "    classic_corr   = np.array(res['classic_analysis'])     # (runs,r)\n",
    "    nonlinear_corr = np.array(res['nonlinear_analysis'])   # (runs,r)\n",
    "\n",
    "    print(f\"\\nOverall correlations:\")\n",
    "    print(f\"  Classic  : {classic_corr.mean():.4f} ± {classic_corr.std():.4f}\")\n",
    "    print(f\"  Nonlinear: {nonlinear_corr.mean():.4f} ± \"\n",
    "          f\"{nonlinear_corr.std():.4f}\")\n",
    "\n",
    "    print(\"\\nPer‑factor means:\")\n",
    "    header = f\"{'Factor':<8}{'Classic':>12}{'Nonlinear':>12}\"\n",
    "    print(header + \"\\n\" + \"-\" * len(header))\n",
    "    for i in range(classic_corr.shape[1]):\n",
    "        print(f\"{i+1:<8}{classic_corr[:, i].mean():>12.4f}\"\n",
    "                    f\"{nonlinear_corr[:, i].mean():>12.4f}\")\n",
    "\n",
    "print_results_summary(results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
