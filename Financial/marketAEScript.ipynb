{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a977f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['PYTHONHASHSEED'] = '42'\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.decomposition import PCA\n",
    "import requests, io\n",
    "from zipfile import ZipFile\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1f4bfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9721e8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data for 198 stocks...\n",
      "Date range: 2015-01-01 to 2024-12-31\n",
      "Processed 10 of 198 stocks\n",
      "Processed 20 of 198 stocks\n",
      "Processed 30 of 198 stocks\n",
      "Processed 40 of 198 stocks\n",
      "Processed 50 of 198 stocks\n",
      "Processed 60 of 198 stocks\n",
      "Processed 70 of 198 stocks\n",
      "Processed 80 of 198 stocks\n",
      "Processed 90 of 198 stocks\n",
      "Processed 100 of 198 stocks\n",
      "Processed 110 of 198 stocks\n",
      "Processed 120 of 198 stocks\n",
      "Processed 130 of 198 stocks\n",
      "Processed 140 of 198 stocks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$WRK: possibly delisted; no timezone found\n",
      "$WRK: possibly delisted; no timezone found\n",
      "$WRK: possibly delisted; no timezone found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRK has insufficient data (0 days)\n",
      "WRK has insufficient data (0 days)\n",
      "WRK has insufficient data (0 days)\n",
      "Processed 150 of 198 stocks\n",
      "Processed 160 of 198 stocks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$PXD: possibly delisted; no timezone found\n",
      "$PXD: possibly delisted; no timezone found\n",
      "$PXD: possibly delisted; no timezone found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PXD has insufficient data (0 days)\n",
      "PXD has insufficient data (0 days)\n",
      "PXD has insufficient data (0 days)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$TWTR: possibly delisted; no price data found  (1d 2015-01-01 -> 2024-12-31)\n",
      "$TWTR: possibly delisted; no price data found  (1d 2015-01-01 -> 2024-12-31)\n",
      "$TWTR: possibly delisted; no price data found  (1d 2015-01-01 -> 2024-12-31)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 170 of 198 stocks\n",
      "TWTR has insufficient data (0 days)\n",
      "TWTR has insufficient data (0 days)\n",
      "TWTR has insufficient data (0 days)\n",
      "Processed 180 of 198 stocks\n",
      "Processed 190 of 198 stocks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$DISH: possibly delisted; no timezone found\n",
      "$DISH: possibly delisted; no timezone found\n",
      "$DISH: possibly delisted; no timezone found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISH has insufficient data (0 days)\n",
      "DISH has insufficient data (0 days)\n",
      "DISH has insufficient data (0 days)\n",
      "Processed 198 of 198 stocks\n",
      "\n",
      "Successfully downloaded 194 stocks\n",
      "Failed downloads: 4\n",
      "Failed tickers: ['WRK', 'PXD', 'TWTR', 'DISH']...\n",
      "Initial data shape: (2515, 194)\n",
      "After removing stocks with insufficient data: (2515, 194)\n",
      "After removing stocks with excessive zero prices: (2515, 194)\n",
      "cleaned data shape: (788, 194)\n",
      "downloads: 194 stocks\n",
      "failed downloads: 4 stocks\n",
      "date range: 2021-11-11 to 2024-12-30\n",
      "trading days: 787\n",
      "\n",
      " metrics:\n",
      "Mean daily return: 0.000292\n",
      "Mean daily volatility: 0.3221\n",
      "\n",
      " Failed tickers: ['WRK', 'PXD', 'TWTR', 'DISH']\n"
     ]
    }
   ],
   "source": [
    "class EnhancedStockDataFramework:\n",
    "    def __init__(self, start_date=\"2015-01-01\", end_date=\"2024-12-31\"):\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.raw_data = None\n",
    "        self.processed_data = None\n",
    "        self.failed_downloads = []\n",
    "        self.successful_tickers = []\n",
    "        \n",
    "    def get_stock_universe(self):\n",
    "        \"\"\"Define stock universes with current, active tickers\"\"\"\n",
    "         # Comprehensive universe (200 stocks)\n",
    "        tickers = [\n",
    "                # tech\n",
    "                'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'META', 'TSLA', 'NVDA', 'NFLX', 'ADBE', 'CRM',\n",
    "                'ORCL', 'INTC', 'AMD', 'QCOM', 'AVGO', 'TXN', 'AMAT', 'LRCX', 'KLAC', 'MRVL',\n",
    "                \n",
    "                # finance\n",
    "                'JPM', 'BAC', 'WFC', 'C', 'GS', 'MS', 'BK', 'USB', 'PNC', 'TFC',\n",
    "                'COF', 'AXP', 'V', 'MA', 'PYPL', 'BRK-B', 'BLK', 'SPGI', 'ICE', 'CME',\n",
    "                \n",
    "                # health and pharma \n",
    "                'JNJ', 'PFE', 'UNH', 'ABT', 'TMO', 'DHR', 'BMY', 'MRK', 'ABBV', 'AMGN',\n",
    "                'GILD', 'BIIB', 'REGN', 'VRTX', 'ILMN', 'ISRG', 'SYK', 'BSX', 'MDT', 'EW',\n",
    "                \n",
    "                # consumer discretionary \n",
    "                'AMZN', 'TSLA', 'HD', 'MCD', 'NKE', 'SBUX', 'TJX', 'LOW', 'BKNG', 'ABNB',\n",
    "                'DIS', 'NFLX', 'CMCSA', 'VZ', 'T', 'CHTR', 'TMUS', 'ROKU', 'SPOT', 'UBER',\n",
    "                \n",
    "                # consumer staples\n",
    "                'PG', 'KO', 'PEP', 'WMT', 'COST', 'CL', 'KMB', 'GIS', 'K', 'CPB',\n",
    "                'TSN', 'HRL', 'SJM', 'CAG', 'MKC', 'CLX', 'CHD', 'EL', 'ULTA', 'TGT',\n",
    "                \n",
    "                # industrial and manufacturing\n",
    "                'BA', 'CAT', 'DE', 'GE', 'HON', 'UPS', 'FDX', 'LMT', 'RTX', 'NOC',\n",
    "                'MMM', 'EMR', 'ETN', 'ITW', 'PH', 'CMI', 'ROK', 'DOV', 'FTV', 'XYL',\n",
    "                \n",
    "                # energy and utilities\n",
    "                'XOM', 'CVX', 'COP', 'EOG', 'PXD', 'VLO', 'MPC', 'PSX', 'KMI', 'OKE',\n",
    "                'NEE', 'DUK', 'SO', 'AEP', 'EXC', 'XEL', 'PEG', 'ED', 'AWK', 'ATO',\n",
    "                \n",
    "                # materials and basic industries \n",
    "                'LIN', 'APD', 'ECL', 'SHW', 'FCX', 'NEM', 'FMC', 'LYB', 'DD', 'DOW',\n",
    "                'PPG', 'NUE', 'STLD', 'MLM', 'VMC', 'PKG', 'IP', 'WRK', 'SON', 'AVY',\n",
    "                \n",
    "                # real estate and REITs\n",
    "                'AMT', 'PLD', 'CCI', 'EQIX', 'SPG', 'O', 'WELL', 'AVB', 'EQR', 'DLR',\n",
    "                'BXP', 'VTR', 'ARE', 'MAA', 'UDR', 'CPT', 'KIM', 'REG', 'FRT', 'BRX',\n",
    "                \n",
    "                # communication services\n",
    "                'GOOGL', 'META', 'NFLX', 'DIS', 'CMCSA', 'VZ', 'T', 'CHTR', 'TMUS', 'TWTR',\n",
    "                'SNAP', 'PINS', 'MTCH', 'IAC', 'FOXA', 'PARA', 'WBD', 'LUMN', 'SIRI', 'DISH',\n",
    "                \n",
    "                # emerging growth and others\n",
    "                'SHOP', 'ZM', 'DOCU', 'OKTA', 'SNOW', 'PLTR', 'RBLX', 'HOOD', 'COIN', 'RIVN'\n",
    "            ]\n",
    "        # Remove duplicates and return unique tickers\n",
    "        return list(set(tickers))\n",
    "    \n",
    "    def download_data(self, tickers, max_retries=3):\n",
    "        print(f\"Downloading data for {len(tickers)} stocks...\")\n",
    "        print(f\"Date range: {self.start_date} to {self.end_date}\")\n",
    "        \n",
    "        successful_data = {}\n",
    "        failed_downloads = []\n",
    "        \n",
    "        batch_size = 10\n",
    "        for i in range(0, len(tickers), batch_size):\n",
    "            batch = tickers[i:i+batch_size]\n",
    "            \n",
    "            for ticker in batch:\n",
    "                success = False\n",
    "                for attempt in range(max_retries):\n",
    "                    try:\n",
    "                        # download each ticker \n",
    "                        stock = yf.Ticker(ticker)\n",
    "                        hist = stock.history(start=self.start_date, end=self.end_date)\n",
    "                        \n",
    "                        if not hist.empty and len(hist) > 252: \n",
    "                            successful_data[ticker] = hist['Close']\n",
    "                            success = True\n",
    "                            break\n",
    "                        else:\n",
    "                            print(f\"{ticker} has insufficient data ({len(hist)} days)\")\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"Attempt {attempt + 1} failed for {ticker}: {str(e)}\")\n",
    "                        continue\n",
    "                \n",
    "                if not success:\n",
    "                    failed_downloads.append(ticker)\n",
    "            \n",
    "            # Progress indicator\n",
    "            print(f\"Processed {min(i + batch_size, len(tickers))} of {len(tickers)} stocks\")\n",
    "        \n",
    "        self.failed_downloads = failed_downloads\n",
    "        self.successful_tickers = list(successful_data.keys())\n",
    "        \n",
    "        if successful_data:\n",
    "            # Convert to DataFrame\n",
    "            self.raw_data = pd.DataFrame(successful_data)\n",
    "            print(f\"\\nSuccessfully downloaded {len(successful_data)} stocks\")\n",
    "            print(f\"Failed downloads: {len(failed_downloads)}\")\n",
    "            if failed_downloads:\n",
    "                print(f\"Failed tickers: {failed_downloads[:10]}...\")  # Show first 10\n",
    "        else:\n",
    "            print(\"no data downloaded\")\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def clean_data(self, min_trading_days=252, max_zero_days=5):\n",
    "        # Clean and filter the data\n",
    "        if self.raw_data is None or self.raw_data.empty:\n",
    "            print(\"no raw data\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"Initial data shape: {self.raw_data.shape}\")\n",
    "        \n",
    "        # remove stocks with insufficient data\n",
    "       #initial_count = len(self.raw_data.columns)\n",
    "        self.raw_data = self.raw_data.dropna(thresh=min_trading_days, axis=1)\n",
    "        print(f\"After removing stocks with insufficient data: {self.raw_data.shape}\")\n",
    "        \n",
    "        # remove stocks with too many zero-price days\n",
    "        zero_counts = (self.raw_data <= 0).sum()\n",
    "        valid_stocks = zero_counts[zero_counts <= max_zero_days].index\n",
    "        self.raw_data = self.raw_data[valid_stocks]\n",
    "        print(f\"After removing stocks with excessive zero prices: {self.raw_data.shape}\")\n",
    "        \n",
    "        # forward fill missing values (up to 5 days)\n",
    "        self.raw_data = self.raw_data.fillna(method='ffill', limit=5)\n",
    "        \n",
    "        # remove any remaining NaN values\n",
    "        self.raw_data = self.raw_data.dropna()\n",
    "        \n",
    "        if self.raw_data.empty:\n",
    "            print(\"dataset is empty\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"cleaned data shape: {self.raw_data.shape}\")\n",
    "        return True\n",
    "    \n",
    "    def calculate_returns(self):\n",
    "        # calculate daily returns and basic statistics\n",
    "        if self.raw_data is None or self.raw_data.empty:\n",
    "            print(\"no data available\")\n",
    "            return False\n",
    "        \n",
    "        # calculate daily returns\n",
    "        returns = self.raw_data.pct_change().dropna()\n",
    "        \n",
    "        # remove extreme outliers \n",
    "        extreme_threshold = 0.5\n",
    "        returns_clean = returns.copy()\n",
    "        \n",
    "        # cap extreme returns\n",
    "        returns_clean = returns_clean.clip(lower=-extreme_threshold, upper=extreme_threshold)\n",
    "        \n",
    "        # calculate basic statistics\n",
    "        mean_returns = returns_clean.mean()\n",
    "        volatility = returns_clean.std() * np.sqrt(252)  \n",
    "        \n",
    "        # create summary statistics\n",
    "        self.processed_data = {\n",
    "            'returns': returns_clean,\n",
    "            'prices': self.raw_data,\n",
    "            'mean_daily_return': mean_returns.mean(),\n",
    "            'mean_daily_volatility': volatility.mean(),\n",
    "            'num_stocks': len(returns_clean.columns),\n",
    "            'num_days': len(returns_clean),\n",
    "            'date_range': f\"{returns_clean.index[0].strftime('%Y-%m-%d')} to {returns_clean.index[-1].strftime('%Y-%m-%d')}\"\n",
    "        }\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def generate_summary(self):\n",
    "        # generate a comprehensive summary of the dataset\n",
    "        if self.processed_data is None:\n",
    "            print(\"no processed data available\")\n",
    "            return\n",
    "        \n",
    "        print(f\"downloads: {self.processed_data['num_stocks']} stocks\")\n",
    "        print(f\"failed downloads: {len(self.failed_downloads)} stocks\")\n",
    "        print(f\"date range: {self.processed_data['date_range']}\")\n",
    "        print(f\"trading days: {self.processed_data['num_days']}\")\n",
    "        \n",
    "        print(f\"\\n metrics:\")\n",
    "        print(f\"Mean daily return: {self.processed_data['mean_daily_return']:.6f}\")\n",
    "        print(f\"Mean daily volatility: {self.processed_data['mean_daily_volatility']:.4f}\")\n",
    "        \n",
    "        if self.failed_downloads:\n",
    "            print(f\"\\n Failed tickers: {self.failed_downloads[:10]}\")\n",
    "    \n",
    "    def run_complete_pipeline(self):\n",
    "        tickers = self.get_stock_universe()\n",
    "        \n",
    "        if not self.download_data(tickers):\n",
    "            print(\"no data downloaded\")\n",
    "            return False\n",
    "        \n",
    "        if not self.clean_data():\n",
    "            print(\"no data remaining after cleaning\")\n",
    "            return False\n",
    "        \n",
    "        if not self.calculate_returns():\n",
    "            print(\"no data available\")\n",
    "            return False\n",
    "        \n",
    "        # Generate summary\n",
    "        self.generate_summary()\n",
    "        \n",
    "        return True\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    framework = EnhancedStockDataFramework(\n",
    "        start_date=\"2015-01-01\",\n",
    "        end_date=\"2024-12-31\"\n",
    "    )\n",
    "    \n",
    "    # run the complete pipeline\n",
    "    success = framework.run_complete_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40a10502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets the fama french factors from the site for the date range \n",
    "\n",
    "def get_daily_fama_french_factors(start_date='2021-11-11', end_date='2024-12-31'):\n",
    "    \"\"\"\n",
    "    Downloads and processes the Fama-French 3-factor daily data from Ken French's website.\n",
    "    \"\"\"\n",
    "    url = \"https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Research_Data_Factors_daily_CSV.zip\"\n",
    "    r = requests.get(url)\n",
    "    z = ZipFile(io.BytesIO(r.content))\n",
    "    \n",
    "    # filename from ZIP \n",
    "    with z.open(\"F-F_Research_Data_Factors_daily.csv\") as f:\n",
    "        df_raw = pd.read_csv(f, skiprows=3)\n",
    "\n",
    "    # Drop footer rows that contain non-date strings\n",
    "    df_raw = df_raw[df_raw.iloc[:, 0].str.match(r'^\\d{6,8}$')].copy()\n",
    "\n",
    "    # Rename columns\n",
    "    df_raw.columns = ['Date', 'Mkt-RF', 'SMB', 'HML', 'RF']\n",
    "    df_raw['Date'] = pd.to_datetime(df_raw['Date'], format='%Y%m%d')\n",
    "    df_raw.set_index('Date', inplace=True)\n",
    "\n",
    "    # Convert percent to decimal\n",
    "    df_raw = df_raw.astype(float) / 100.0\n",
    "\n",
    "    # Filter by date range\n",
    "    mask = (df_raw.index >= pd.to_datetime(start_date)) & (df_raw.index <= pd.to_datetime(end_date))\n",
    "    return df_raw.loc[mask]\n",
    "\n",
    "# Example usage\n",
    "ff_factors = get_daily_fama_french_factors('2021-11-11', '2024-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "407da410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoder frameworks\n",
    "def compute_optimal_A_b_mu(X_np, r):\n",
    "    mu = np.mean(X_np, axis=0)\n",
    "    cov = (X_np - mu).T @ (X_np - mu)\n",
    "    U, S, _ = np.linalg.svd(cov)\n",
    "    Ur = U[:, :r]  \n",
    "    \n",
    "    A = Ur @ Ur.T  \n",
    "    b = np.zeros(r)  \n",
    "    \n",
    "    return A, b, mu, Ur\n",
    "\n",
    "\n",
    "# plain linear autoencoder \n",
    "class ClassicAffineAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, r):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.r = r\n",
    "        \n",
    "        self.projection = nn.Linear(input_dim, input_dim, bias=True)\n",
    "        \n",
    "        self.decoder = nn.Linear(r, input_dim, bias=True)\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        projected = self.projection(x)  \n",
    "        return projected[:, :self.r]  \n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "    \n",
    "# optimal autoencoder \n",
    "class OptimalAffineAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, r, Ur, mu):\n",
    "        super().__init__()\n",
    "        self.Ur = torch.tensor(Ur, dtype=torch.float32)  \n",
    "        self.mu = torch.tensor(mu, dtype=torch.float32)\n",
    "\n",
    "    def encoder(self, x):\n",
    "        x_centered = x - self.mu\n",
    "        return x_centered @ self.Ur  \n",
    "\n",
    "    def decoder(self, z):\n",
    "        return z @ self.Ur.T + self.mu  \n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)\n",
    "    \n",
    "# nonlinear autoencoder\n",
    "class NonlinearAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, bottleneck_dim, hidden_dim=5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.Linear(hidden_dim, bottleneck_dim)\n",
    "        )\n",
    "        \n",
    "        # decoder:\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(bottleneck_dim, hidden_dim),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "\n",
    "# train/val function \n",
    "def train_autoencoder(model, train_loader, val_loader, num_epochs=300, lr=1e-3):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon = model(batch)\n",
    "            loss = criterion(recon, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item() * batch.size(0)\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader.dataset)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                recon = model(batch)\n",
    "                loss = criterion(recon, batch)\n",
    "                total_val_loss += loss.item() * batch.size(0)\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(avg_val_loss)\n",
    "    return model, train_losses, val_losses\n",
    "#results['']\n",
    "\n",
    "# val only \n",
    "def valOnlyOptimalAffineAutoencoder(model, val_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = batch.to(device)\n",
    "            recon = model(batch)\n",
    "            loss = criterion(recon, batch)\n",
    "            total_loss += loss.item() * batch.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader.dataset)\n",
    "    return avg_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ead5b4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Summary:\n",
      "Validation MSEs:\n",
      "optimal_mse: 0.00028766\n",
      "pca_mse: 0.00029557\n",
      "No training MSEs available for these models.\n",
      "Cumulative Explained Variance and Factor Stability:\n",
      "\n",
      "OPTIMAL - Cumulative Explained Variance Breakdown:\n",
      "  Factor 1 : 0.125\n",
      "  Factor 2 :   0.137 \n",
      "  Factor 3 :  0.080 \n",
      " Total Explained Variance: 0.342\n",
      "  factor balance: 0.586\n",
      "\n",
      "PCA - Cumulative Explained Variance Breakdown:\n",
      "  Factor 1 : 0.109\n",
      "  Factor 2 :   0.144 \n",
      "  Factor 3 :  0.080 \n",
      " Total Explained Variance: 0.333\n",
      "  factor balance: 0.555\n",
      "\n",
      "FF - Cumulative Explained Variance Breakdown:\n",
      "  Factor 1 : 0.126\n",
      "  Factor 2 :   0.018 \n",
      "  Factor 3 :  0.103 \n",
      " Total Explained Variance: 0.248\n",
      "  factor balance: 0.146\n"
     ]
    }
   ],
   "source": [
    "# varimax and explained variance defs \n",
    "def varimax_rotation(loadings, gamma=1.0, q=20, tol=1e-6):\n",
    "    loadings = np.array(loadings)\n",
    "    n_features, n_factors = loadings.shape\n",
    "    \n",
    "    T = np.eye(n_factors)\n",
    "    \n",
    "    for iteration in range(q):\n",
    "        T_old = T.copy()\n",
    "        L = loadings @ T\n",
    "        # Varimax criterion is to maximize variance of squared loadings\n",
    "        # compute gradient of varimax objective\n",
    "        u = n_features * L**3 - gamma * L @ np.diag(np.sum(L**2, axis=0))\n",
    "        \n",
    "        # SVD to find optimal rotation\n",
    "        A = loadings.T @ u\n",
    "        U, s, Vt = np.linalg.svd(A)\n",
    "        T = U @ Vt\n",
    "\n",
    "        # Check convergence\n",
    "        if np.sum((T - T_old)**2) < tol:\n",
    "            break\n",
    "    \n",
    "    rotated_loadings = loadings @ T\n",
    "    return rotated_loadings, T\n",
    "\n",
    "#  cumulative explained variance  \n",
    "def compute_explained_variance(factors, original_data):\n",
    "    loadings = np.linalg.lstsq(factors, original_data, rcond=None)[0]  \n",
    "    \n",
    "    # Reconstruct data using each factor individually\n",
    "    total_var = np.var(original_data, axis=0, ddof=1).sum()\n",
    "    explained_vars = []\n",
    "    \n",
    "    for i in range(factors.shape[1]):\n",
    "        # Reconstruct using only factor i\n",
    "        reconstructed = np.outer(factors[:, i], loadings[i, :])\n",
    "        explained_var = np.var(reconstructed, axis=0, ddof=1).sum()\n",
    "        explained_vars.append(explained_var)\n",
    "    \n",
    "    explained_var_ratio = np.array(explained_vars) / total_var\n",
    "    cumulative_var = np.cumsum(explained_var_ratio)\n",
    "    \n",
    "    return explained_var_ratio, cumulative_var\n",
    "\n",
    "def analyze_factor_loadings(factors, original_data, method_name=\"\"):\n",
    "    # Compute factor loadings via regression: loadings = (factors^T * factors)^-1 * factors^T * data\n",
    "    factors = np.array(factors)\n",
    "    original_data = np.array(original_data)\n",
    "    \n",
    "    # Compute loadings: each column is the loading for one factor\n",
    "    loadings = np.linalg.lstsq(factors, original_data, rcond=None)[0].T  # shape: (n_features, n_factors)\n",
    "    \n",
    "    # Perform varimax rotation on loadings\n",
    "    rotated_loadings, rotation_matrix = varimax_rotation(loadings)\n",
    "    \n",
    "    # Rotate the factors accordingly\n",
    "    rotated_factors = factors @ rotation_matrix\n",
    "    \n",
    "    # Compute explained variance\n",
    "    explained_var_ratio, cumulative_var = compute_explained_variance(factors, original_data)\n",
    "    rotated_explained_var_ratio, rotated_cumulative_var = compute_explained_variance(rotated_factors, original_data)\n",
    "    \n",
    "    results = {\n",
    "        'loadings': loadings,\n",
    "        'rotated_loadings': rotated_loadings,\n",
    "        'rotation_matrix': rotation_matrix,\n",
    "        'rotated_factors': rotated_factors,\n",
    "        'explained_var_ratio': explained_var_ratio,\n",
    "        'cumulative_var': cumulative_var,\n",
    "        'rotated_explained_var_ratio': rotated_explained_var_ratio,\n",
    "        'rotated_cumulative_var': rotated_cumulative_var\n",
    "    }\n",
    "    return results\n",
    "\n",
    "def add_factor_analysis_to_pipeline():\n",
    "    # Get validation data for this run\n",
    "    val_tensor = X_tensor[val_data.indices]\n",
    "    X_val_np = val_tensor.numpy()\n",
    "    \n",
    "    # Analyze each method\n",
    "    classic_analysis = analyze_factor_loadings(\n",
    "        results['classic_factors'][-1], X_val_np, \"Classic AE\"\n",
    "    )\n",
    "    optimal_analysis = analyze_factor_loadings(\n",
    "        results['optimal_factors'][-1], X_val_np, \"Optimal AE\"\n",
    "    )\n",
    "    pca_analysis = analyze_factor_loadings(\n",
    "        results['pca_factors'][-1], X_val_np, \"PCA\"\n",
    "    )\n",
    "    nonlinear_analysis = analyze_factor_loadings(\n",
    "        results['nonlinear_factors'][-1], X_val_np, \"Nonlinear AE\"\n",
    "    )   \n",
    "    \n",
    "    # Store results\n",
    "    results.setdefault('classic_analysis', []).append(classic_analysis)\n",
    "    results.setdefault('optimal_analysis', []).append(optimal_analysis)\n",
    "    results.setdefault('pca_analysis', []).append(pca_analysis)\n",
    "    results.setdefault('nonlinear_analysis', []).append(nonlinear_analysis)\n",
    "\n",
    "\n",
    "\n",
    "# After all runs complete, analyze average results\n",
    "def analyze_average_factor_results(results):\n",
    "   \n",
    "    methods = ['classic', 'optimal', 'pca', 'nonlinear']\n",
    "    \n",
    "    print(\"AVERAGE FACTOR ANALYSIS RESULTS ACROSS ALL RUNS\")\n",
    "    \n",
    "    for method in methods:\n",
    "        analysis_key = f'{method}_analysis'\n",
    "        if analysis_key in results:\n",
    "            analyses = results[analysis_key]\n",
    "            \n",
    "            # Average explained variance ratios\n",
    "            avg_explained_var = np.mean([a['explained_var_ratio'] for a in analyses], axis=0)\n",
    "            avg_cumulative_var = np.mean([a['cumulative_var'] for a in analyses], axis=0)\n",
    "            avg_rotated_explained_var = np.mean([a['rotated_explained_var_ratio'] for a in analyses], axis=0)\n",
    "            avg_rotated_cumulative_var = np.mean([a['rotated_cumulative_var'] for a in analyses], axis=0)\n",
    "            \n",
    "            print(f\"\\n{method.upper()} METHOD:\")\n",
    "            print(f\"  Average Explained Variance Ratio: {avg_explained_var}\")\n",
    "            print(f\"  Average Cumulative Variance: {avg_cumulative_var}\")\n",
    "            print(f\"  Average Rotated Explained Variance Ratio: {avg_rotated_explained_var}\")\n",
    "            print(f\"  Average Rotated Cumulative Variance: {avg_rotated_cumulative_var}\")\n",
    "            \n",
    "            # Show factor interpretability (how much variance is concentrated)\n",
    "            loading_concentration = np.mean([np.std(np.abs(a['rotated_loadings']), axis=0) for a in analyses], axis=0)\n",
    "            print(f\"  Loading Concentration (higher = more interpretable): {loading_concentration}\")\n",
    "\n",
    "# THIS IS WHERE THE ACTUAL PIPELINE STARTS !!! \n",
    "X_np = framework.processed_data['returns']  \n",
    "X_tensor = torch.from_numpy(X_np.values).float()    \n",
    "\n",
    "n_samples = X_tensor.shape[0]\n",
    "train_size = int(0.8 * n_samples)\n",
    "val_size = n_samples - train_size\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)\n",
    "\n",
    "# slice\n",
    "train_data = X_tensor[:train_size]\n",
    "val_data = X_tensor[train_size:]\n",
    "val_indices = np.arange(train_size, len(X_np))  \n",
    "val_dates = X_np.index[val_indices]\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n",
    "val_loader   = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# inputs and dims \n",
    "input_dim = X_np.shape[1]\n",
    "r = 3\n",
    "A, b, mu, Ur = compute_optimal_A_b_mu(X_np, r)\n",
    "\n",
    "# results dict \n",
    "results = {\n",
    "    'optimal_mse': [],\n",
    "    'optimal_factors': [],\n",
    "}\n",
    "    # init models\n",
    "model_optimal = OptimalAffineAutoencoder(input_dim, r, Ur, mu)\n",
    "\n",
    "\n",
    "loss_optimal_val = valOnlyOptimalAffineAutoencoder(model_optimal, val_loader)\n",
    "\n",
    "\n",
    "X_train_np = train_data.numpy()\n",
    "X_val_np = val_data.numpy()\n",
    "\n",
    "pca = PCA(n_components=r)\n",
    "pca.fit(X_train_np)\n",
    "\n",
    "Z_pca_val = pca.transform(X_val_np)  # latent\n",
    "X_pca_reconstructed = pca.inverse_transform(Z_pca_val)  # reconstruction\n",
    "\n",
    "mse_pca = np.mean((X_val_np - X_pca_reconstructed) ** 2)\n",
    "results.setdefault('pca_mse', []).append(mse_pca)\n",
    "results.setdefault('pca_factors', []).append(Z_pca_val)\n",
    "\n",
    "results['optimal_mse'].append(loss_optimal_val)\n",
    "\n",
    "with torch.no_grad():\n",
    "    X_val_np = val_data.numpy()\n",
    "        \n",
    "        # Store latent representations\n",
    "    optimal_factors = model_optimal.encoder(val_data).cpu().numpy()\n",
    "   \n",
    "\n",
    "    results['optimal_factors'].append(optimal_factors)\n",
    "\n",
    "# Perform factor analysis with rotation\n",
    "    optimal_analysis = analyze_factor_loadings(optimal_factors, X_val_np, \"\")\n",
    "    pca_analysis = analyze_factor_loadings(results['pca_factors'][-1], X_val_np, \"\")\n",
    "\n",
    "# Store analysis results\n",
    "    results.setdefault('optimal_analysis', []).append(optimal_analysis)\n",
    "    results.setdefault('pca_analysis', []).append(pca_analysis)\n",
    "\n",
    "# fix ff3 data to match validation dates \n",
    "val_dates_naive = val_dates.tz_localize(None)\n",
    "ff_val = ff_factors.loc[val_dates_naive]\n",
    "ff_factors_latent = ff_val[['Mkt-RF', 'SMB', 'HML']].values\n",
    "ff_factors_latent = ff_factors_latent[:, :3]\n",
    "X_dates_naive = X_np.index.tz_localize(None) if X_np.index.tz else X_np.index\n",
    "ff_dates_naive = ff_factors.index.tz_localize(None) if ff_factors.index.tz else ff_factors.index\n",
    "common_dates = X_dates_naive.intersection(ff_dates_naive)\n",
    "\n",
    "results.setdefault('ff_factors', []).append(ff_factors_latent)\n",
    "ff_analysis = analyze_factor_loadings(ff_factors_latent, X_val_np, \"Fama-French (GT)\")\n",
    "results.setdefault('ff_analysis', []).append(ff_analysis)\n",
    "results.setdefault('pca_factors', []).append(Z_pca_val)\n",
    "\n",
    "\n",
    "print(\"Results Summary:\")\n",
    "\n",
    "print(\"Validation MSEs:\")\n",
    "for key in ['optimal_mse', 'pca_mse']:\n",
    "    values = results[key]\n",
    "    print(f\"{key}: {np.mean(values):.8f}\")\n",
    "print(\"No training MSEs available for these models.\")\n",
    "\n",
    "# Compare factor stability across runs\n",
    "def compare_factor_stability(results):\n",
    "    \n",
    "    print(\"Cumulative Explained Variance and Factor Stability:\")\n",
    "\n",
    "    methods = ['optimal', 'pca', 'ff']\n",
    "    \n",
    "    for method in methods:\n",
    "        analysis_key = f'{method}_analysis'\n",
    "        if analysis_key in results:\n",
    "            analyses = results[analysis_key]\n",
    "            \n",
    "            # what does this do? \n",
    "            explained_vars = [a['rotated_explained_var_ratio'] for a in analyses]\n",
    "            mean_explained = np.mean(explained_vars, axis=0)\n",
    "\n",
    "            print(f\"\\n{method.upper()} - Cumulative Explained Variance Breakdown:\")\n",
    "            print(f\"  Factor 1 : {mean_explained[0]:.3f}\")\n",
    "            print(f\"  Factor 2 :   {mean_explained[1]:.3f} \")\n",
    "            print(f\"  Factor 3 :  {mean_explained[2]:.3f} \")\n",
    "            \n",
    "            total_explained = np.sum(explained_vars)\n",
    "            print(f\" Total Explained Variance: {total_explained:.3f}\")\n",
    "            \n",
    "            # \n",
    "            max_factor = np.max(explained_vars)\n",
    "            min_factor = np.min(explained_vars)\n",
    "            balance_ratio = min_factor / max_factor\n",
    "            print(f\"  factor balance: {balance_ratio:.3f}\")\n",
    "\n",
    "compare_factor_stability(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6dea81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed run 10/100\n",
      "Completed run 20/100\n",
      "Completed run 30/100\n",
      "Completed run 40/100\n",
      "Completed run 50/100\n",
      "Completed run 60/100\n",
      "Completed run 70/100\n",
      "Completed run 80/100\n",
      "Completed run 90/100\n",
      "Completed run 100/100\n",
      "\n",
      " Results summary across 100 runs\n",
      "\n",
      " MSE results:\n",
      "\n",
      " Training MSEs:\n",
      "  linear model:    0.00049307 ± 0.00000990\n",
      "  nonlinear model: 0.00050453 ± 0.00000664\n",
      "\n",
      " Validaton MSEs:\n",
      "  Linear Model:    0.00040028 ± 0.00000420\n",
      "  Nonlinear Model: 0.00040572 ± 0.00000308\n",
      "\n",
      " Cumulative Explained Variance and Factor Stability:\n",
      "\n",
      "Linear - Rotated Factor Stability:\n",
      "  Factor 1: 0.0513 ± 0.0474\n",
      "  Factor 2: 0.0487 ± 0.0456\n",
      "  Factor 3: 0.0555 ± 0.0567\n",
      " Total Explained Variance: 0.1556 ± 0.0453\n",
      "  Factor Balance Ratio: 0.8777\n",
      "\n",
      "Nonlinear - Rotated Factor Stability:\n",
      "  Factor 1: 0.0648 ± 0.0657\n",
      "  Factor 2: 0.0840 ± 0.0595\n",
      "  Factor 3: 0.0485 ± 0.0601\n",
      " Total Explained Variance: 0.1974 ± 0.0611\n",
      "  Factor Balance Ratio: 0.5775\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_np = framework.processed_data['returns']  \n",
    "X_tensor = torch.from_numpy(X_np.values).float()  \n",
    "train_data = X_tensor[:train_size]\n",
    "val_data = X_tensor[train_size:]\n",
    "val_indices = np.arange(train_size, len(X_np))  \n",
    "val_dates = X_np.index[val_indices]\n",
    "val_tensor = X_tensor[val_indices]\n",
    "X_val_np = val_tensor.numpy()\n",
    "    \n",
    "results = {\n",
    "    'linear_analysis': [],\n",
    "    'nonlinear_analysis': [],\n",
    "    'linear_train_mse': [],\n",
    "    'linear_val_mse': [],\n",
    "    'nonlinear_train_mse': [],\n",
    "    'nonlinear_val_mse': []\n",
    "}\n",
    "\n",
    "# Run 100 iterations\n",
    "for run in range(100):\n",
    "\n",
    "    # Print progress every 10 runs\n",
    "    if (run + 1) % 10 == 0:\n",
    "        print(f\"Completed run {run + 1}/100\")\n",
    "    \n",
    "    # Init model and train model\n",
    "    modellinear = ClassicAffineAutoencoder(input_dim, r).to(device)\n",
    "    modelnonlinear = NonlinearAutoencoder(input_dim, r).to(device)\n",
    "    \n",
    "    modellinear, train_losslinear, val_losslinear = train_autoencoder(\n",
    "        modellinear, train_loader, val_loader, num_epochs=150, lr=0.001)\n",
    "    modelnonlinear, train_lossnonlinear, val_lossnonlinear = train_autoencoder(\n",
    "        modelnonlinear, train_loader, val_loader, num_epochs=100, lr=0.001)\n",
    "    \n",
    "    # Store final MSE values (assuming losses are MSE)\n",
    "    results['linear_train_mse'].append(train_losslinear[-1])  # Final training MSE\n",
    "    results['linear_val_mse'].append(val_losslinear[-1])      # Final validation MSE\n",
    "    results['nonlinear_train_mse'].append(train_lossnonlinear[-1])  # Final training MSE\n",
    "    results['nonlinear_val_mse'].append(val_lossnonlinear[-1])      # Final validation MSE\n",
    "    \n",
    "    # Get factors\n",
    "    classic_factors = modellinear.encoder(val_data).detach().cpu().numpy()\n",
    "    nonlinear_factors = modelnonlinear.encoder(val_data).detach().cpu().numpy()\n",
    "    \n",
    "    # Analyze factor loadings (without printing)\n",
    "    classic_analysis = analyze_factor_loadings(classic_factors, X_val_np, \"\")\n",
    "    nonlinear_analysis = analyze_factor_loadings(nonlinear_factors, X_val_np, \"\")\n",
    "    \n",
    "    # Store analysis results\n",
    "    results['linear_analysis'].append(classic_analysis)\n",
    "    results['nonlinear_analysis'].append(nonlinear_analysis)\n",
    "\n",
    "# Extract explained variance ratios from each analysis\n",
    "linear_explained_vars = [analysis['rotated_explained_var_ratio'] for analysis in results['linear_analysis']]\n",
    "nonlinear_explained_vars = [analysis['rotated_explained_var_ratio'] for analysis in results['nonlinear_analysis']]\n",
    "\n",
    "# Convert to numpy arrays\n",
    "linear_explained_vars = np.array(linear_explained_vars)\n",
    "nonlinear_explained_vars = np.array(nonlinear_explained_vars)\n",
    "\n",
    "# Convert MSE lists to numpy arrays\n",
    "linear_train_mse = np.array(results['linear_train_mse'])\n",
    "linear_val_mse = np.array(results['linear_val_mse'])\n",
    "nonlinear_train_mse = np.array(results['nonlinear_train_mse'])\n",
    "nonlinear_val_mse = np.array(results['nonlinear_val_mse'])\n",
    "\n",
    "# Compute statistics\n",
    "linear_mean = np.mean(linear_explained_vars, axis=0)\n",
    "linear_std = np.std(linear_explained_vars, axis=0)\n",
    "nonlinear_mean = np.mean(nonlinear_explained_vars, axis=0)\n",
    "nonlinear_std = np.std(nonlinear_explained_vars, axis=0)\n",
    "\n",
    "# Compute MSE statistics\n",
    "linear_train_mse_mean = np.mean(linear_train_mse)\n",
    "linear_train_mse_std = np.std(linear_train_mse)\n",
    "linear_val_mse_mean = np.mean(linear_val_mse)\n",
    "linear_val_mse_std = np.std(linear_val_mse)\n",
    "nonlinear_train_mse_mean = np.mean(nonlinear_train_mse)\n",
    "nonlinear_train_mse_std = np.std(nonlinear_train_mse)\n",
    "nonlinear_val_mse_mean = np.mean(nonlinear_val_mse)\n",
    "nonlinear_val_mse_std = np.std(nonlinear_val_mse)\n",
    "\n",
    "# Convert to arrays for more statistics\n",
    "linear_cumulative = [analysis['rotated_cumulative_var'] for analysis in results['linear_analysis']]\n",
    "nonlinear_cumulative = [analysis['rotated_cumulative_var'] for analysis in results['nonlinear_analysis']]\n",
    "\n",
    "linear_cumulative = np.array(linear_cumulative)\n",
    "nonlinear_cumulative = np.array(nonlinear_cumulative)\n",
    "\n",
    "linear_cumulative_mean = np.mean(linear_cumulative, axis=0)\n",
    "linear_cumulative_std = np.std(linear_cumulative, axis=0)\n",
    "nonlinear_cumulative_mean = np.mean(nonlinear_cumulative, axis=0)\n",
    "nonlinear_cumulative_std = np.std(nonlinear_cumulative, axis=0)\n",
    "\n",
    "# results summary printing\n",
    "print(\"\\n Results summary across 100 runs\")\n",
    "\n",
    "# MSE results\n",
    "print(\"\\n MSE results:\")\n",
    "print(f\"\\n Training MSEs:\")\n",
    "print(f\"  linear model:    {linear_train_mse_mean:.8f} ± {linear_train_mse_std:.8f}\")\n",
    "print(f\"  nonlinear model: {nonlinear_train_mse_mean:.8f} ± {nonlinear_train_mse_std:.8f}\")\n",
    "\n",
    "print(f\"\\n Validaton MSEs:\")\n",
    "print(f\"  Linear Model:    {linear_val_mse_mean:.8f} ± {linear_val_mse_std:.8f}\")\n",
    "print(f\"  Nonlinear Model: {nonlinear_val_mse_mean:.8f} ± {nonlinear_val_mse_std:.8f}\")\n",
    "\n",
    "# Factor stability results\n",
    "print(\"\\n Cumulative Explained Variance and Factor Stability:\")\n",
    "\n",
    "methods = [('Linear', linear_mean, linear_std, linear_cumulative_std), ('Nonlinear', nonlinear_mean, nonlinear_std, nonlinear_cumulative_std)]\n",
    "\n",
    "for method_name, mean_explained, std_explained, cumulative_std in methods:\n",
    "    print(f\"\\n{method_name} - Rotated Factor Stability:\")\n",
    "    print(f\"  Factor 1: {mean_explained[0]:.4f} ± {std_explained[0]:.4f}\")\n",
    "    print(f\"  Factor 2: {mean_explained[1]:.4f} ± {std_explained[1]:.4f}\")\n",
    "    print(f\"  Factor 3: {mean_explained[2]:.4f} ± {std_explained[2]:.4f}\")\n",
    "    \n",
    "    # Standard deviation calculations\n",
    "    total_explained = np.sum(mean_explained)\n",
    "    std_explained = np.std(total_explained)\n",
    "    print(f\" Total Explained Variance: {total_explained:.4f} ± {cumulative_std[2]:.4f}\")\n",
    "\n",
    "    # Factor balance calculations\n",
    "    max_factor = np.max(mean_explained)\n",
    "    min_factor = np.min(mean_explained)\n",
    "    balance_ratio = min_factor / max_factor\n",
    "    balance_std = np.std(balance_ratio)\n",
    "    print(f\"  Factor Balance Ratio: {balance_ratio:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
